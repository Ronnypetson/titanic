{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j6dxGxcHAx5P"
      },
      "outputs": [],
      "source": [
        "tr = ToTensor()\n",
        "\n",
        "img_dim = 28\n",
        "\n",
        "def _transform(x):\n",
        "  x = x.resize((img_dim, img_dim))\n",
        "  return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "\n",
        "bsize = 32\n",
        "\n",
        "MNIST_train_data = MNIST(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = MNIST(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def _cat2d(rows, cols, d=32):\n",
        "  \"\"\"\n",
        "  Index in the log-softmax scale.\n",
        "  After sotmax (in the partition dimension)\n",
        "  -inf --> 0\n",
        "  1.0  --> 1\n",
        "  \"\"\"\n",
        "  assert rows + cols <= d\n",
        "  inf = 1.0\n",
        "  idx = np.zeros((rows, cols, d)) - inf\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      idx[row, col, row] = 1.0\n",
        "      idx[row, col, rows + col] = 1.0\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx\n",
        "\n",
        "def cartesian_idx(rows, cols, d=2):\n",
        "  idx = np.zeros((rows, cols, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      # idx[row, col, 0] = (1 + row) / rows\n",
        "      # idx[row, col, 1] = (1 + col) / cols\n",
        "      idx[row, col, 0] = 2.0 * ((row) / rows) - 1.0 ### (row + 1)\n",
        "      idx[row, col, 1] = 2.0 * ((col) / cols) - 1.0 ### (col + 1)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _sgbmd(u, v, idxu, idxv, sim=None, f=None, normalize=True) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Slow General Batch Maromba Dot\"\n",
        "  Slower, more general, implementation for the \"batch maromba dot\" operation.\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  sim: index similarity function\n",
        "  f: value function\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  sim = Pairwise(sim)\n",
        "  f = Pairwise(f)\n",
        "  ###\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  ###\n",
        "  # sims: (M * N) x 1 x (d_u * d_v)\n",
        "  # vals: (M * N) x (d_u * d_v) x d_val\n",
        "  sims = sim(idxu, idxv).reshape(m * n, 1, d_u * d_v) ###\n",
        "  norm = 1.0\n",
        "  if normalize:\n",
        "    # norm: (M * N) x 1\n",
        "    norm = sims.sum(dim=-1)\n",
        "  vals = f(u, v)\n",
        "  vals = vals.reshape(m * n, d_u * d_v, d_val)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.bmm(sims, vals).squeeze(1)\n",
        "  eps = 1e-8\n",
        "  dot = (dot / (norm + eps)).reshape(m, n, d_val)\n",
        "  return dot\n",
        "\n",
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  # Pdb().set_trace()\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  # normalizer: M x N x 1\n",
        "  idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv ** 6.0\n",
        "  # idxuv = nn.functional.relu(idxuv - bias) ###\n",
        "  mag = 2.0 # 10.0 # 200.0\n",
        "  idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # idxuv = nn.functional.gumbel_softmax(\n",
        "  #     6.0 * idxuv - 3.0, dim=2, hard=False, tau=0.2\n",
        "  # )\n",
        "  # idxuv = get_eye(m, d_u, d_v, n, idxuv.device)\n",
        "  # idxuv = idxuv / (idxuv.sum(dim=2).unsqueeze(2) + 1e-6)\n",
        "  # Pdb().set_trace()\n",
        "  ###\n",
        "  # normalizer = idxuv.reshape(m, d_u * d_v, n).sum(dim=1).reshape(m, n, 1)\n",
        "  ###\n",
        "  # normalizer = 1.0 - 1e-6\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  # dot = dot / (normalizer + 1e-6)\n",
        "  return dot\n",
        "\n",
        "def _gbmd(u, v, idxu, idxv, kernel=None, idx_part=None) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"General Batch Maromba Dot\"\n",
        "  Shorter implementation for the \"batch maromba dot\" operation.\n",
        "  u: M x d_u\n",
        "  v: N x d_v\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u = u.shape\n",
        "  n, d_v = v.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  assert (m, d_u, d_idx) == idxu.shape\n",
        "  assert (n, d_v, d_idx) == idxv.shape\n",
        "  if kernel:\n",
        "    idxu = kernel(idxu, idx_part)\n",
        "    idxv = kernel(idxv, idx_part)\n",
        "  # uidxu: M x d_idx\n",
        "  # vidxv: N x d_idx\n",
        "  uidxu = torch.bmm(u.reshape(m, 1, d_u), idxu).squeeze(1)\n",
        "  vidxv = torch.bmm(v.reshape(n, 1, d_v), idxv).squeeze(1)\n",
        "  dot = uidxu @ vidxv.T\n",
        "  ### Under experimentation\n",
        "  normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "  dot = dot / (normalizer + 1e-8) ###\n",
        "  ###\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9hF3Uoi3tF"
      },
      "source": [
        "#### Índices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UrPFWDtli55C"
      },
      "outputs": [],
      "source": [
        "def _fast_kernel_idx_sum(idxu, idxv, k, idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, idx_part)\n",
        "  kidxv = k(idxv, idx_part)\n",
        "  d_idx_k = kidxu.shape[-1]\n",
        "  assert kidxu.shape[:-1] == idxu.shape[:-1]\n",
        "  assert kidxv.shape[:-1] == idxv.shape[:-1]\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx_k)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx_k)\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  # sidx: (M * d_idx) x N + (N * d_idx) x M\n",
        "  sidx = (\n",
        "      (iTki @ skj.T).reshape(m, d_idx, n).permute(0, 2, 1)\n",
        "      + (jTkj @ ski.T).reshape(n, d_idx, m).permute(2, 0, 1)\n",
        "  )\n",
        "  sidx = sidx / norm\n",
        "  sidx = sidx.repeat(batch_m, 1, 1)\n",
        "  return sidx\n",
        "\n",
        "def _fast_kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx)\n",
        "  # iTki_kjTj: M x N x d_idx x d_idx\n",
        "  iTki_kjTj = (iTki @ jTkj.T).reshape(m, d_idx, n, d_idx).permute(0, 2, 1, 3)\n",
        "  diag = torch.diagonal(iTki_kjTj, dim1=2, dim2=3)\n",
        "  ###\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  diag = diag / norm\n",
        "  ###\n",
        "  diag = diag.repeat(batch_m, 1, 1)\n",
        "  return diag\n",
        "\n",
        "def _kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # ski: (M * N) x d_idx\n",
        "  # skj: (M * N) x d_idx\n",
        "  # norm: M x N x 1\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  ski = ski.unsqueeze(1).repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "  skj = skj.unsqueeze(1).repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "  # idxu, kidxu: (M * d_u) x d_idx x 1\n",
        "  # idxv, kidxv: (N * d_v) x d_idx x 1\n",
        "  idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "  idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "  kidxu = kidxu.reshape(m * d_u, d_idx, 1)\n",
        "  kidxv = kidxv.reshape(n * d_v, d_idx, 1)\n",
        "  # sikiT: M x d_idx x d_idx\n",
        "  # sjkjT: N x d_idx x d_idx\n",
        "  sikiT = torch.bmm(idxu, kidxu.permute(0, 2, 1))\n",
        "  sikiT = sikiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "  sjkjT = torch.bmm(idxv, kidxv.permute(0, 2, 1))\n",
        "  sjkjT = sjkjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1)\n",
        "  del kidxu\n",
        "  del kidxv\n",
        "  del idxu\n",
        "  del idxv\n",
        "  # sikiT: (M * N) x d_idx x d_idx\n",
        "  # sjkjT: (M * N) x d_idx x d_idx\n",
        "  sikiT = sikiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  sjkjT = sjkjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  # diag_sikiT_skjjT: (M * N) x d_idx\n",
        "  # skjjT = sjkjT.permute(0, 2, 1)\n",
        "  # diag_sikiT_skjjT = torch.diagonal(torch.bmm(sikiT, skjjT), dim1=1, dim2=2)\n",
        "  # diag_sikiT_skjjT = diag_sikiT_skjjT.unsqueeze(-1)\n",
        "  xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski)\n",
        "  # xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski) - diag_sikiT_skjjT\n",
        "  # xor_idx = diag_sikiT_skjjT\n",
        "  xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "  xor_idx = xor_idx / norm\n",
        "  return xor_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    kernel = _soft_kernel\n",
        "    # kernel = _cosine_kernel\n",
        "    # mdot = _gbmd(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1]),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1]),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     kernel=kernel,\n",
        "    #     idx_part=self._idx_part,\n",
        "    # )\n",
        "    # mdot = _sgbmd(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     sim=relu_cosine,\n",
        "    #     # sim=squared_cosine,\n",
        "    #     f=vecprod,\n",
        "    # )\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    ###\n",
        "    # mdot = _rdot(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    # )\n",
        "    ###\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    # _kernel_idx # _fast_kernel_idx # _fast_kernel_idx_sum\n",
        "    # midx = _fast_kernel_idx_sum(\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     kernel,\n",
        "    #     self._idx_part,\n",
        "    # )\n",
        "    # midx = _sgbmd(\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     sim=relu_cosine,\n",
        "    #     # sim=squared_cosine,\n",
        "    #     # f=vecsum,\n",
        "    #     f=vecmean,\n",
        "    # )\n",
        "    ###\n",
        "    onesa = torch.ones(self.idx.shape).to(self.idx.device)\n",
        "    onesb = torch.ones(b.idx.shape).to(b.idx.device)\n",
        "    # midx = norm_normalize(\n",
        "    #     norm_normalize(_nsbmd(aidx, onesb, aidx, bidx))\n",
        "    #     + norm_normalize(_nsbmd(onesa, bidx, aidx, bidx))\n",
        "    # )\n",
        "    # midx = norm_normalize(_nsbmd(aidx, bidx, aidx, bidx))\n",
        "    midx = (\n",
        "        _nsbmd(aidx, onesb, aidx, bidx)\n",
        "        + _nsbmd(onesa, bidx, aidx, bidx)\n",
        "    ) / 2.0\n",
        "    # Pdb().set_trace()\n",
        "    ###\n",
        "    # midx = norm_normalize(\n",
        "    #     norm_normalize(_rdot(aidx, onesb, aidx, bidx))\n",
        "    #     + norm_normalize(_rdot(onesa, bidx, aidx, bidx))\n",
        "    # )\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "def idx2d(\n",
        "    channels: int,\n",
        "    rows: int,\n",
        "    cols: int,\n",
        "    w: int,\n",
        "    h: int,\n",
        "    stride: int=2,\n",
        "    dilation: int=1,\n",
        "    device=\"cpu\"\n",
        "  ):\n",
        "  idx = []\n",
        "  dilh = 1 + dilation * (h - 1)\n",
        "  dilw = 1 + dilation * (w - 1)\n",
        "  for row in range(0, rows - (dilh - 1), stride):\n",
        "    for col in range(0, cols - (dilw - 1), stride):\n",
        "      for ch in range(channels):\n",
        "        for drow in range(0, dilh, dilation):\n",
        "          for dcol in range(0, dilw, dilation):\n",
        "            idx.append(\n",
        "                cols * rows * ch\n",
        "                + cols * (row + drow)\n",
        "                + (col + dcol)\n",
        "            )\n",
        "  idx = torch.tensor(idx).long().to(device)\n",
        "  return idx\n",
        "\n",
        "def unsort(idxs):\n",
        "  ridxs = [0 for _ in idxs]\n",
        "  for i, idx in enumerate(idxs):\n",
        "    ridxs[idx] = i\n",
        "  ridxs = torch.tensor(ridxs).long().to(idxs.device)\n",
        "  return ridxs\n",
        "\n",
        "def get_perms(tmp_idx):\n",
        "  idxs, _idxs = [], []\n",
        "  for dim in range(tmp_idx.shape[-1]):\n",
        "    ordering = torch.argsort(tmp_idx[:, dim], stable=True)\n",
        "    idxs.append(ordering.cpu().detach())\n",
        "    _idxs.append(unsort(ordering).cpu().detach())\n",
        "  return idxs, _idxs\n",
        "\n",
        "def resort(k, src, tgt):\n",
        "  assert src == 0 or tgt == 0\n",
        "  global idxs, _idxs\n",
        "  if tgt == 0:\n",
        "    return idxs[src][k]\n",
        "  return _idxs[tgt][k]\n",
        "\n",
        "def hoods(dims, k0, w, _min=0, _max=None):\n",
        "  assert len(dims) == len(w), f\"{len(dims)} != {len(w)}\"\n",
        "  if len(dims) == 0:\n",
        "    return [k0] # [k0.item()]\n",
        "  _hoods = []\n",
        "  global idxs, _idxs\n",
        "  _k0d = resort(k0, 0, dims[-1]) #, idxs, _idxs)\n",
        "  for _w in range(-(w[-1] // 2), (w[-1] // 2) + (w[-1] % 2)):\n",
        "    # k0d = min(_max, max(_min, _k0d + _w))\n",
        "    k0d = torch.clip(_k0d + _w, min=_min, max=_max)\n",
        "    _hoods += hoods(\n",
        "        dims[:-1],\n",
        "        resort(\n",
        "            k0d,\n",
        "            dims[-1], 0,\n",
        "            # idxs, _idxs\n",
        "        ),\n",
        "        w[:-1],\n",
        "        # idxs, _idxs,\n",
        "        _min, _max\n",
        "    )\n",
        "  return _hoods\n",
        "\n",
        "idxs, _idxs = None, None\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def idxhood(xidx, ws, stride):\n",
        "  xidx = xidx.reshape(-1, 2).cpu().detach().numpy()\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  all_hoods = neigh.kneighbors(xidx, return_distance=False).reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods\n",
        "\n",
        "def _idxhood(xidx, ws, stride):\n",
        "  \"\"\"\n",
        "  xidx: in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  dims = tuple(range(xidx.shape[-1]))\n",
        "  global idxs, _idxs\n",
        "  idxs, _idxs = get_perms(xidx)\n",
        "  pivots = torch.tensor([piv for piv in range(0, len(xidx), stride)]).long()\n",
        "  all_hoods = hoods(dims, pivots, ws, 0, len(xidx) - 1)\n",
        "  # all_hoods = torch.tensor(all_hoods).long().T.reshape(-1)\n",
        "  all_hoods = torch.cat(all_hoods, dim=0).reshape(len(all_hoods), -1).T\n",
        "  all_hoods = all_hoods.reshape(-1)\n",
        "  # Pdb().set_trace()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZ8zHIcPQPS"
      },
      "source": [
        "#### MModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tQoFxrDIPScK"
      },
      "outputs": [],
      "source": [
        "class MModule(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    # self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    self.W = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params), device=device) - 1.0\n",
        "    )\n",
        "    self.W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params, idx_dim), device=device) - 1.0\n",
        "    )\n",
        "    # self.W_idx = _W_idx\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    ###\n",
        "    if probe_dim:\n",
        "      self.probe = nn.Linear(probe_dim, 10).to(device) # 288, 400, 512\n",
        "    ###\n",
        "    # self.activation = nn.ReLU()\n",
        "    self.activation = nn.ELU()\n",
        "\n",
        "  def _W_step(\n",
        "      self,\n",
        "      x: MTensor,\n",
        "      W: MTensor,\n",
        "      sets,\n",
        "      samples,\n",
        "      random=True,\n",
        "      conv=False,\n",
        "      filter_size=4,\n",
        "      activation=True,\n",
        "      regular_dot=False):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # Put 1 into x\n",
        "    if not conv:\n",
        "      filter_size = in_dim\n",
        "    assert (in_dim % filter_size) == 0\n",
        "    num_windows = (in_dim // filter_size)\n",
        "    # one = MTensor(\n",
        "    #     torch.ones((n * num_windows), 1).to(self.device),\n",
        "    #     torch.ones((n * num_windows), 1, idx_dim).to(self.device),\n",
        "    # )\n",
        "    x = MTensor.reshape(x, (n * num_windows, filter_size))\n",
        "    # Sample W\n",
        "    if conv:\n",
        "      ### filter_size + 1\n",
        "      assert (sets * samples) % (filter_size) == 0\n",
        "      numw_windows = (sets * samples) // (filter_size)\n",
        "      sets, samples = numw_windows, (filter_size)\n",
        "    W_sets = MTensor.reshape(W, (sets, samples))\n",
        "    ## mdot: N x sets\n",
        "    # mdot: (N * num_windows) x numw_windows\n",
        "    mdot = x @ W_sets\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    # mdot: N x num_windows x numw_windows\n",
        "    if conv:\n",
        "      ### Várias \"imagens\" coladas em um sentido\n",
        "      mdot = MTensor.reshape(mdot, (n, num_windows, numw_windows))\n",
        "    return mdot\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    channels = 1\n",
        "    img_h, img_w = img_dim, img_dim\n",
        "    filter_whs = [(3, 3), (3, 3)]\n",
        "    strides = [2, 1]\n",
        "    filter_w, filter_h = filter_whs[0]\n",
        "    stride = strides[0]\n",
        "    filter_area = filter_w * filter_h\n",
        "    filter_volume = channels * filter_area\n",
        "    self.all_pools = [x[:4]]\n",
        "    idx = idx2d(\n",
        "        channels,\n",
        "        img_h, img_w,\n",
        "        filter_w, filter_h,\n",
        "        stride=stride,\n",
        "        device=self.device\n",
        "    )\n",
        "    x = x[:, idx]\n",
        "    ###\n",
        "    pool = x\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      if conv:\n",
        "        pool = self._W_step(\n",
        "            pool,\n",
        "            self.MW[:, wl: wr],\n",
        "            self.sets[step],\n",
        "            self.samples[step],\n",
        "            random=False,\n",
        "            conv=conv,\n",
        "            filter_size=filter_volume,\n",
        "            activation=activate,\n",
        "        )\n",
        "      else:\n",
        "        pool.data = self.probe(pool.data)\n",
        "      ###\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      ###\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        filter_volume = channels * filter_area\n",
        "        pool = MTensor.permute(pool, (0, 2, 1))\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        # assert img_dim % stride == 0\n",
        "        img_h = (img_h - filter_h + stride) // stride\n",
        "        img_w = (img_w - filter_w + stride) // stride\n",
        "        assert img_h * img_w == img_area\n",
        "        # cols = pool.data.shape[1] // rows\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        filter_w, filter_h = filter_whs[nxt_conv_step]\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_area = filter_w * filter_h\n",
        "        filter_volume = channels * filter_area\n",
        "        if nxt_conv:\n",
        "          idx = idx2d(\n",
        "              channels,\n",
        "              img_h, img_w,\n",
        "              filter_w, filter_h,\n",
        "              stride=stride,\n",
        "              device=self.device\n",
        "          )\n",
        "          pool = pool[:, idx]\n",
        "      ###\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mlldpkcPFvk"
      },
      "source": [
        "#### MModule II"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Oipx_P9qYUUb"
      },
      "outputs": [],
      "source": [
        "class MModule2(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    # self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    self.W = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params), device=device) - 1.0\n",
        "    )\n",
        "    self.W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params, idx_dim), device=device) - 1.0\n",
        "    )\n",
        "    # self.W_idx = _W_idx\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    ###\n",
        "    if probe_dim:\n",
        "      self.probe = nn.Linear(probe_dim, 10).to(device) # 288, 400, 512\n",
        "    ###\n",
        "    # self.activation = nn.ReLU()\n",
        "    self.activation = nn.ELU()\n",
        "    self._prev_idx = None\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    n = x.data.shape[0]\n",
        "    filter_whs = [(3, 3, 1), (3, 3, 2), (3, 3, 2)]\n",
        "    strides = [1, 1, 1]\n",
        "    stride = strides[0]\n",
        "    filter_volume = np.prod(filter_whs[0])\n",
        "    self.all_pools = [x[:4]]\n",
        "    idxx = idxhood(x.idx[0], filter_whs[0], strides[0]) ### FIX\n",
        "    # pool: N x (num_windows * window_volume)\n",
        "    pool = x[:, idxx]\n",
        "    ###\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    # idxw = idxhood(\n",
        "    #     mw.idx,\n",
        "    #     filter_whs[0],\n",
        "    #     strides[0]\n",
        "    # ) ### FIX\n",
        "    # mw = mw[:, idxw]\n",
        "    for step in range(n_sets):\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[0, wl: wr],\n",
        "          (self.sets[step], self.samples[step])\n",
        "      )\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      if conv:\n",
        "        # pool: (N * num_windows) x  sets\n",
        "        # Pdb().set_trace()\n",
        "        pool = MTensor.reshape(pool, (-1, filter_volume)) @ mw\n",
        "        pool = MTensor.reshape(pool, (n, -1, self.sets[step]))\n",
        "      else:\n",
        "        pool.data = self.probe(pool.data)\n",
        "      ###\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      ###\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        # pool = MTensor.permute(pool, (0, 2, 1))\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_volume = np.prod(filter_whs[nxt_conv_step])\n",
        "        if nxt_conv:\n",
        "          idxx = idxhood(\n",
        "              pool.idx[0],\n",
        "              filter_whs[nxt_conv_step],\n",
        "              strides[nxt_conv_step]\n",
        "          ) ### FIX\n",
        "          pool = pool[:, idxx]\n",
        "      ###\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    # Pdb().set_trace()\n",
        "    # if self._prev_idx is not None:\n",
        "    #   print((self._prev_idx - self.all_pools[-1][0].idx).mean())\n",
        "    # self._prev_idx = self.all_pools[-1][0].idx\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  y_pred = 10.0 * y_pred.data\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 2 # 3 # rows + cols + hidden_dim + clf_dim # 3\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, d=idx_dim)\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OisDCAuLCmQ8"
      },
      "outputs": [],
      "source": [
        "# tmp_idx = template_x_idx[0].reshape(-1, 3)[:, :2]\n",
        "# idxs, _idxs = get_perms(tmp_idx)\n",
        "# sampled = hoods([0, 1], 14 * 28 + 14, [3, 3], idxs, _idxs, 0, 783)\n",
        "# sampled = np.array([[idx // 28, idx % 28] for idx in sampled])\n",
        "# # print(sampled)\n",
        "# plt.scatter(sampled[:, 0], sampled[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tmp_idx = template_x_idx[0].reshape(-1, 2).cpu().detach().numpy()\n",
        "# # set desired number of neighbors\n",
        "# neigh = NearestNeighbors(n_neighbors=9)\n",
        "# neigh.fit(tmp_idx)\n",
        "# # select indices of k nearest neighbors of the vectors in the input list\n",
        "# neighbors = neigh.kneighbors(tmp_idx, return_distance=False)\n",
        "# print(neighbors[0])\n",
        "# print(neighbors[1])\n",
        "# print(neighbors[28])\n",
        "# print(neighbors[29])"
      ],
      "metadata": {
        "id": "OoGmgn1K--XO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyzd22RQX-Yg"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "HNheVxvNNK30",
        "outputId": "ee9f7eb9-6c8e-4144-e731-8ddbbb18c008"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0014897455694153905 0.5777047276496887\n",
            "-0.0003565885708667338 0.5784240365028381\n"
          ]
        }
      ],
      "source": [
        "hidden_dim = 6272\n",
        "start_mode = True\n",
        "valid_mode = False\n",
        "# TODO: Visualize conv layer output\n",
        "samples = [\n",
        "    # in_ch * h * w,\n",
        "    1 * 3 * 3,\n",
        "    2 * 3 * 3,\n",
        "    2 * 3 * 3,\n",
        "    # 4 * 8 * 3 * 3,\n",
        "    hidden_dim,\n",
        "]\n",
        "sets = [\n",
        "    # out_ch\n",
        "    2,\n",
        "    2,\n",
        "    2,\n",
        "    # 1,\n",
        "    num_classes\n",
        "]\n",
        "n_params = int(np.array(samples).dot(np.array(sets)))\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if start_mode:\n",
        "  # model = MModule(\n",
        "  model = MModule2(\n",
        "      n_params=n_params,\n",
        "      idx_dim=idx_dim,\n",
        "      samples=samples,\n",
        "      sets=sets,\n",
        "      device=device,\n",
        "      probe_dim=hidden_dim,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3) # 1e-2\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "\n",
        "num_epochs = 720 * 4\n",
        "epoch_len = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  ###\n",
        "  print(model.W.mean().item(), model.W.std().item())\n",
        "  print(model.W_idx.mean().item(), model.W_idx.std().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  else:\n",
        "    group_cols = [\"epoch\", \"train loss\"]\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  (\n",
        "    df_train[group_cols]\n",
        "    .groupby(\"epoch\")\n",
        "    .agg(lambda x: x.median(skipna=True))\n",
        "    # .tail(200)\n",
        "    .plot(figsize=(16, 3), grid=True)\n",
        "  )\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG5gdtyeWnjJ"
      },
      "outputs": [],
      "source": [
        "# tidx = aidx.reshape(32, -1, 9, 3)[0, 100].cpu().detach().numpy(); tidx = tidx.reshape(-1, 3)\n",
        "# tidx = aidx.reshape(32, -1, 9, 2)[0].cpu().detach().numpy(); tidx = tidx.reshape(-1, 2)\n",
        "\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 1] * 0.0})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "# midx.reshape(32, -1, 3)[0, 100:105].cpu().detach().numpy()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tJHxWRO_xoX"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t0uL2kM1Okfd"
      },
      "outputs": [],
      "source": [
        "imgs = [img.data.cpu().detach() for img in model.all_pools]\n",
        "idxs = [img.idx.cpu().detach() for img in model.all_pools]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj5tP_tfMAjw",
        "outputId": "a07884e5-f445-432e-b67e-8abb54a6ae34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([4, 784])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "[img.shape for img in imgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRWVQRznvaer"
      },
      "outputs": [],
      "source": [
        "shapes = [\n",
        "    (img_dim, img_dim, 1),\n",
        "    ((img_dim - 0) // 1, (img_dim - 0) // 1, 1),\n",
        "    # ((img_dim - 0) // 1, (img_dim - 0) // 1, 8),\n",
        "    # (((img_dim - 2) // 2 - 2) // 1, ((img_dim - 2) // 2 - 2) // 1, 4),\n",
        "    # (((img_dim - 4) // 1 - 4) // 2, ((img_dim - 4) // 1 - 4) // 2, 8),\n",
        "    # ((img_dim - 4) // 2 - 2, (img_dim - 4) // 2 - 2, 4),\n",
        "    # ((img_dim - 4) // 2 - 4, (img_dim - 4) // 2 - 4, 8),\n",
        "]\n",
        "# display.clear_output(wait=True)\n",
        "plt.close()\n",
        "for idx, img in enumerate(imgs[:len(shapes)]):\n",
        "  img = img[0]\n",
        "  print(img.shape)\n",
        "  img = img.reshape(*shapes[idx])\n",
        "  rows, cols = 1, shapes[idx][2]\n",
        "  fig, ax = plt.subplots(rows, cols, figsize=(min(18, 3 * cols), 3))\n",
        "  for ch in range(cols):\n",
        "    img_ = img[:, :, ch].numpy()\n",
        "    if cols > 1:\n",
        "      ax[ch].imshow(img_)\n",
        "    else:\n",
        "      ax.imshow(img_)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgY4NUoRagWO",
        "outputId": "89523279-db6d-40dc-d58f-2c60fd3a5025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([784, 3])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"d5e54f39-1541-400c-bcad-f999c1b8e9ed\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d5e54f39-1541-400c-bcad-f999c1b8e9ed\")) {                    Plotly.newPlot(                        \"d5e54f39-1541-400c-bcad-f999c1b8e9ed\",                        [{\"x\":[-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"y\":[-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,1.0],\"z\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"type\":\"scatter3d\",\"scene\":\"scene\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]}},\"scene2\":{\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]}},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d5e54f39-1541-400c-bcad-f999c1b8e9ed');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([784, 1, 3])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"c27fed79-1dce-41c3-a197-bfb510834148\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c27fed79-1dce-41c3-a197-bfb510834148\")) {                    Plotly.newPlot(                        \"c27fed79-1dce-41c3-a197-bfb510834148\",                        [{\"x\":[-4.886675834655762,-1.7191669940948486,-1.783086895942688,-1.7830870151519775,-1.783086895942688,-1.7830870151519775,-1.7830870151519775,-1.7830870151519775,-1.7830870151519775,-1.783086895942688,-1.7830870151519775,-1.783086895942688,-1.783086895942688,-1.783086895942688,-1.783086895942688,-1.7830870151519775,-1.783086895942688,-1.7830870151519775,-1.7830870151519775,-1.7830870151519775,-1.783086895942688,-1.7830867767333984,-1.7830870151519775,-1.783086895942688,-1.7830870151519775,-1.783086895942688,-1.7830870151519775,-1.5876548290252686,-4.456830024719238,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302642822266,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.081302165985107,-4.137710094451904,-3.915308713912964,-3.759873628616333,-3.759873151779175,-3.759873628616333,-3.759873151779175,-3.759873628616333,-3.759873390197754,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873628616333,-3.759873390197754,-3.759873151779175,-3.759873628616333,-3.759873151779175,-3.759873628616333,-3.759873151779175,-3.759873628616333,-3.816281318664551,-3.5938804149627686,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384450912475586,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4384453296661377,-3.4948530197143555,-3.272451639175415,-3.117016553878784,-3.117016077041626,-3.117016553878784,-3.117016077041626,-3.117016553878784,-3.117016315460205,-3.117016553878784,-3.117016553878784,-3.117016553878784,-3.117016315460205,-3.117016553878784,-3.117016553878784,-3.117016553878784,-3.117016553878784,-3.117016315460205,-3.117016553878784,-3.117016315460205,-3.117016553878784,-3.117016553878784,-3.117016315460205,-3.117016077041626,-3.117016553878784,-3.117016077041626,-3.117016553878784,-3.117016077041626,-3.117016553878784,-3.173424243927002,-2.9510231018066406,-2.795588254928589,-2.7955880165100098,-2.795588254928589,-2.7955880165100098,-2.795588254928589,-2.7955880165100098,-2.795588254928589,-2.795588254928589,-2.7955880165100098,-2.7955880165100098,-2.795588254928589,-2.795588254928589,-2.795588254928589,-2.795588254928589,-2.7955880165100098,-2.7955880165100098,-2.7955880165100098,-2.795588254928589,-2.795588254928589,-2.7955880165100098,-2.7955880165100098,-2.795588254928589,-2.7955880165100098,-2.795588254928589,-2.7955880165100098,-2.795588254928589,-2.8519959449768066,-2.629594564437866,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741594791412354,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.4741592407226562,-2.530566930770874,-2.308166027069092,-2.152730941772461,-2.152730703353882,-2.152730941772461,-2.152730703353882,-2.152730941772461,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730703353882,-2.152730941772461,-2.152730703353882,-2.152730941772461,-2.152730703353882,-2.152730941772461,-2.2091383934020996,-1.9867373704910278,-1.831302285194397,-1.8313021659851074,-1.831302285194397,-1.8313021659851074,-1.831302285194397,-1.8313021659851074,-1.831302285194397,-1.831302285194397,-1.8313021659851074,-1.8313021659851074,-1.831302285194397,-1.831302285194397,-1.831302285194397,-1.831302285194397,-1.8313021659851074,-1.8313021659851074,-1.8313021659851074,-1.831302285194397,-1.831302285194397,-1.8313021659851074,-1.8313021659851074,-1.831302285194397,-1.8313021659851074,-1.831302285194397,-1.8313021659851074,-1.831302285194397,-1.8877099752426147,-1.665308952331543,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.5098737478256226,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.5098737478256226,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.5098737478256226,-1.509873628616333,-1.5098737478256226,-1.509873628616333,-1.509873628616333,-1.5098735094070435,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.509873628616333,-1.5662813186645508,-1.3438801765441895,-1.188444972038269,-1.1884448528289795,-1.188444972038269,-1.1884448528289795,-1.188444972038269,-1.1884450912475586,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.188444972038269,-1.1884448528289795,-1.188444972038269,-1.1884448528289795,-1.188444972038269,-1.1884448528289795,-1.188444972038269,-1.244852900505066,-1.022451639175415,-0.8670165538787842,-0.8670164346694946,-0.8670165538787842,-0.8670164346694946,-0.8670165538787842,-0.8670164942741394,-0.8670165538787842,-0.8670165538787842,-0.8670164346694946,-0.8670164942741394,-0.8670165538787842,-0.8670165538787842,-0.8670165538787842,-0.8670165538787842,-0.8670164942741394,-0.8670164346694946,-0.8670164942741394,-0.8670165538787842,-0.8670165538787842,-0.8670164346694946,-0.8670164346694946,-0.8670165538787842,-0.8670164346694946,-0.8670165538787842,-0.8670164346694946,-0.8670165538787842,-0.9234243631362915,-0.7010231614112854,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.5455878973007202,-0.6019957065582275,-0.3795945644378662,-0.2241593301296234,-0.22415927052497864,-0.2241593301296234,-0.22415927052497864,-0.2241593301296234,-0.22415930032730103,-0.2241593301296234,-0.2241593301296234,-0.22415927052497864,-0.22415930032730103,-0.2241593301296234,-0.2241593301296234,-0.2241593301296234,-0.2241593301296234,-0.22415930032730103,-0.22415927052497864,-0.22415930032730103,-0.2241593301296234,-0.2241593301296234,-0.22415927052497864,-0.22415927052497864,-0.2241593301296234,-0.22415927052497864,-0.2241593301296234,-0.22415927052497864,-0.2241593301296234,-0.28056713938713074,-0.05816599726676941,0.09726926684379578,0.09726932644844055,0.09726926684379578,0.09726932644844055,0.09726926684379578,0.09726929664611816,0.09726926684379578,0.09726926684379578,0.09726932644844055,0.09726929664611816,0.09726926684379578,0.09726926684379578,0.09726926684379578,0.09726926684379578,0.09726929664611816,0.09726932644844055,0.09726929664611816,0.09726926684379578,0.09726926684379578,0.09726932644844055,0.09726932644844055,0.09726926684379578,0.09726932644844055,0.09726926684379578,0.09726932644844055,0.09726926684379578,0.040861546993255615,0.26326262950897217,0.41869795322418213,0.4186980426311493,0.41869795322418213,0.4186980426311493,0.41869795322418213,0.41869792342185974,0.41869795322418213,0.41869795322418213,0.41869795322418213,0.4186979830265045,0.41869795322418213,0.41869795322418213,0.41869795322418213,0.41869795322418213,0.4186979830265045,0.41869795322418213,0.4186979830265045,0.41869795322418213,0.41869795322418213,0.41869792342185974,0.4186980426311493,0.41869795322418213,0.4186980426311493,0.41869795322418213,0.4186980426311493,0.41869795322418213,0.3622899651527405,0.5846911668777466,0.740126371383667,0.7401264309883118,0.740126371383667,0.7401264309883118,0.740126371383667,0.7401264905929565,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.740126371383667,0.7401263117790222,0.7401264309883118,0.740126371383667,0.7401264309883118,0.740126371383667,0.7401264309883118,0.740126371383667,0.6837186813354492,0.9061197638511658,1.0615551471710205,1.061555027961731,1.0615551471710205,1.061555027961731,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.061555027961731,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.061555027961731,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.0615551471710205,1.061555027961731,1.0615551471710205,1.061555027961731,1.0615551471710205,1.061555027961731,1.0615551471710205,1.0051472187042236,1.227548360824585,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.382983684539795,1.3829835653305054,1.3829835653305054,1.382983684539795,1.382983684539795,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.382983684539795,1.382983684539795,1.382983684539795,1.3829835653305054,1.3829835653305054,1.382983684539795,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.3829835653305054,1.326575756072998,1.5489768981933594,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044122219085693,1.7044121026992798,1.7044121026992798,1.7044122219085693,1.7044122219085693,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044122219085693,1.7044122219085693,1.7044122219085693,1.7044121026992798,1.7044121026992798,1.7044122219085693,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.7044121026992798,1.6480042934417725,1.8704053163528442,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258407592773438,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258407592773438,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258407592773438,2.0258405208587646,2.0258407592773438,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,2.0258405208587646,1.9694331884384155,2.191833972930908,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472697734832764,2.347269296646118,2.347269296646118,2.347269296646118,2.3472695350646973,2.347269296646118,2.347269296646118,2.347269296646118,2.347269296646118,2.3472695350646973,2.347269296646118,2.3472695350646973,2.347269296646118,2.347269296646118,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.3472695350646973,2.2908616065979004,2.5132627487182617,2.6686975955963135,2.6686978340148926,2.6686975955963135,2.6686978340148926,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686975955963135,2.6686978340148926,2.6686975955963135,2.6686978340148926,2.6686975955963135,2.6686978340148926,2.6686975955963135,2.6122899055480957,2.834691286087036,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126848220825,2.990126609802246,2.990126609802246,2.990126609802246,2.990126848220825,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126848220825,2.990126609802246,2.990126848220825,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.990126609802246,2.933718204498291,3.1561198234558105,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.3115546703338623,3.2551469802856445,3.477548360824585,3.632983684539795,3.632983922958374,3.632983684539795,3.632983922958374,3.632983684539795,3.632983922958374,3.632983684539795,3.632983684539795,3.632983684539795,3.632983922958374,3.632983684539795,3.632983684539795,3.632983684539795,3.632983684539795,3.632983922958374,3.632983684539795,3.632983922958374,3.632983684539795,3.632983684539795,3.632983684539795,3.632983922958374,3.632983684539795,3.632983922958374,3.632983684539795,3.632983922958374,3.632983684539795,3.576575756072998,3.7989766597747803,3.9544122219085693,3.954411745071411,3.9544122219085693,3.954411745071411,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.9544122219085693,3.954411745071411,3.9544122219085693,3.954411745071411,3.9544122219085693,3.954411745071411,3.9544122219085693,4.280557632446289,1.4625678062438965,1.5530153512954712,1.5530153512954712,1.5530153512954712,1.5530153512954712,1.5530154705047607,1.5530154705047607,1.5530154705047607,1.5530152320861816,1.5530154705047607,1.5530152320861816,1.5530152320861816,1.5530152320861816,1.5530152320861816,1.5530154705047607,1.5530152320861816,1.5530154705047607,1.5530154705047607,1.5530154705047607,1.5530154705047607,1.5530154705047607,1.5530153512954712,1.5530153512954712,1.5530153512954712,1.5530153512954712,1.5530153512954712,1.810866355895996,4.868991851806641],\"y\":[-6.395786285400391,-6.2483439445495605,-6.134737014770508,-5.8133087158203125,-5.491879940032959,-5.170451641082764,-4.84902286529541,-4.527594566345215,-4.206165790557861,-3.884737014770508,-3.5633084774017334,-3.24187970161438,-2.9204514026641846,-2.59902286529541,-2.2775940895080566,-1.9561656713485718,-1.6347370147705078,-1.3133084774017334,-0.991879940032959,-0.670451283454895,-0.34902262687683105,-0.027593612670898438,0.2938346862792969,0.6152629852294922,0.9366917610168457,1.258120059967041,1.5795483589172363,0.44839155673980713,-5.221894264221191,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997004985809326,0.6211292743682861,0.9425575733184814,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997004985809326,0.6211292743682861,0.9425575733184814,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744523048400879,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727561950683594,0.2997004985809326,0.6211295127868652,0.9425575733184814,1.263986587524414,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.034973621368408,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727561950683594,0.2997004985809326,0.6211295127868652,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.034973621368408,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.034973621368408,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.034973621368408,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.034973621368408,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727561950683594,0.2997004985809326,0.6211295127868652,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727561950683594,0.2997004985809326,0.6211295127868652,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744519472122192,-4.034973621368408,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744520664215088,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727561950683594,0.2997002601623535,0.621129035949707,0.9425578117370605,1.263986587524414,1.5854148864746094,-0.6744523048400879,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744523048400879,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997004985809326,0.6211292743682861,0.9425575733184814,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997004985809326,0.6211292743682861,0.9425575733184814,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.4502997398376465,-6.128870964050293,-5.807442665100098,-5.486013412475586,-5.164585590362549,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878870964050293,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.950299620628357,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645853519439697,-0.3431568145751953,-0.021728038787841797,0.2997004985809326,0.6211292743682861,0.9425575733184814,1.263986349105835,1.5854148864746094,-0.6744521856307983,-4.03497314453125,-6.450299263000488,-6.128870964050293,-5.807442665100098,-5.486013889312744,-5.164585113525391,-4.843156814575195,-4.521728515625,-4.2002997398376465,-3.878871202468872,-3.5574426651000977,-3.236013889312744,-2.9145853519439697,-2.5931568145751953,-2.271728277206421,-1.9502997398376465,-1.6288710832595825,-1.3074424266815186,-0.9860138893127441,-0.6645852327346802,-0.3431568145751953,-0.021727800369262695,0.2997004985809326,0.621129035949707,0.9425578117370605,1.263986349105835,1.5854148864746094,0.544434666633606,-5.076879024505615,-6.38291072845459,-6.061481952667236,-5.740053653717041,-5.4186248779296875,-5.097196578979492,-4.775767803192139,-4.454339504241943,-4.13291072845459,-3.8114821910858154,-3.490053653717041,-3.1686248779296875,-2.847196578979492,-2.5257678031921387,-2.2043392658233643,-1.8829107284545898,-1.5614821910858154,-1.240053415298462,-0.9186251163482666,-0.5971965789794922,-0.27576780319213867,0.04566073417663574,0.36708927154541016,0.6885180473327637,1.009946346282959,1.3313751220703125,1.5842347145080566,1.7717907428741455],\"z\":[-2.4495668411254883,-2.5754709243774414,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.7972854375839233,-1.7972854375839233,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.7972854375839233,-1.7972856760025024,-1.797285556793213,-1.797285556793213,-1.7972854375839233,-1.7972856760025024,-1.7972853183746338,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.797285556793213,-1.2736033201217651,-2.340745687484741,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.5381232500076294,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381234884262085,-2.2138710021972656,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381234884262085,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381234884262085,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381234884262085,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381234884262085,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-2.0522654056549072,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.538123369216919,-2.2138710021972656,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-2.052265167236328,-2.0522656440734863,-1.5381232500076294,-2.2138710021972656,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.052265167236328,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-2.052265167236328,-2.0522654056549072,-1.4503039121627808,-2.462769031524658,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395511627197266,-2.1395509243011475,-2.1395511627197266,-2.1395511627197266,-2.1395511627197266,-2.1395511627197266,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-2.1395509243011475,-1.3477543592453003,-1.3416709899902344],\"type\":\"scatter3d\",\"scene\":\"scene\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]}},\"scene2\":{\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]}},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c27fed79-1dce-41c3-a197-bfb510834148');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def scatter3d(x, y, z):\n",
        "  plot_df = pd.DataFrame({\"x\": x, \"y\": y, \"z\": z})\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None)\n",
        "  return fig\n",
        "\n",
        "# plt.clf(); plt.cla()\n",
        "# plt.close()\n",
        "for i, idx in enumerate(idxs[:len(shapes)]):\n",
        "  idx = idx[0]\n",
        "  print(idx.shape)\n",
        "  idx = idx.reshape(-1, shapes[i][2], idx_dim)\n",
        "  rows, cols = 1 + (idx.shape[1] - 1) // 2, 2\n",
        "  # fig = plt.figure(figsize=(min(18, 3 * cols), 3))\n",
        "  fig = make_subplots(\n",
        "    rows=rows, cols=cols,\n",
        "    specs=[[{\"type\": \"scene\"} for _ in range(cols)] for _ in range(rows)],\n",
        "    # row_heights=[10 for _ in range(rows)],\n",
        "    vertical_spacing=0.05\n",
        "  )\n",
        "  fig.update_layout(\n",
        "    height=600 * rows,\n",
        "    width=600 * cols\n",
        ")\n",
        "  for ch in range(idx.shape[1]):\n",
        "    idx_ = idx[:, ch].numpy()\n",
        "    # ax = fig.add_subplot(rows, cols, ch + 1, projection=\"3d\")\n",
        "    # ax.scatter(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2], marker=\"+\")\n",
        "    # fig = scatter3d(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2])\n",
        "    row, col = (ch // cols) + 1, (ch % cols) + 1\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=idx_[::, 0],\n",
        "            y=idx_[::, 1],\n",
        "            z=idx_[::, 2],\n",
        "            # color=None,\n",
        "            # colorscale=\"Viridis\",\n",
        "            # showscale=False\n",
        "        ),\n",
        "        row=row,\n",
        "        col=col,\n",
        "    )\n",
        "  fig.show()\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFWbwRwuE9lu"
      },
      "outputs": [],
      "source": [
        "# model.MW.idx[0, 0]\n",
        "threshold = rows * cols * hidden_dim\n",
        "param_id = 0\n",
        "layer_idx = model.MW.idx[:, param_id: param_id + 100] # threshold\n",
        "n_parts = idx_dim // img_dim\n",
        "scaled_idx = (\n",
        "    MTensor\n",
        "    ._soft_kernel(layer_idx, img_dim)[0]\n",
        "    .reshape(-1, n_parts, img_dim)\n",
        ") * (n_parts ** 0.5)\n",
        "idx_att = torch.argmax(\n",
        "    scaled_idx,\n",
        "    dim=-1\n",
        ")[:, :2]\n",
        "idx_att = idx_att.cpu().detach().numpy()\n",
        "grid = np.zeros((rows, cols))\n",
        "# for pos in range(len(idx_att)):\n",
        "#   idxx, idxy = idx_att[pos]\n",
        "#   idxx, idxy = int(idxx), int(idxy)\n",
        "#   # grid[int(idxx), int(idxy)] += 1\n",
        "#   grid[idxx, idxy] += scaled_idx[pos, 0, idxx] * scaled_idx[pos, 1, idxy]\n",
        "for idxx in range(rows):\n",
        "  for idxy in range(cols):\n",
        "    grid[idxx, idxy] = (scaled_idx[:, 0, idxx] * scaled_idx[:, 1, idxy]).sum()\n",
        "grid = grid / grid.max()\n",
        "\n",
        "# plt.imshow(grid)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.matshow(grid, cmap=\"seismic\")\n",
        "\n",
        "for (i, j), z in np.ndenumerate(grid):\n",
        "    ax.text(j, i, \"{:0.2f}\".format(z), ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSCHvt7xQq6q"
      },
      "outputs": [],
      "source": [
        "MTensor._soft_kernel(y_pred.idx, img_dim)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-K_7fUh2anJ"
      },
      "source": [
        "### Visualização dos índices dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNJnesCt2f1p"
      },
      "outputs": [],
      "source": [
        "soft_W_idx = scaled_idx # MTensor._soft_kernel(model.MW.idx, img_dim)\n",
        "threshold = 100 # rows * cols * hidden_dim\n",
        "# First layer\n",
        "soft_W_idx = soft_W_idx[:, :threshold].reshape(1, -1, idx_dim)\n",
        "# Last layer\n",
        "# soft_W_idx = soft_W_idx[:, threshold:].reshape(1, -1, idx_dim)\n",
        "soft_W_idx = soft_W_idx.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gaTHY1L2it1"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "sample_idx = np.random.choice(\n",
        "    len(soft_W_idx),\n",
        "    min(len(soft_W_idx), 10000),\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "W_idx_tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=10,\n",
        ").fit_transform(soft_W_idx[sample_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows6icrE2unf"
      },
      "outputs": [],
      "source": [
        "plot_df = pd.DataFrame(\n",
        "    {\n",
        "        \"W_idx x tsne\": W_idx_tsne[:, 0],\n",
        "        \"W_idx y tsne\": W_idx_tsne[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "plot_df.plot.scatter(\n",
        "    x=\"W_idx x tsne\",\n",
        "    y=\"W_idx y tsne\",\n",
        "    figsize=(24, 4),\n",
        "    grid=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUDvxRS3yZu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "RGCfrrmCXap_",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "e0TdCxX0Jzn0",
        "_T9hF3Uoi3tF",
        "kTfYY3SQXNJF",
        "jdZ8zHIcPQPS",
        "QQRFtDATXUmH",
        "039kGqbPXp4d"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPkuei5TKe3mmxtjx4E8gEh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}