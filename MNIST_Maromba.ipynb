{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j6dxGxcHAx5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c451a1-3bfd-4971-934e-9d3bbd0d0fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to MNIST_root/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 68609461.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/cifar-10-python.tar.gz to MNIST_root/\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to MNIST_root_test/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 70763124.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/cifar-10-python.tar.gz to MNIST_root_test/\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ]\n",
        ")\n",
        "\n",
        "tr = ToTensor()\n",
        "# cifar10_norm = Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "cifar10_norm = Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "_channels = 3\n",
        "channels = 1\n",
        "img_dim = 32\n",
        "\n",
        "def _transform(x):\n",
        "  # return (\n",
        "  #     cifar10_norm(tr(x))\n",
        "  #     .reshape(channels, img_dim, img_dim)\n",
        "  #     .permute(1, 2, 0)\n",
        "  #     .reshape(-1)\n",
        "  # )\n",
        "  return (\n",
        "      cifar10_norm(tr(x))\n",
        "      .reshape(_channels, img_dim, img_dim)\n",
        "      .permute(1, 2, 0)\n",
        "      .mean(dim=-1)\n",
        "      .reshape(-1)\n",
        "  )\n",
        "  # return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "  # return (tr(x)).reshape(-1)\n",
        "\n",
        "bsize = 8 ###\n",
        "\n",
        "# SOURCE_DATASET = MNIST\n",
        "# SOURCE_DATASET = FashionMNIST\n",
        "SOURCE_DATASET = CIFAR10\n",
        "\n",
        "MNIST_train_data = SOURCE_DATASET(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = SOURCE_DATASET(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NH7FAomMIRJC"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# # config = {\n",
        "# #     \"features\": {\n",
        "# #         \"pre-set\": channels,\n",
        "# #         \"sets\":    [1024,    1024,    1,],\n",
        "# #         \"samples\": [25,      1,       1024,],\n",
        "# #     },\n",
        "# #     \"params\": {\n",
        "# #         \"sets\":    [4,       1,       num_classes,],\n",
        "# #         \"samples\": [(76, 1), (1, 5),  (1025, 1),],\n",
        "# #         \"is conv\": [True,    True,    False,]\n",
        "# #     },\n",
        "# # }\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 4, 5, padding=\"same\", bias=False)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 8, 5, padding=\"same\", bias=False)\n",
        "#         self.conv2 = nn.Conv2d(4, 1, 1, padding=\"same\", bias=False)\n",
        "#         self.fc1 = nn.Linear(1024, 10, bias=False)\n",
        "#         # self.fc2 = nn.Linear(100, 10, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.elu(self.conv1(x))\n",
        "#         x = F.elu(self.conv2(x))\n",
        "#         # x = F.elu(self.conv3(x))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n7DuUfPdITHx"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3spYLqaUIa3n"
      },
      "outputs": [],
      "source": [
        "# trainloader = train_data_loader\n",
        "# for epoch in range(2):  # loop over the dataset multiple times\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 60 == 59:    # print every 60 mini-batches\n",
        "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 60:.3f}')\n",
        "#             running_loss = 0.0\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aE6KCRaT_zjU"
      },
      "outputs": [],
      "source": [
        "# img, lbl = next(iter(test_data_loader))\n",
        "# img = img.reshape(-1, 3, 32, 32)\n",
        "# img = img.permute(0, 2, 3, 1).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xHhauWzb_4b1"
      },
      "outputs": [],
      "source": [
        "# img[0].min(), img[0].max(), img[0].mean()\n",
        "\n",
        "# cifar10_classes = [\n",
        "#     \"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "# ]\n",
        "# _idx = 3\n",
        "# print(cifar10_classes[lbl[_idx].item()])\n",
        "# plt.imshow(img[_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = 2.0 * ((row + offset) / rows) - 1.0\n",
        "        idx[row, col, ch, 1] = 2.0 * ((col + offset) / cols) - 1.0\n",
        "        idx[row, col, ch, 2] = 0.001 * ((ch  + offset) /  chs) - 0.005\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx\n",
        "\n",
        "def _cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = (row + offset)\n",
        "        idx[row, col, ch, 1] = (col + offset)\n",
        "        idx[row, col, ch, 2] = (ch  + offset)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu, eps=1e-6):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "def poly1norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  idxu = (0.5 ** 0.5) * torch.cat([idxu, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "def poly2norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  _idxu = idxu.reshape((-1, d_idx, 1))\n",
        "  middle = (\n",
        "      torch.bmm(_idxu, _idxu.permute(0, 2, 1))\n",
        "      .reshape((*idxu.shape[:-1], d_idx ** 2))\n",
        "  )\n",
        "  idxu = 0.5 * torch.cat([(2.0 ** 0.5) * idxu, middle, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict as dd\n",
        "from itertools import product\n",
        "\n",
        "@lru_cache()\n",
        "def poly_terms(idx_dim, degree):\n",
        "  combs = dd(int)\n",
        "  ranges = (range(idx_dim) for _ in range(degree))\n",
        "  for idxs in product(*ranges):\n",
        "      comb = tuple(sorted(idxs))\n",
        "      combs[comb] += 1\n",
        "  return list(combs.items())\n",
        "\n",
        "\"\"\"\n",
        "sigmoid(8*x - 4)\n",
        "1/(1 + e^4)\n",
        "+ (8 e^4 x)/(1 + e^4)^2\n",
        "+ (32 e^4 (e^4 - 1) x^2)/(1 + e^4)^3\n",
        "+ (256 (e^4 - 4 e^8 + e^12) x^3)/(3 (1 + e^4)^4)\n",
        "+ (512 e^4 (-1 + 11 e^4 - 11 e^8 + e^12) x^4)/(3 (1 + e^4)^5)\n",
        "+ (4096 (e^4 - 26 e^8 + 66 e^12 - 26 e^16 + e^20) x^5)/(15 (1 + e^4)^6)\n",
        "+ O(x^6)\n",
        "(Taylor series)\n",
        "-------------------------\n",
        "0.017986  * x^0\n",
        "0.14130   * x^1\n",
        "0.5448747 * x^2\n",
        "1.3474883 * x^3\n",
        "2.290065  * x^4\n",
        "2.4479883 * x^5\n",
        "\"\"\"\n",
        "\n",
        "def poly_norm(idxu, degree):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  pre_shape = idxu.shape[:-1]\n",
        "  idx_dim = idxu.shape[-1]\n",
        "  idxu = normalized(idxu)\n",
        "  terms = poly_terms(idx_dim, degree)\n",
        "  factors = torch.tensor([term[1] for term in terms]).float().to(idxu.device)\n",
        "  factors = factors ** 0.5\n",
        "  intidx = torch.tensor([list(term[0]) for term in terms]).long().to(idxu.device)\n",
        "  intidx = intidx.reshape(-1)\n",
        "  idxu = idxu.reshape(-1, idx_dim)[:, intidx]\n",
        "  idxu = idxu.reshape(*pre_shape, degree, -1)\n",
        "  idxu = idxu.prod(dim=-2) * factors.reshape(*((1,) * len(pre_shape)), idxu.shape[-1])\n",
        "  return idxu\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye\n",
        "\n",
        "def log_sinkhorn(log_alpha, n_iter):\n",
        "    \"\"\" https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/sampling/permutations.html\n",
        "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
        "    Concerning nonnegative matrices and doubly stochastic\n",
        "    matrices. Pacific Journal of Mathematics, 1967\n",
        "    Args:\n",
        "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
        "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
        "      n_iters: number of sinkhorn iterations\n",
        "    \"\"\"\n",
        "    for _ in range(n_iter):\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "    return log_alpha.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _knndot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"k-NN Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  num_neigh = 1\n",
        "  dots = []\n",
        "  q_idxu = idxu.cpu().detach().numpy().reshape(-1, d_idx)\n",
        "  for _pos in range(n):\n",
        "    neigh = NearestNeighbors(n_neighbors=num_neigh, metric=\"cosine\")\n",
        "    neigh.fit(idxv[_pos].cpu().detach().numpy().reshape(-1, d_idx))\n",
        "    n_idxu = neigh.kneighbors(\n",
        "        q_idxu, return_distance=False\n",
        "    ).reshape(-1)\n",
        "    n_idxu = torch.from_numpy(n_idxu).long()\n",
        "    _v = v[_pos].reshape(-1, d_val)[n_idxu].reshape(m, d_u, d_val)\n",
        "    # _dot: M x d_val x d_val\n",
        "    _dot = torch.bmm(_v.permute(0, 2, 1), _v)\n",
        "    # _dot: M x 1 x d_val\n",
        "    _dot = torch.diagonal(_dot, dim1=1, dim2=2).unsqueeze(1)\n",
        "    dots.append(_dot)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.cat(dots, dim=1)\n",
        "  return dot\n",
        "\n",
        "def _icbmd(u, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Conv Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x 1 x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_v == 1\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: M x d_u x N\n",
        "  idxv = (\n",
        "      (idxu.reshape(m * d_u, d_idx))\n",
        "      @ idxv.permute(2, 1, 0).reshape(d_idx, n)\n",
        "  ).reshape(m, d_u, n)\n",
        "  # idxv: M x N x d\n",
        "  idxv = torch.bmm(\n",
        "      u.permute(0, 2, 1),\n",
        "      idxv\n",
        "  ).permute(0, 2, 1)\n",
        "  return idxv\n",
        "\n",
        "def _ibmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  v: N x d_v x d_valv\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu, d_valv = u.shape[-1], v.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxu = poly_norm(idxu, 1) # / (d_u)\n",
        "  # idxv = poly_norm(idxv, 1) # / (d_v)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: N x d_idx x d_valv\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxv: M x d_u x (d_valv * N)\n",
        "  idxv = (\n",
        "      idxu.reshape(m * d_u, d_idx)\n",
        "      @ idxv.permute(1, 2, 0).reshape(d_idx, d_valv * n)\n",
        "  ).reshape(m, d_u, d_valv * n)\n",
        "  # idxv: M x d_valu x (d_valv * N)\n",
        "  idxv = torch.bmm(u.permute(0, 2, 1), idxv)\n",
        "  if d_valv == 1:\n",
        "    # idxv: M x N x d_valu\n",
        "    idxv = idxv.reshape(m, d_valu, n).permute(0, 2, 1)\n",
        "  else:\n",
        "    # idxv: M x N x d_valu or error\n",
        "    idxv = idxv.reshape(m, d_valu, d_valv, n).permute(0, 3, 1, 2)\n",
        "    idxv = torch.diagonal(idxv, dim1=2, dim2=3)\n",
        "  return idxv\n",
        "\n",
        "def _fbmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Fast Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxu: M x d_val x d_idx\n",
        "  # idxv: N x d_idx x d_val\n",
        "  idxu = torch.bmm(u.permute(0, 2, 1), idxu)\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxu: M x N x d_val\n",
        "  idxu = (\n",
        "    (\n",
        "        idxu.reshape(m * d_val, d_idx)\n",
        "        @ (\n",
        "            idxv\n",
        "            .permute(0, 2, 1)\n",
        "            .reshape(n * d_val, d_idx)\n",
        "            .T\n",
        "          )\n",
        "    ).reshape(m, d_val, n, d_val)\n",
        "    .permute(0, 2, 1, 3)\n",
        "  )\n",
        "  idxu = torch.diagonal(idxu, dim1=2, dim2=3)\n",
        "  return idxu\n",
        "\n",
        "def batch_mdot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  if d_idx * (d_u + n) < n * d_u * (d_idx + 1):\n",
        "    return _fbmd(u, v, idxu, idxv)\n",
        "  else:\n",
        "    return _ibmd(u, v, idxu, idxv)\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  ###\n",
        "  # idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # mag = 80.0\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv, dim=2)\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), 6)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  from random import randint\n",
        "  bsidx = randint(0, m - 1)\n",
        "  m_ = 1 # m,\n",
        "  idxuv = (\n",
        "      idxu[bsidx].reshape(m_ * d_u, d_idx)\n",
        "      @ idxv.reshape(n * d_v, d_idx).T\n",
        "  )\n",
        "  idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  mag = 80.0\n",
        "  ###\n",
        "  # siter = 6\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), siter)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  #### Tanh seems to work for high-dimensional idx\n",
        "  #### ReLU(x - alpha) / (1.0 - alpha) works for small samples\n",
        "  #### Índices idxu bem espaçados induzem um bom (??) casamento\n",
        "  #### para d_v suficientemente grande e algum alpha\n",
        "  # alpha = 1.0 - 1.0 / d_v # 0.95\n",
        "  # alpha = min(0.999, alpha)\n",
        "  # alpha = min(0.999, max(0.9, 1.0 - 1.0 / d_v))\n",
        "  alpha = 1.0 - 1e-4\n",
        "  idxuv = nn.functional.relu(idxuv - alpha) / (1.0 - alpha)\n",
        "  ###\n",
        "  idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # from random import randint\n",
        "  # bsidx = randint(0, m - 1)\n",
        "  # m_ = 1\n",
        "  # mag = 80.0\n",
        "  # # idxu = log_sinkhorn(mag * idxu[bsidx].unsqueeze(0), 6)\n",
        "  # # idxv = log_sinkhorn(mag * idxv, 6)\n",
        "  # idxu = nn.functional.softmax(mag * idxu[bsidx].unsqueeze(0), dim=1)\n",
        "  # idxv = nn.functional.softmax(mag * idxv, dim=1)\n",
        "  # idxuv = idxu.reshape(m_ * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    if dim == -1:\n",
        "      dim = len(mt.data.shape) - 1\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    ###\n",
        "    # _nsbmd\n",
        "    # _rdot\n",
        "    # _knndot\n",
        "    # _fbmd\n",
        "    # _ibmd, _mbmd\n",
        "    # batch_mdot\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    # ###\n",
        "    # _aidx = aidx.mean(dim=-2, keepdim=True)\n",
        "    # _bidx = bidx.mean(dim=-2, keepdim=True)\n",
        "    # onesa = torch.ones(_aidx.shape).to(_aidx.device)\n",
        "    # onesb = torch.ones(_bidx.shape).to(_bidx.device)\n",
        "    # midx = (\n",
        "    #     _nsbmd(_aidx, onesb, _aidx, _bidx)\n",
        "    #     + _nsbmd(onesa, _bidx, _aidx, _bidx)\n",
        "    # ) / 2.0\n",
        "    ###\n",
        "    _aidx = aidx.mean(dim=-2, keepdim=True)\n",
        "    _bidx = bidx.mean(dim=-2, keepdim=True)\n",
        "    _bnorm = _bidx.norm(dim=-1)\n",
        "    _bidx *= 0.0\n",
        "    _bidx[:, :, 2] = _bnorm\n",
        "    _aidx[:, :, 2] = 0.0\n",
        "    # midx = (\n",
        "    #     _aidx + _bidx.permute(1, 0, 2)\n",
        "    # ) / 2.0\n",
        "    # midx = (_aidx + _bidx.permute(1, 0, 2))\n",
        "    __bidx = (\n",
        "        torch.arange(1, _bidx.shape[0] + 1)\n",
        "        .to(_bidx.device)\n",
        "        .reshape(1, _bidx.shape[0], 1)\n",
        "        / (_bidx.shape[0])\n",
        "    )\n",
        "    midx = _aidx + 1e-3 * __bidx\n",
        "    ###\n",
        "    # midx = _icbmd(aidx, aidx, bidx.sum(dim=-2, keepdim=True)) # / aidx.shape[-2]\n",
        "    ###\n",
        "    # midx = (\n",
        "    #     _nsbmd(aidx, 1.0 - bidx, aidx, bidx)\n",
        "    #     + _nsbmd(1.0 - aidx, bidx, aidx, bidx)\n",
        "    # )\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "def idxhood(xidx, ws, stride=None, num_sets=None, sample=False):\n",
        "  xidx = xidx.reshape(-1, xidx.shape[-1]).cpu().detach().numpy()\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  nneigh = min(nneigh, len(xidx))\n",
        "  neigh = NearestNeighbors(\n",
        "      n_neighbors=nneigh,\n",
        "      # algorithm=\"brute\",\n",
        "      # metric=\"minkowski\",\n",
        "      # p=1,\n",
        "  )\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  if sample:\n",
        "    if num_sets is None:\n",
        "      num_sets = (len(xidx) + (len(xidx) % stride)) // stride\n",
        "    num_sets = min(num_sets, len(xidx))\n",
        "    subidx = np.random.choice(len(xidx), size=num_sets, replace=False)\n",
        "    all_hoods = neigh.kneighbors(\n",
        "        xidx[subidx], return_distance=False,\n",
        "    )\n",
        "    all_hoods = set(tuple(sorted(hood)) for hood in all_hoods)\n",
        "    all_hoods = np.array(list(all_hoods))\n",
        "    ###\n",
        "    # hood_means = xidx[all_hoods.reshape(-1)]\n",
        "    # hood_means = hood_means.reshape(len(all_hoods), nneigh, xidx.shape[-1])\n",
        "    # hood_mins = hood_means.min(axis=1)[:, None, :]\n",
        "    # hood_maxes = hood_means.max(axis=1)[:, None, :]\n",
        "    # hood_norm = (hood_means - hood_mins) / (hood_maxes - hood_mins + 1e-6)\n",
        "    # hood_norm_ = hood_norm.mean(axis=1)\n",
        "    # hood_consensus = np.median(hood_norm_, axis=0)\n",
        "    # hood_filter = (np.linalg.norm(hood_norm_ - hood_consensus[None, :], axis=1) < 1e-4)\n",
        "    # all_hoods = all_hoods[hood_filter]\n",
        "    ###\n",
        "  else:\n",
        "    _neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "    _ids = np.array([[pos // img_dim, pos % img_dim] for pos in range(len(xidx))])\n",
        "    # _ids = np.array([[pos] for pos in range(len(xidx))])\n",
        "    _neigh.fit(_ids)\n",
        "    if stride is None:\n",
        "      stride = len(xidx) // num_sets\n",
        "    all_hoods = _neigh.kneighbors(\n",
        "        _ids[::stride], return_distance=False\n",
        "    ) # .reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCh8kNiFl15G"
      },
      "source": [
        "#### MModule IV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PAqc0d7Kl6Ud"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def _count_params(config):\n",
        "  tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "  n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "  return n_params\n",
        "\n",
        "class MModule4(nn.Module):\n",
        "  def __init__(self, config, idx_dim=3, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    assert len(config[\"features\"][\"sets\"]) > 0\n",
        "    assert len(config[\"params\"][\"sets\"]) > 0\n",
        "    assert len(config[\"features\"][\"sets\"]) == len(config[\"params\"][\"sets\"])\n",
        "    for key in [\"sets\", \"samples\"]:\n",
        "      assert len(config[\"features\"][key]) == len(config[\"features\"][\"sets\"])\n",
        "      assert len(config[\"params\"][key]) == len(config[\"params\"][\"sets\"])\n",
        "    self._idx_dim = idx_dim\n",
        "    self.device = device\n",
        "    self._config = config\n",
        "    self._config[\"params\"][\"samples\"] = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "    self._ch0 = config[\"features\"][\"pre-set\"]\n",
        "    self._is_conv = config[\"params\"][\"is conv\"]\n",
        "    self._feat_samples = config[\"features\"][\"samples\"]\n",
        "    self._n_params = _count_params(config)\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (self._n_params,), idx_dim, device\n",
        "    )\n",
        "    # self.activation = nn.ELU()\n",
        "    self.activation = nn.ReLU()\n",
        "    # self.activation = nn.LeakyReLU()\n",
        "    self._probe = nn.Linear(self._feat_samples[-1], 10).to(device)\n",
        "    self._num_fwd = 0\n",
        "    ###\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    self._bn_w = nn.Parameter(torch.ones(n_layers - 1).to(device))\n",
        "    self._bn_b = nn.Parameter(torch.zeros(n_layers - 1).to(device))\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _mag = 0.1 # 2.0 # 1000.0 * 0.01\n",
        "    _W_idx = _mag * torch.rand((*shape, idxdim), device=device) - (_mag / 2.0)\n",
        "    _std = 0.01\n",
        "    _W = nn.Parameter(\n",
        "        _std * torch.randn(shape, device=device)\n",
        "    )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def _put_one(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    device = x.data.device\n",
        "    assert device == x.idx.device\n",
        "    ones_idx = self._ones_idx.repeat(n, 1, 1)\n",
        "    dummy_ones = MTensor(\n",
        "        values=torch.ones(n, 1).to(device),\n",
        "        indices=ones_idx,\n",
        "    )\n",
        "    x = MTensor.cat([x, dummy_ones], dim=1)\n",
        "    return x\n",
        "\n",
        "  def _reinit_indices(self, pool, wl, wr, samples, psamples):\n",
        "    \"\"\"\n",
        "    pool: N x mshape\n",
        "    params: param_shape\n",
        "    \"\"\"\n",
        "    params = self.MW[wl: wr]\n",
        "    idx_dim = pool.idx.shape[-1]\n",
        "    assert params.idx.shape[-1] == idx_dim\n",
        "    assert psamples <= samples\n",
        "    pool_idx = pool.idx.reshape(-1, samples, idx_dim)\n",
        "    pool_idx = minmax_normalize(pool_idx, eps=0.0)\n",
        "    pool_idx = torch.nan_to_num(pool_idx)\n",
        "    ###\n",
        "    # pool_idx = minmax_normalize(pool_idx).reshape(-1, idx_dim)\n",
        "    # idx_max = pool_idx.max(dim=0, keepdim=True)[0]\n",
        "    # idx_min = pool_idx.min(dim=0, keepdim=True)[0]\n",
        "    # idx_itv = idx_max - idx_min\n",
        "    # n_samples = len(params.idx.reshape(-1, idx_dim))\n",
        "    # sampled_idx = torch.rand((n_samples, idx_dim), device=pool.data.device)\n",
        "    # sampled_idx = sampled_idx * idx_itv + idx_min\n",
        "    ###\n",
        "    device = pool.data.device\n",
        "    m = pool_idx.shape[0]\n",
        "    n_samples = wr - wl\n",
        "    n = n_samples // psamples\n",
        "    fset_iidx = torch.randint(0, m - 1, (n, 1)).repeat(1, psamples).to(device)\n",
        "    fset_iidx = fset_iidx * samples\n",
        "    # fsamp_iidx = torch.randint(0, samples - 1, (n, psamples)).to(device)\n",
        "    fsamp_iidx = torch.randperm(samples)[:psamples].reshape(1, psamples)\n",
        "    fsamp_iidx = fsamp_iidx.repeat(n, 1).to(device)\n",
        "    _iidx = (fset_iidx + fsamp_iidx).long().reshape(-1)\n",
        "    sampled_idx = pool_idx.reshape(-1, idx_dim)[_iidx]\n",
        "    sampled_idx = sampled_idx.reshape(n_samples, idx_dim)\n",
        "    self.MW.idx[wl: wr] = sampled_idx\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    ###\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    n = x.data.shape[0]\n",
        "    param_sets = deepcopy(self._config[\"params\"][\"sets\"])\n",
        "    param_samples = deepcopy(self._config[\"params\"][\"samples\"])\n",
        "    feat_sets = deepcopy(self._config[\"features\"][\"sets\"])\n",
        "    feat_samples = deepcopy(self._config[\"features\"][\"samples\"])\n",
        "    ###\n",
        "    # self._curr_sets = feat_sets\n",
        "    # self._curr_samples = feat_samples\n",
        "    ###\n",
        "    self.all_pools = []\n",
        "    self.all_samples = []\n",
        "    pool = x\n",
        "    pool = MTensor.reshape(pool, (n, self._ch0, -1))\n",
        "    pool = MTensor.permute(pool, (0, 2, 1))\n",
        "    wl, wr = 0, param_sets[0] * param_samples[0]\n",
        "    for step in range(n_layers):\n",
        "      self.all_pools.append(pool[:4])\n",
        "      if self._is_conv[step]:\n",
        "        idx_slice = pool.idx[0, :, 0]\n",
        "      else:\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        # idx_slice = pool.idx[0]\n",
        "        idx_slice = pool.idx[0, :, :3]\n",
        "      # pool = MTensor.reshape(pool, (n, -1))\n",
        "      # idx_slice = pool.idx[0]\n",
        "      idxx = idxhood(\n",
        "          idx_slice,\n",
        "          feat_samples[step],\n",
        "          num_sets=feat_sets[step],\n",
        "          sample=True,\n",
        "      )\n",
        "      ###\n",
        "      feat_sets[step] = idxx.shape[0]\n",
        "      feat_samples[step] = idxx.shape[1]\n",
        "      idxx = idxx.reshape(-1)\n",
        "      ###\n",
        "      # pool: N x (num_windows * window_volume)\n",
        "      pool = pool[:, idxx]\n",
        "      ###\n",
        "      if self._num_fwd == 0:\n",
        "        self._reinit_indices(pool, wl, wr, feat_samples[step], param_samples[step])\n",
        "      ###\n",
        "      self.all_samples.append(pool[:4])\n",
        "      ###\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[wl: wr],\n",
        "          (param_sets[step], param_samples[step])\n",
        "      )\n",
        "      ###\n",
        "      # pool: (N * num_windows) x sets\n",
        "      # pool = MTensor.reshape(pool, (-1, feat_samples[step])) @ mw\n",
        "      ### o 1 do bias parece que atrapalha\n",
        "      # pool = (\n",
        "      #     self._put_one(MTensor.reshape(\n",
        "      #         pool, (n * feat_sets[step], -1)\n",
        "      #     ))\n",
        "      #     @ mw\n",
        "      # )\n",
        "      ###\n",
        "      maromba_only = False\n",
        "      if maromba_only or (step < n_layers - 1):\n",
        "        pool = (\n",
        "            MTensor.reshape(\n",
        "                pool, (n * feat_sets[step], -1)\n",
        "            )\n",
        "            @ mw\n",
        "        )\n",
        "      else:\n",
        "        pool = self._probe(pool.data.reshape(n, -1))\n",
        "        pool = MTensor(\n",
        "            pool,\n",
        "            torch.zeros((*pool.shape, self._idx_dim)).to(pool.device)\n",
        "        )\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, -1))\n",
        "      # pool: N x (num_windows * sets)\n",
        "      if step < n_layers - 1:\n",
        "        pool.data = self.activation(pool.data)\n",
        "        # pool.data = nn.functional.batch_norm(\n",
        "        #     pool.data,\n",
        "        #     pool.data.mean(dim=0).detach(),\n",
        "        #     pool.data.var(dim=0).detach(),\n",
        "        #     training=True,\n",
        "        #     weight=self._bn_w[step],\n",
        "        #     bias=self._bn_b[step],\n",
        "        # )\n",
        "      else:\n",
        "        break\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, feat_sets[step], param_sets[step]))\n",
        "      ###\n",
        "      nxt_step = step + 1\n",
        "      next_wr = wr + param_sets[nxt_step] * param_samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    self._num_fwd += 1\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  # y_pred = 10.0 * y_pred.data ### WHY 10x?\n",
        "  y_pred = y_pred.data ### WHY 10x?\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 3 # 500 # 3 # 10\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, chs=channels, offset=1, d=idx_dim, mag=2.0) # 1000.0\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_m1YvjxBdj9"
      },
      "source": [
        "### Visualizações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UZ4DrI6mBn39"
      },
      "outputs": [],
      "source": [
        "def plot_features(x: MTensor):\n",
        "  \"\"\"\n",
        "  x.data: in_dim\n",
        "  x.idx:  in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  n, idx_dim = x.idx.shape\n",
        "  assert x.data.shape == (n,)\n",
        "  tidx = x.idx.cpu().detach().numpy()\n",
        "  tdata = x.data.cpu().detach().numpy()\n",
        "  plot_df = pd.DataFrame(\n",
        "      {\n",
        "          \"x\": tidx[:, 0],\n",
        "          \"y\": tidx[:, 1],\n",
        "          \"z\": tidx[:, 2],\n",
        "          \"val\": tdata,\n",
        "      }\n",
        "  )\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=\"val\")\n",
        "  fig.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQbo5JtHw3bq"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGn5VTZPw-1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "9fc3a994-e7ad-4f69-9e5a-0d19f22a8bda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "hidden_dim = 10\n",
        "start_mode = False\n",
        "valid_mode = False\n",
        "index_mode = False\n",
        "\n",
        "# TODO: Visualize conv layer output\n",
        "###\n",
        "# config = {\n",
        "#     \"features\": {\n",
        "#         \"pre-set\": channels,\n",
        "#         \"sets\":    [784, 2*784, 4*784, 1,],\n",
        "#         \"samples\": [9, 18, 36, 2904,],\n",
        "#     },\n",
        "#     \"params\": {\n",
        "#         \"sets\":    [2, 4, 6, num_classes,],\n",
        "#         \"samples\": [9, 18, 36, 784,],\n",
        "#         \"is conv\": [False, False, False, False]\n",
        "#     },\n",
        "# }\n",
        "###\n",
        "### Complete indices require not undersampling\n",
        "config = {\n",
        "    \"features\": {\n",
        "        \"pre-set\": channels,\n",
        "        \"sets\":    [1024, 1024, 1,],\n",
        "        \"samples\": [9, 36, 5176,],\n",
        "    },\n",
        "    \"params\": {\n",
        "        \"sets\":    [4, 8, num_classes,],\n",
        "        \"samples\": [9, 36, 1024,],\n",
        "        \"is conv\": [False, False, False]\n",
        "    },\n",
        "}\n",
        "\n",
        "tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "conv_params = int(np.dot(config[\"params\"][\"sets\"][:-1], tot_samples[:-1]))\n",
        "n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "\n",
        "if start_mode:\n",
        "  model = MModule4(\n",
        "      config=config,\n",
        "      idx_dim=idx_dim,\n",
        "      device=device,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  # optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "  widx0 = model.MW.idx[:, :conv_params].clone().detach()\n",
        "  train_log = {\n",
        "      \"train loss\": [],\n",
        "      \"eval loss\": [],\n",
        "      \"acc\": [],\n",
        "      \"set\": [],\n",
        "      \"epoch\": [],\n",
        "  }\n",
        "  epoch = 0\n",
        "\n",
        "num_epochs = 7200\n",
        "epoch_len = 10 # 60\n",
        "\n",
        "while epoch < num_epochs:\n",
        "  epoch += 1\n",
        "  ###\n",
        "  # widx_diff = (model.MW.idx[:, :conv_params] - widx0)\n",
        "  # print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    ###\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    _loss = loss.item()\n",
        "    # loss += 1e-1 * torch.cat(model._penalties, dim=0).sum()\n",
        "    ###\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(_loss)\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    metric_cols = [\"eval loss\", \"acc\"]\n",
        "    set_val = \"eval\"\n",
        "  else:\n",
        "    metric_cols = [\"train loss\",]\n",
        "    set_val = \"train\"\n",
        "  if index_mode:\n",
        "    _layer = 2\n",
        "    _batchidx = 0\n",
        "    feat_map = True\n",
        "    if feat_map:\n",
        "      pool = model.all_pools[_layer]\n",
        "      pool = MTensor.reshape(pool[_batchidx], (-1,))\n",
        "      display.clear_output(wait=True)\n",
        "      plot_features(pool)\n",
        "    else:\n",
        "      pool = model.all_samples[_layer - 1]\n",
        "      _shape = (\n",
        "          model._curr_sets[_layer - 1],\n",
        "          model._curr_samples[_layer - 1],\n",
        "      )\n",
        "      _set = (model._curr_sets[_layer - 1]) // 2\n",
        "      pool = MTensor.reshape(pool[_batchidx], _shape)[_set]\n",
        "      display.clear_output(wait=True)\n",
        "      plot_features(pool)\n",
        "    from time import sleep\n",
        "    sleep(3)\n",
        "  else:\n",
        "    group_cols = [\"epoch\"] + metric_cols\n",
        "    df_train = pd.DataFrame(train_log)\n",
        "    df_train = df_train[df_train[\"set\"] == set_val]\n",
        "    display.clear_output(wait=True)\n",
        "    (\n",
        "      df_train[group_cols]\n",
        "      .groupby(\"epoch\")\n",
        "      .agg(lambda x: x.median(skipna=True))\n",
        "      .reset_index()\n",
        "      .sort_values(\"epoch\", ascending=True)\n",
        "      .tail(30)[metric_cols]\n",
        "      .plot(figsize=(16, 3), grid=True)\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "n5Hm-pCJqjTm"
      },
      "outputs": [],
      "source": [
        "# tidx = idxu.reshape(32, -1, 3)[0].cpu().detach().numpy()\n",
        "# tidx = idxu.reshape(32, -1, 18, 3)[0, 0].cpu().detach().numpy()\n",
        "# tidx = idxv.reshape(-1, 3).cpu().detach().numpy()\n",
        "## tidx = model.W_idx[0, :18].cpu().detach().numpy()\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "##\n",
        "# phi = idxu[0] @ idxv[0].T\n",
        "# import seaborn as sns\n",
        "# sns.heatmap(phi.cpu().detach().numpy()); plt.show()\n",
        "##\n",
        "# iidx = xidx[(all_hoods[hood_filter]).reshape(-1)].reshape(sum(hood_filter), -1, 3)\n",
        "# iidx_ = iidx.mean(axis=1)\n",
        "##\n",
        "\n",
        "def plot_match(idxu, idxv, _iu=0, _iv=0):\n",
        "  iu = idxu.cpu().detach().numpy()\n",
        "  iv = idxv.cpu().detach().numpy()\n",
        "  tidx = np.concatenate([iu[0], iv[0]], axis=0)\n",
        "  colors = [0] * iu.shape[1] + [1] * iv.shape[1]\n",
        "  plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=colors); fig.show();\n",
        "\n",
        "def plot_phi(idxuv, _iu=0, _iv=0):\n",
        "  import seaborn as sns\n",
        "  sns.heatmap(idxuv[_iu, :, :, _iv].detach().numpy())\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahDaeazcapob"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "RGCfrrmCXap_",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "kTfYY3SQXNJF",
        "1SknOTQ7O9BS",
        "vCh8kNiFl15G",
        "QQRFtDATXUmH",
        "039kGqbPXp4d",
        "8_m1YvjxBdj9"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMcNtj6uKTmuOPZwIUuADcb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}