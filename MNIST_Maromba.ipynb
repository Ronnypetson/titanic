{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjqzQvulwB9yAMJkMaWDuP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pylab as plt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "def breakpoint():\n",
        "    Pdb().set_trace()\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "RGCfrrmCXap_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr = ToTensor()\n",
        "\n",
        "def _transform(x):\n",
        "  return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "\n",
        "bsize = 32\n",
        "\n",
        "MNIST_train_data = MNIST(\n",
        "    'MNIST_root/',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = MNIST(\n",
        "    'MNIST_root_test/',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "metadata": {
        "id": "j6dxGxcHAx5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sin_arr(d, idx, rows):\n",
        "  _x = (np.arange(0, d) / d) * (4 * np.pi * (1 + idx / rows))\n",
        "  return np.sin(_x)\n",
        "\n",
        "def _cos_arr(d, idx, cols):\n",
        "  _x = (np.arange(0, d) / d) * (4 * np.pi * (1 + idx / cols))\n",
        "  return np.cos(_x)\n",
        "\n",
        "def _ind_arr(d, idx, bins):\n",
        "  _x = np.zeros(d)\n",
        "  idx = (d * idx) // bins\n",
        "  _x[idx] = 1.0\n",
        "  return _x\n",
        "\n",
        "def _2ind_arr(d, idx, bins):\n",
        "  return 2 * _ind_arr(d, idx, bins)\n",
        "\n",
        "def _bincat2d(rows, cols, d=32):\n",
        "  bitsr = len(format(rows, \"0b\"))\n",
        "  bitsc = len(format(cols, \"0b\"))\n",
        "  assert 2 * (bitsr + bitsc) <= d\n",
        "  idx = np.zeros((rows, cols, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      binr = format(row, f\"0{bitsr}b\")\n",
        "      binc = format(col, f\"0{bitsc}b\")\n",
        "      for pos, bit in enumerate(binr):\n",
        "        idxidx = 2 * pos + int(bit)\n",
        "        idx[row, col, idxidx] = 1.0\n",
        "      for pos, bit in enumerate(binc):\n",
        "        idxidx = 2 * bitsr + 2 * pos + int(bit)\n",
        "        idx[row, col, idxidx] = 1.0\n",
        "  return idx\n",
        "\n",
        "def _cat2d(rows, cols, d=32):\n",
        "  \"\"\"\n",
        "  Index in the log-softmax scale.\n",
        "  After sotmax (in the partition dimension)\n",
        "  -inf --> 0\n",
        "  1.0  --> 1\n",
        "  \"\"\"\n",
        "  assert rows + cols <= d\n",
        "  inf = 1.0\n",
        "  idx = np.zeros((rows, cols, d)) - inf\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      idx[row, col, row] = 1.0\n",
        "      idx[row, col, rows + col] = 1.0\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx\n",
        "\n",
        "def _posenc(shape, f_row, f_col, d=32, nonlin=None):\n",
        "  \"\"\"\n",
        "  3D Positional encodings (f_row(row) + f_col(col))\n",
        "  \"\"\"\n",
        "  assert len(shape) == 2\n",
        "  rows, cols = shape\n",
        "  idx_sin = np.zeros((rows, d))\n",
        "  idx_cos = np.zeros((cols, d))\n",
        "  for idx in range(rows):\n",
        "    idx_sin[idx] = f_row(d, idx, rows)\n",
        "  for idx in range(cols):\n",
        "    idx_cos[idx] = f_col(d, idx, cols)\n",
        "  idx_sin = torch.from_numpy(idx_sin)\n",
        "  idx_cos = torch.from_numpy(idx_cos)\n",
        "  idx = (\n",
        "      idx_sin.reshape((rows, 1, d)).repeat(1, cols, 1)\n",
        "      + idx_cos.reshape((1, cols, d)).repeat(rows, 1, 1)\n",
        "  )\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  if nonlin:\n",
        "    idx = nonlin(idx)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binidx = _cat2d(5, 2, d=10)\n",
        "# binidx @ binidx.T\n",
        "# np.exp(binidx[0]) / np.exp(binidx[0]).sum()"
      ],
      "metadata": {
        "id": "taSPF5ITc5vQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rows, cols, d = 5, 5, 32\n",
        "# pos = _posenc(\n",
        "#     (rows, cols),\n",
        "#     _ind_arr, # _sin_arr,\n",
        "#     _2ind_arr, # _cos_arr,\n",
        "#     d=d,\n",
        "#     nonlin=lambda x: x / 3.0, # torch.sigmoid\n",
        "# ).reshape(rows, cols, d)\n",
        "# fig, axs = plt.subplots(nrows=rows, ncols=cols, layout=None)\n",
        "# for row in range(rows):\n",
        "#   for col in range(cols):\n",
        "#     axs[row][col].plot(range(d), pos[row, col].numpy())\n",
        "# plt.show()\n",
        "# print(pos[0] @ pos[1].T)"
      ],
      "metadata": {
        "id": "fyWPtI28l-pp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x, y = MNIST_train_data[0]\n",
        "# plt.imshow(np.array(x.reshape(28, 28))), y"
      ],
      "metadata": {
        "id": "KS12YlllA3xE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe Tensor Maromba"
      ],
      "metadata": {
        "id": "kTfYY3SQXNJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = 28\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def _gbmd(self, u, v, idxu, idxv, kernel=None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    'General Batch Maromba Dot'\n",
        "    Shorter implementation for the 'batch maromba dot' operation.\n",
        "    u: M x d_u\n",
        "    v: N x d_v\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u = u.shape\n",
        "    n, d_v = v.shape\n",
        "    d_idx = idxu.shape[-1]\n",
        "    assert (m, d_u, d_idx) == idxu.shape\n",
        "    assert (n, d_v, d_idx) == idxv.shape\n",
        "    if kernel:\n",
        "      idxu = kernel(idxu, self._idx_part)\n",
        "      idxv = kernel(idxv, self._idx_part)\n",
        "    # uidxu: M x d_idx\n",
        "    # vidxv: N x d_idx\n",
        "    uidxu = torch.bmm(u.reshape(m, 1, d_u), idxu).squeeze(1)\n",
        "    vidxv = torch.bmm(v.reshape(n, 1, d_v), idxv).squeeze(1)\n",
        "    dot = uidxu @ vidxv.T\n",
        "    ### Under experimentation\n",
        "    normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "    dot = dot / normalizer\n",
        "    ###\n",
        "    return dot\n",
        "\n",
        "  @staticmethod\n",
        "  def _soft_kernel(idxu, part_dim):\n",
        "    \"\"\"\n",
        "    idxu: M x d_u x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    assert d_idx % part_dim == 0\n",
        "    norm_idxu = torch.softmax(idxu.reshape(m, d_u, -1, part_dim), dim=-1)\n",
        "    norm_idxu = norm_idxu.reshape(m, d_u, d_idx)\n",
        "    return norm_idxu\n",
        "\n",
        "  def _kernel_idx(self, idxu, idxv, k):\n",
        "    \"\"\"\n",
        "    k: callable: A x B x C -> A x B x C\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    n, d_v, _ = idxv.shape\n",
        "    assert d_idx == idxv.shape[-1]\n",
        "    # kidxu: M x d_u x d_idx\n",
        "    # kidxv: N x d_v x d_idx\n",
        "    kidxu = k(idxu, self._idx_part)\n",
        "    kidxv = k(idxv, self._idx_part)\n",
        "    assert kidxu.shape == idxu.shape\n",
        "    assert kidxv.shape == idxv.shape\n",
        "    # ski: (M * N) x d_idx\n",
        "    # skj: (M * N) x d_idx\n",
        "    # norm: M x N x 1\n",
        "    ski = kidxu.sum(dim=1)\n",
        "    skj = kidxv.sum(dim=1)\n",
        "    norm = (ski @ skj.T).unsqueeze(-1)\n",
        "    ski = ski.unsqueeze(1).repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "    skj = skj.unsqueeze(1).repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "    # idxu, kidxu: (M * d_u) x d_idx x 1\n",
        "    # idxv, kidxv: (N * d_v) x d_idx x 1\n",
        "    idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "    idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "    kidxu = kidxu.reshape(m * d_u, d_idx, 1)\n",
        "    kidxv = kidxv.reshape(n * d_v, d_idx, 1)\n",
        "    # sikiT: M x d_idx x d_idx\n",
        "    # sjkjT: N x d_idx x d_idx\n",
        "    sikiT = torch.bmm(idxu, kidxu.permute(0, 2, 1))\n",
        "    sikiT = sikiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "    sjkjT = torch.bmm(idxv, kidxv.permute(0, 2, 1))\n",
        "    sjkjT = sjkjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1)\n",
        "    # sikiT: (M * N) x d_idx x d_idx\n",
        "    # sjkjT: (M * N) x d_idx x d_idx\n",
        "    sikiT = sikiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    sjkjT = sjkjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    # diag_sikiT_skjjT: (M * N) x d_idx\n",
        "    skjjT = sjkjT.permute(0, 2, 1)\n",
        "    diag_sikiT_skjjT = torch.diagonal(torch.bmm(sikiT, skjjT), dim1=1, dim2=2)\n",
        "    diag_sikiT_skjjT = diag_sikiT_skjjT.unsqueeze(-1)\n",
        "    xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski) - diag_sikiT_skjjT\n",
        "    xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "    xor_idx = xor_idx / norm\n",
        "    return xor_idx\n",
        "\n",
        "  def _xor_idx(self, idxu, idxv):\n",
        "    \"\"\"\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    n, d_v, _ = idxv.shape\n",
        "    assert d_idx == idxv.shape[-1]\n",
        "    ### Under experimentation\n",
        "    # idxu = nn.functional.relu(idxu)\n",
        "    # idxv = nn.functional.relu(idxv)\n",
        "    # norm_idxu = torch.softmax(idxu.reshape(m, d_u, -1, self._idx_part), dim=-1)\n",
        "    # norm_idxu = norm_idxu.reshape(m, d_u, d_idx)\n",
        "    # norm_idxv = torch.softmax(idxv.reshape(n, d_v, -1, self._idx_part), dim=-1)\n",
        "    # norm_idxv = norm_idxv.reshape(n, d_v, d_idx)\n",
        "    normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "    normalizer = normalizer.unsqueeze(-1)\n",
        "    ###\n",
        "    # idxu: (M * d_u) x d_idx x 1\n",
        "    # idxv: (N * d_v) x d_idx x 1\n",
        "    idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "    idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "    # siiT: M x d_idx x d_idx\n",
        "    # sjjT: N x d_idx x d_idx\n",
        "    siiT = torch.bmm(idxu, idxu.permute(0, 2, 1))\n",
        "    siiT = siiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "    sjjT = torch.bmm(idxv, idxv.permute(0, 2, 1))\n",
        "    sjjT = sjjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1) ###\n",
        "    # siiT: (M * N) x d_idx x d_idx\n",
        "    # sjjT: (M * N) x d_idx x d_idx\n",
        "    siiT = siiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    sjjT = sjjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    # si: (M * N) x d_idx x 1\n",
        "    # sj: (M * N) x d_idx x 1\n",
        "    idxu = idxu.reshape(m, d_u, d_idx)\n",
        "    idxv = idxv.reshape(n, d_v, d_idx)\n",
        "    si = idxu.sum(dim=1).unsqueeze(1)\n",
        "    si = si.repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "    sj = idxv.sum(dim=1).unsqueeze(0)\n",
        "    sj = sj.repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "    diag_siiT_sjjT = torch.diagonal(torch.bmm(siiT, sjjT), dim1=1, dim2=2)\n",
        "    diag_siiT_sjjT = diag_siiT_sjjT.unsqueeze(-1)\n",
        "    # xor_idx = torch.bmm(siiT, sj) + torch.bmm(sjjT, si) - 2 * diag_siiT_sjjT\n",
        "    xor_idx = torch.bmm(siiT, sj) + torch.bmm(sjjT, si) - diag_siiT_sjjT\n",
        "    # xor_idx = xor_idx.reshape(m, n, d_idx) / d_u ### TODO: check this\n",
        "    xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "    # xor_idx = torch.sigmoid(xor_idx)\n",
        "    ### Under experimentation\n",
        "    xor_idx = xor_idx / normalizer\n",
        "    ###\n",
        "    return xor_idx\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns 'mdot'\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    mdot = self._gbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1]),\n",
        "        b.data.reshape(-1, b.data.shape[-1]),\n",
        "        aidx,\n",
        "        bidx,\n",
        "        kernel=MTensor._soft_kernel\n",
        "    )\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # midx = self._xor_idx(aidx, bidx)\n",
        "    midx = self._kernel_idx(aidx, bidx, MTensor._soft_kernel)\n",
        "    midx = midx.reshape(apre + bpre + (d_idx,))\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    # adata = self.data.reshape(n * out_a, in_a).unsqueeze(1)\n",
        "    # aidx = self.idx.reshape(n * out_a, in_a, d_idx)\n",
        "    # bdata = b.data.reshape(n * out_b, in_b).unsqueeze(1)\n",
        "    # bidx = b.idx.reshape(n * out_b, in_b, d_idx)\n",
        "    # # aidxa: N x out_a x d_idx\n",
        "    # # bidxb: N x out_b x d_idx\n",
        "    # aidxa = torch.bmm(adata, aidx).squeeze(1).reshape(n, out_a, d_idx)\n",
        "    # bidxb = torch.bmm(bdata, bidx).squeeze(1).reshape(n, out_b, d_idx)\n",
        "    # # dot: N x out_a x out_b\n",
        "    # dot = torch.bmm(aidxa, bidxb.permute(0, 2, 1))\n",
        "    # # idx = self._xor_idx(aidx, bidx)\n",
        "    # return dot\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ],
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe do Módulo Treinável"
      ],
      "metadata": {
        "id": "yGg59zEqYGe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MModule(nn.Module):\n",
        "  def __init__(self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    _W_idx = torch.randn((1, n_params, idx_dim), device=device)\n",
        "    # _W_idx = nn.functional.relu(_W_idx)\n",
        "    # _W_idx = _W_idx / _W_idx.max()\n",
        "    self.W_idx = nn.Parameter(_W_idx)\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    self.activation = nn.ReLU()\n",
        "    # self.activation = nn.Tanh() # nn.Sigmoid()\n",
        "\n",
        "  def _msample(self, x: MTensor, n_sets, n_samples):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "\n",
        "    Returns\n",
        "    x_sets: N x n_sets x n_samples\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    x_sets = []\n",
        "    for _ in range(n_sets):\n",
        "      idx = np.random.choice(in_dim, n_samples, replace=False)\n",
        "      idx = torch.tensor(idx).long()\n",
        "      # x_sampled.data: N x 1 x n_samples\n",
        "      x_sampled = MTensor.unsqueeze(x[:, idx], dim=1)\n",
        "      x_sets.append(x_sampled)\n",
        "    # x_sets.data: N x n_sets x n_samples\n",
        "    x_sets = MTensor.cat(x_sets, dim=1)\n",
        "    return x_sets\n",
        "\n",
        "  def _W_step(\n",
        "      self,\n",
        "      x: MTensor,\n",
        "      W: MTensor,\n",
        "      sets,\n",
        "      samples,\n",
        "      random=True,\n",
        "      activation=True):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # Put 1 into x\n",
        "    one = MTensor(\n",
        "        torch.ones(n, 1).to(self.device),\n",
        "        torch.ones(n, 1, idx_dim).to(self.device),\n",
        "    )\n",
        "    x = MTensor.cat([x, one], dim=1)\n",
        "    # Sample W\n",
        "    # W_sets = []\n",
        "    # for _ in range(self.sets):\n",
        "    #   idx = np.random.choice(self.n_params, self.samples, replace=False)\n",
        "    #   idx = torch.tensor(idx).long()\n",
        "    #   W_sets.append(self.MW[:, idx])\n",
        "    # W_sets = MTensor.cat(W_sets, dim=0)\n",
        "    # W_sets = self._msample(self.MW, self.sets, self.samples)\n",
        "    if random:\n",
        "      W_sets = self._msample(W, sets, samples)\n",
        "      W_sets = MTensor.squeeze(W_sets, 0)\n",
        "    else:\n",
        "      W_sets = MTensor.reshape(W, (sets, samples))\n",
        "    # mdot: N x sets\n",
        "    mdot = x @ W_sets\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    return mdot\n",
        "\n",
        "  def _pool_step(self, x: MTensor, sets, samples, activation=True):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # x0: N x 1 x samples\n",
        "    # x1: N x sets x samples\n",
        "    x0 = self._msample(x, 1, samples)\n",
        "    x1 = self._msample(x, sets, samples)\n",
        "    # mdot.data: N x sets\n",
        "    mdot = x0 * x1\n",
        "    mdot = MTensor.squeeze(mdot, 1)\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    return mdot\n",
        "\n",
        "  def _forward(self, x: MTensor, n_steps=2):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    assert n_steps > 0\n",
        "    # pool: N x sets\n",
        "    activate = (n_steps > 1)\n",
        "    pool = self._W_step(x, activation=activate)\n",
        "    for step in range(1, n_steps):\n",
        "      # pool: N x (in_dim + step * sets)\n",
        "      activate = (step < n_steps - 1)\n",
        "      if step % 2 == 0:\n",
        "        pool_new = self._pool_step(pool, activation=activate)\n",
        "      else:\n",
        "        pool_new = self._W_step(pool, activation=activate)\n",
        "      pool = MTensor.cat([pool, pool_new], dim=1)\n",
        "    return pool_new\n",
        "  \n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    pool = x\n",
        "    # Pdb().set_trace()\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      pool = self._W_step(\n",
        "          pool,\n",
        "          self.MW[:, wl: wr],\n",
        "          self.sets[step],\n",
        "          self.samples[step],\n",
        "          random=False,\n",
        "          activation=activate\n",
        "      )\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ],
      "metadata": {
        "id": "Oipx_P9qYUUb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função de Custo"
      ],
      "metadata": {
        "id": "QQRFtDATXUmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, 28)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  # y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)"
      ],
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino"
      ],
      "metadata": {
        "id": "039kGqbPXp4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "rows, cols = 28, 28\n",
        "idx_dim = rows + cols + 28\n",
        "\n",
        "# template_x_idx = _posenc(\n",
        "#     (rows, cols),\n",
        "#     _ind_arr,\n",
        "#     _2ind_arr,\n",
        "#     d=idx_dim,\n",
        "#     nonlin=lambda x: x / 3.0,\n",
        "# )\n",
        "template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MModule(\n",
        "    n_params=784 * 100 + 100 * 10,\n",
        "    idx_dim=idx_dim,\n",
        "    samples=[784, 100],\n",
        "    sets=[100, 10],\n",
        "    device=device\n",
        ")\n",
        "optimizer = Adam(model.parameters(), lr=1e0)\n",
        "\n",
        "num_epochs = 180\n",
        "batch_size = 32\n",
        "epoch_len = 60 # len(MNIST_train_data) // batch_size\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  # for x, y in iter(train_data_loader):\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    # Pdb().set_trace()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  model.eval()\n",
        "  for x, y in iter(test_data_loader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    acc = maromba_accuracy(y, y_pred)\n",
        "    train_log[\"eval loss\"].append(loss.item())\n",
        "    train_log[\"train loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(acc.item())\n",
        "    train_log[\"set\"].append(\"eval\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  df_train[group_cols].groupby(\"epoch\").agg(lambda x: x.median(skipna=True)).plot(figsize=(24, 4))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "HNheVxvNNK30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "f9733d9a-427f-4460-9dc6-84b2f8e2339b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_log)"
      ],
      "metadata": {
        "id": "jPdqOyP9Sdx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred.idx = nn.functional.softmax(y_pred.idx, dim=-1)\n",
        "y_pred.idx = MTensor._soft_kernel(y_pred.idx, 28)\n",
        "\n",
        "index_match = torch.bmm(y_pred.idx, y.idx.permute(0, 2, 1))\n",
        "# index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "# y_true_match: N x 1 x d_out(pred)\n",
        "# y_pred_match: N x 1 x d_out(true)\n",
        "y_pred_match = torch.bmm(y_pred.data.unsqueeze(1), index_match)"
      ],
      "metadata": {
        "id": "mGSWz0GB9api"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_match[0].sum(dim=-1)\n",
        "# y_pred_match\n",
        "# torch.argmax(y.data, dim=-1)"
      ],
      "metadata": {
        "id": "cR6CRFgWCpFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "loss = ce(y_pred_match.squeeze(1), torch.argmax(y.data, dim=-1))\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "lAQUfdi_9wUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a4de48-eb2d-4baf-ee51-afd489f0b017"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_match[0]\n",
        "# index_match[0]\n",
        "# y_pred.idx[0]\n",
        "# y.idx[0]\n",
        "# y_pred.data[0: 5]\n",
        "[param.grad.max() for param in model.parameters()]"
      ],
      "metadata": {
        "id": "teVFiPYE_KrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa88803d-f5c1-4986-e76c-67f6c9898aea"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(4.5104e-06, device='cuda:0'), tensor(1.2249e-06, device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.MW.idx[0, 0]"
      ],
      "metadata": {
        "id": "2DWnC3k4LO6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c67e78f-edd8-437f-fd0d-a702405b8b8d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1096, -0.0213, -0.6580,  1.8463,  1.4446, -0.6535,  0.1520, -0.9652,\n",
              "        -1.3128,  1.9377,  0.2271, -0.2653, -0.7357, -0.2907,  1.2089, -0.9251,\n",
              "         0.2283,  0.2890,  0.3031, -0.8820,  0.3434, -0.3008,  0.1397,  0.7365,\n",
              "         1.0411,  0.3571,  1.2009,  1.0325,  0.5204, -0.7089,  0.2773,  0.1966,\n",
              "        -0.3079,  1.3117,  1.1683,  1.5912,  0.4856, -0.9992,  0.4280, -0.0457,\n",
              "        -0.2666, -0.6095,  0.5748, -0.7036, -2.7725, -0.5445,  0.3759, -0.2472,\n",
              "         0.8977,  0.3297, -2.4283, -0.8938, -1.6312, -1.2962, -1.1294, -0.7006,\n",
              "        -0.6006,  0.7505, -1.8695,  0.9489,  1.2722, -0.9007, -0.1659,  0.4382,\n",
              "         0.1644,  1.4534, -0.6958, -1.3836, -1.0057,  0.3172, -0.4258, -0.8655,\n",
              "        -0.5688,  0.1299, -1.4437, -0.5919,  0.2205,  0.9302, -0.5005,  0.1060,\n",
              "         0.4416,  1.0990, -1.3321,  0.1972], device='cuda:0',\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwvDxtrN_fEk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}