{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j6dxGxcHAx5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0cdacc-d141-4fa1-9e18-4e4a67f07387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to MNIST_root/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17136528.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/FashionMNIST/raw/train-images-idx3-ubyte.gz to MNIST_root/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to MNIST_root/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 271816.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/FashionMNIST/raw/train-labels-idx1-ubyte.gz to MNIST_root/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to MNIST_root/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4953144.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_root/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to MNIST_root/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 16234794.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_root/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to MNIST_root_test/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16753802.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/FashionMNIST/raw/train-images-idx3-ubyte.gz to MNIST_root_test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to MNIST_root_test/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 277596.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/FashionMNIST/raw/train-labels-idx1-ubyte.gz to MNIST_root_test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to MNIST_root_test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5022646.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_root_test/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to MNIST_root_test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 9491110.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_root_test/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ]\n",
        ")\n",
        "\n",
        "tr = ToTensor()\n",
        "# cifar10_norm = Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "cifar10_norm = Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "channels = 1\n",
        "img_dim = 28\n",
        "\n",
        "def _transform(x):\n",
        "  # x = x.resize((channels, img_dim, img_dim))\n",
        "  # return (\n",
        "  #     cifar10_norm(tr(x))\n",
        "  #     .reshape(channels, img_dim, img_dim)\n",
        "  #     .permute(1, 2, 0)\n",
        "  #     .reshape(-1)\n",
        "  # )\n",
        "  # return (tr(x) / 255.0).reshape(-1)\n",
        "  # return transform(x).reshape(-1)\n",
        "  return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "  # return (tr(x)).reshape(-1)\n",
        "  # return (tr(x).mean(dim=0)).reshape(-1)\n",
        "\n",
        "bsize = 8 ###\n",
        "\n",
        "SOURCE_DATASET = MNIST\n",
        "SOURCE_DATASET = FashionMNIST\n",
        "# SOURCE_DATASET = CIFAR10\n",
        "\n",
        "MNIST_train_data = SOURCE_DATASET(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = SOURCE_DATASET(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NH7FAomMIRJC"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# # config = {\n",
        "# #     \"features\": {\n",
        "# #         \"pre-set\": channels,\n",
        "# #         \"sets\":    [1024,    1024,    1,],\n",
        "# #         \"samples\": [25,      1,       1024,],\n",
        "# #     },\n",
        "# #     \"params\": {\n",
        "# #         \"sets\":    [4,       1,       num_classes,],\n",
        "# #         \"samples\": [(76, 1), (1, 5),  (1025, 1),],\n",
        "# #         \"is conv\": [True,    True,    False,]\n",
        "# #     },\n",
        "# # }\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 4, 5, padding=\"same\", bias=False)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 8, 5, padding=\"same\", bias=False)\n",
        "#         self.conv2 = nn.Conv2d(4, 1, 1, padding=\"same\", bias=False)\n",
        "#         self.fc1 = nn.Linear(1024, 10, bias=False)\n",
        "#         # self.fc2 = nn.Linear(100, 10, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.elu(self.conv1(x))\n",
        "#         x = F.elu(self.conv2(x))\n",
        "#         # x = F.elu(self.conv3(x))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n7DuUfPdITHx"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3spYLqaUIa3n"
      },
      "outputs": [],
      "source": [
        "# trainloader = train_data_loader\n",
        "# for epoch in range(2):  # loop over the dataset multiple times\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 60 == 59:    # print every 60 mini-batches\n",
        "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 60:.3f}')\n",
        "#             running_loss = 0.0\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aE6KCRaT_zjU"
      },
      "outputs": [],
      "source": [
        "# img, lbl = next(iter(test_data_loader))\n",
        "# img = img.reshape(-1, 3, 32, 32)\n",
        "# img = img.permute(0, 2, 3, 1).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xHhauWzb_4b1"
      },
      "outputs": [],
      "source": [
        "# img[0].min(), img[0].max(), img[0].mean()\n",
        "\n",
        "# cifar10_classes = [\n",
        "#     \"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "# ]\n",
        "# _idx = 3\n",
        "# print(cifar10_classes[lbl[_idx].item()])\n",
        "# plt.imshow(img[_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = 2.0 * ((row + offset) / rows) - 1.0\n",
        "        idx[row, col, ch, 1] = 2.0 * ((col + offset) / cols) - 1.0\n",
        "        idx[row, col, ch, 2] = 0.01 * ((ch  + offset) /  chs) - 0.05\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx\n",
        "\n",
        "def _cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = (row + offset)\n",
        "        idx[row, col, ch, 1] = (col + offset)\n",
        "        idx[row, col, ch, 2] = (ch  + offset)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "def poly1norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  idxu = (0.5 ** 0.5) * torch.cat([idxu, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "def poly2norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  _idxu = idxu.reshape((-1, d_idx, 1))\n",
        "  middle = (\n",
        "      torch.bmm(_idxu, _idxu.permute(0, 2, 1))\n",
        "      .reshape((*idxu.shape[:-1], d_idx ** 2))\n",
        "  )\n",
        "  idxu = 0.5 * torch.cat([(2.0 ** 0.5) * idxu, middle, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict as dd\n",
        "from itertools import product\n",
        "\n",
        "@lru_cache()\n",
        "def poly_terms(idx_dim, degree):\n",
        "  combs = dd(int)\n",
        "  ranges = (range(idx_dim) for _ in range(degree))\n",
        "  for idxs in product(*ranges):\n",
        "      comb = tuple(sorted(idxs))\n",
        "      combs[comb] += 1\n",
        "  return list(combs.items())\n",
        "\n",
        "\"\"\"\n",
        "sigmoid(8*x - 4)\n",
        "1/(1 + e^4)\n",
        "+ (8 e^4 x)/(1 + e^4)^2\n",
        "+ (32 e^4 (e^4 - 1) x^2)/(1 + e^4)^3\n",
        "+ (256 (e^4 - 4 e^8 + e^12) x^3)/(3 (1 + e^4)^4)\n",
        "+ (512 e^4 (-1 + 11 e^4 - 11 e^8 + e^12) x^4)/(3 (1 + e^4)^5)\n",
        "+ (4096 (e^4 - 26 e^8 + 66 e^12 - 26 e^16 + e^20) x^5)/(15 (1 + e^4)^6)\n",
        "+ O(x^6)\n",
        "(Taylor series)\n",
        "-------------------------\n",
        "0.017986  * x^0\n",
        "0.14130   * x^1\n",
        "0.5448747 * x^2\n",
        "1.3474883 * x^3\n",
        "2.290065  * x^4\n",
        "2.4479883 * x^5\n",
        "\"\"\"\n",
        "\n",
        "def poly_norm(idxu, degree):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  pre_shape = idxu.shape[:-1]\n",
        "  idx_dim = idxu.shape[-1]\n",
        "  idxu = normalized(idxu)\n",
        "  terms = poly_terms(idx_dim, degree)\n",
        "  factors = torch.tensor([term[1] for term in terms]).float().to(idxu.device)\n",
        "  factors = factors ** 0.5\n",
        "  intidx = torch.tensor([list(term[0]) for term in terms]).long().to(idxu.device)\n",
        "  intidx = intidx.reshape(-1)\n",
        "  idxu = idxu.reshape(-1, idx_dim)[:, intidx]\n",
        "  idxu = idxu.reshape(*pre_shape, degree, -1)\n",
        "  idxu = idxu.prod(dim=-2) * factors.reshape(*((1,) * len(pre_shape)), idxu.shape[-1])\n",
        "  return idxu\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye\n",
        "\n",
        "def log_sinkhorn(log_alpha, n_iter):\n",
        "    \"\"\" https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/sampling/permutations.html\n",
        "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
        "    Concerning nonnegative matrices and doubly stochastic\n",
        "    matrices. Pacific Journal of Mathematics, 1967\n",
        "    Args:\n",
        "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
        "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
        "      n_iters: number of sinkhorn iterations\n",
        "    \"\"\"\n",
        "    for _ in range(n_iter):\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "    return log_alpha.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _knndot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"k-NN Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  num_neigh = 1\n",
        "  dots = []\n",
        "  q_idxu = idxu.cpu().detach().numpy().reshape(-1, d_idx)\n",
        "  for _pos in range(n):\n",
        "    neigh = NearestNeighbors(n_neighbors=num_neigh, metric=\"cosine\")\n",
        "    neigh.fit(idxv[_pos].cpu().detach().numpy().reshape(-1, d_idx))\n",
        "    n_idxu = neigh.kneighbors(\n",
        "        q_idxu, return_distance=False\n",
        "    ).reshape(-1)\n",
        "    n_idxu = torch.from_numpy(n_idxu).long()\n",
        "    _v = v[_pos].reshape(-1, d_val)[n_idxu].reshape(m, d_u, d_val)\n",
        "    # _dot: M x d_val x d_val\n",
        "    _dot = torch.bmm(_v.permute(0, 2, 1), _v)\n",
        "    # _dot: M x 1 x d_val\n",
        "    _dot = torch.diagonal(_dot, dim1=1, dim2=2).unsqueeze(1)\n",
        "    dots.append(_dot)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.cat(dots, dim=1)\n",
        "  return dot\n",
        "\n",
        "def _icbmd(u, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Conv Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x 1 x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_v == 1\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: M x d_u x N\n",
        "  idxv = (\n",
        "      (idxu.reshape(m * d_u, d_idx))\n",
        "      @ idxv.permute(2, 1, 0).reshape(d_idx, n)\n",
        "  ).reshape(m, d_u, n)\n",
        "  # idxv: M x N x d\n",
        "  idxv = torch.bmm(\n",
        "      u.permute(0, 2, 1),\n",
        "      idxv\n",
        "  ).permute(0, 2, 1)\n",
        "  return idxv\n",
        "\n",
        "def _ibmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  v: N x d_v x d_valv\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu, d_valv = u.shape[-1], v.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxu = poly_norm(idxu, 1) # / (d_u)\n",
        "  # idxv = poly_norm(idxv, 1) # / (d_v)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: N x d_idx x d_valv\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxv: M x d_u x (d_valv * N)\n",
        "  idxv = (\n",
        "      idxu.reshape(m * d_u, d_idx)\n",
        "      @ idxv.permute(1, 2, 0).reshape(d_idx, d_valv * n)\n",
        "  ).reshape(m, d_u, d_valv * n)\n",
        "  # idxv: M x d_valu x (d_valv * N)\n",
        "  idxv = torch.bmm(u.permute(0, 2, 1), idxv)\n",
        "  if d_valv == 1:\n",
        "    # idxv: M x N x d_valu\n",
        "    idxv = idxv.reshape(m, d_valu, n).permute(0, 2, 1)\n",
        "  else:\n",
        "    # idxv: M x N x d_valu or error\n",
        "    idxv = idxv.reshape(m, d_valu, d_valv, n).permute(0, 3, 1, 2)\n",
        "    idxv = torch.diagonal(idxv, dim1=2, dim2=3)\n",
        "  return idxv\n",
        "\n",
        "def _fbmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Fast Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxu: M x d_val x d_idx\n",
        "  # idxv: N x d_idx x d_val\n",
        "  idxu = torch.bmm(u.permute(0, 2, 1), idxu)\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxu: M x N x d_val\n",
        "  idxu = (\n",
        "    (\n",
        "        idxu.reshape(m * d_val, d_idx)\n",
        "        @ (\n",
        "            idxv\n",
        "            .permute(0, 2, 1)\n",
        "            .reshape(n * d_val, d_idx)\n",
        "            .T\n",
        "          )\n",
        "    ).reshape(m, d_val, n, d_val)\n",
        "    .permute(0, 2, 1, 3)\n",
        "  )\n",
        "  idxu = torch.diagonal(idxu, dim1=2, dim2=3)\n",
        "  return idxu\n",
        "\n",
        "def batch_mdot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  if d_idx * (d_u + n) < n * d_u * (d_idx + 1):\n",
        "    return _fbmd(u, v, idxu, idxv)\n",
        "  else:\n",
        "    return _ibmd(u, v, idxu, idxv)\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  ###\n",
        "  # idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # mag = 80.0\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv, dim=2)\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), 6)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  from random import randint\n",
        "  bsidx = randint(0, m - 1)\n",
        "  m_ = 1 # m,\n",
        "  idxuv = (\n",
        "      idxu[bsidx].reshape(m_ * d_u, d_idx)\n",
        "      @ idxv.reshape(n * d_v, d_idx).T\n",
        "  )\n",
        "  idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  mag = 80.0\n",
        "  ###\n",
        "  # siter = 6\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), siter)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  #### Tanh seems to work for high-dimensional idx\n",
        "  #### ReLU(x - alpha) / (1.0 - alpha) works for small samples\n",
        "  # alpha = 1.0 - 1.0 / d_v # 0.95\n",
        "  # alpha = min(0.999, alpha)\n",
        "  alpha = 0.97\n",
        "  idxuv = nn.functional.relu((idxuv - alpha) / (1.0 - alpha))\n",
        "  ###\n",
        "  idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # from random import randint\n",
        "  # bsidx = randint(0, m - 1)\n",
        "  # m_ = 1\n",
        "  # mag = 80.0\n",
        "  # # idxu = log_sinkhorn(mag * idxu[bsidx].unsqueeze(0), 6)\n",
        "  # # idxv = log_sinkhorn(mag * idxv, 6)\n",
        "  # idxu = nn.functional.softmax(mag * idxu[bsidx].unsqueeze(0), dim=1)\n",
        "  # idxv = nn.functional.softmax(mag * idxv, dim=1)\n",
        "  # idxuv = idxu.reshape(m_ * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    if dim == -1:\n",
        "      dim = len(mt.data.shape) - 1\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    ###\n",
        "    # _nsbmd\n",
        "    # _rdot\n",
        "    # _knndot\n",
        "    # _fbmd\n",
        "    # _ibmd, _mbmd\n",
        "    # batch_mdot\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    # ###\n",
        "    _aidx = aidx.mean(dim=-2, keepdim=True)\n",
        "    _bidx = bidx.mean(dim=-2, keepdim=True)\n",
        "    onesa = torch.ones(_aidx.shape).to(_aidx.device)\n",
        "    onesb = torch.ones(_bidx.shape).to(_bidx.device)\n",
        "    midx = (\n",
        "        _nsbmd(_aidx, onesb, _aidx, _bidx)\n",
        "        + _nsbmd(onesa, _bidx, _aidx, _bidx)\n",
        "    ) / 2.0\n",
        "    ###\n",
        "    # midx = _icbmd(aidx, aidx, bidx.sum(dim=-2, keepdim=True)) # / aidx.shape[-2]\n",
        "    ###\n",
        "    # midx = (\n",
        "    #     _nsbmd(aidx, 1.0 - bidx, aidx, bidx)\n",
        "    #     + _nsbmd(1.0 - aidx, bidx, aidx, bidx)\n",
        "    # )\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "def idxhood(xidx, ws, stride=None, num_sets=None, sample=False):\n",
        "  xidx = xidx.reshape(-1, xidx.shape[-1]).cpu().detach().numpy()\n",
        "  # xidx = xidx / np.linalg.norm(xidx, axis=-1)[:, None]\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  neigh = NearestNeighbors(n_neighbors=nneigh,)\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  if sample:\n",
        "    if num_sets is None:\n",
        "      num_sets = (len(xidx) + (len(xidx) % stride)) // stride\n",
        "    subidx = np.random.choice(len(xidx), size=num_sets, replace=False)\n",
        "    all_hoods = neigh.kneighbors(\n",
        "        xidx[subidx], return_distance=False,\n",
        "    ).reshape(-1)\n",
        "  else:\n",
        "    _neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "    _ids = np.array([[pos // img_dim, pos % img_dim] for pos in range(len(xidx))])\n",
        "    # _ids = np.array([[pos] for pos in range(len(xidx))])\n",
        "    _neigh.fit(_ids)\n",
        "    if stride is None:\n",
        "      stride = len(xidx) // num_sets\n",
        "    all_hoods = _neigh.kneighbors(\n",
        "        _ids[::stride], return_distance=False\n",
        "    ).reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "VvlcR_tmuyy2"
      },
      "outputs": [],
      "source": [
        "# from pandas.core.arrays.categorical import Shape\n",
        "\n",
        "class MModule3(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=3, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self._samples = samples\n",
        "    self.samples = [int(np.prod(samp)) for samp in samples]\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (1, n_params), idx_dim, device\n",
        "    )\n",
        "    if probe_dim:\n",
        "      n_classes = 10\n",
        "      self._pw, self._pw_idx, self.probe = self._make_pmt(\n",
        "          (n_classes, probe_dim), idx_dim, device\n",
        "      )\n",
        "    self.activation = nn.ELU()\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _W = nn.Parameter(\n",
        "        2.0 * torch.rand(shape, device=device) - 1.0\n",
        "    )\n",
        "    _W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((*shape, idxdim), device=device) - 1.0\n",
        "    )\n",
        "    # _W_idx = (\n",
        "    #     2.0 * torch.rand((*shape, idxdim), device=device) - 1.0\n",
        "    # )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    n = x.data.shape[0]\n",
        "    filter_whs = [(samp[1], samp[2], samp[0]) for samp in self._samples[:-1]]\n",
        "    strides = self.sets[:-1]\n",
        "    stride = strides[0]\n",
        "    filter_volume = np.prod(filter_whs[0])\n",
        "    self.all_pools = [x[:4]]\n",
        "    idxx = idxhood(x.idx[0], filter_whs[0], strides[0], sample=True) ### FIX\n",
        "    # pool: N x (num_windows * window_volume)\n",
        "    pool = x[:, idxx]\n",
        "    ###\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[0, wl: wr],\n",
        "          (self.sets[step], self.samples[step])\n",
        "      )\n",
        "      if conv:\n",
        "        # pool: (N * num_windows) x  sets\n",
        "        pool = MTensor.reshape(pool, (-1, filter_volume)) @ mw\n",
        "        pool = MTensor.reshape(pool, (n, -1, self.sets[step]))\n",
        "      else:\n",
        "        # pool.data = self.probe(pool.data)\n",
        "        # pool: N x n_classes\n",
        "        pool = pool @ self.probe\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_volume = np.prod(filter_whs[nxt_conv_step])\n",
        "        if nxt_conv:\n",
        "          idxx = idxhood(\n",
        "              pool.idx[0],\n",
        "              filter_whs[nxt_conv_step],\n",
        "              strides[nxt_conv_step],\n",
        "              sample=True,\n",
        "          ) ### FIX\n",
        "          pool = pool[:, idxx]\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCh8kNiFl15G"
      },
      "source": [
        "#### MModule IV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "PAqc0d7Kl6Ud"
      },
      "outputs": [],
      "source": [
        "def _count_params(config):\n",
        "  tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "  n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "  return n_params\n",
        "\n",
        "class MModule4(nn.Module):\n",
        "  def __init__(self, config, idx_dim=3, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    assert len(config[\"features\"][\"sets\"]) > 0\n",
        "    assert len(config[\"params\"][\"sets\"]) > 0\n",
        "    assert len(config[\"features\"][\"sets\"]) == len(config[\"params\"][\"sets\"])\n",
        "    for key in [\"sets\", \"samples\"]:\n",
        "      assert len(config[\"features\"][key]) == len(config[\"features\"][\"sets\"])\n",
        "      assert len(config[\"params\"][key]) == len(config[\"params\"][\"sets\"])\n",
        "    self._idx_dim = idx_dim\n",
        "    self.device = device\n",
        "    self._config = config\n",
        "    self._config[\"params\"][\"samples\"] = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "    self._ch0 = config[\"features\"][\"pre-set\"]\n",
        "    self._is_conv = config[\"params\"][\"is conv\"]\n",
        "    self._feat_samples = config[\"features\"][\"samples\"]\n",
        "    self._n_params = _count_params(config)\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (self._n_params,), idx_dim, device\n",
        "    )\n",
        "    # _std = 0.1\n",
        "    # self._ones_idx = nn.Parameter(\n",
        "    #     _std * torch.randn((1, 1, idx_dim), device=device)\n",
        "    # )\n",
        "    # self._ones_idx = torch.zeros((1, 1, idx_dim), device=device)\n",
        "    # self.activation = nn.ELU()\n",
        "    self.activation = nn.ReLU()\n",
        "    # self.activation = nn.LeakyReLU()\n",
        "    self._probe = nn.Linear(self._feat_samples[-1], 10).to(device)\n",
        "    self._num_fwd = 0\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _mag = 0.1 # 2.0 # 1000.0 * 0.01\n",
        "    _W_idx = _mag * torch.rand((*shape, idxdim), device=device) - (_mag / 2.0)\n",
        "    ###\n",
        "    # _W_idx = _W_idx.reshape(-1, idxdim)\n",
        "    # _W_idx[:, 2] = 1.0\n",
        "    # _W_idx = _W_idx.reshape(*shape, idxdim)\n",
        "    ###\n",
        "    # _W_idx = nn.Parameter(\n",
        "    #   _mag * torch.rand((*shape, idxdim), device=device) - (_mag / 2.0)\n",
        "    # )\n",
        "    # _std = 1.0\n",
        "    # _W_idx = nn.Parameter(\n",
        "    #   _std * torch.randn((*shape, idxdim), device=device)\n",
        "    # )\n",
        "    # _W = torch.ones(shape).float().to(device)\n",
        "    _std = 0.01\n",
        "    # _W = _std * torch.randn(shape, device=device)\n",
        "    # _W = _mag * torch.rand(shape, device=device) - (_mag / 2.0)\n",
        "    _W = nn.Parameter(\n",
        "        _std * torch.randn(shape, device=device)\n",
        "    )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def _put_one(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    device = x.data.device\n",
        "    assert device == x.idx.device\n",
        "    ones_idx = self._ones_idx.repeat(n, 1, 1)\n",
        "    dummy_ones = MTensor(\n",
        "        values=torch.ones(n, 1).to(device),\n",
        "        indices=ones_idx,\n",
        "    )\n",
        "    x = MTensor.cat([x, dummy_ones], dim=1)\n",
        "    return x\n",
        "\n",
        "  def _reinit_indices(self, pool, params):\n",
        "    \"\"\"\n",
        "    pool: N x mshape\n",
        "    params: param_shape\n",
        "    \"\"\"\n",
        "    idx_dim = pool.idx.shape[-1]\n",
        "    assert params.idx.shape[-1] == idx_dim\n",
        "    pool_idx = pool.idx.reshape(-1, idx_dim)\n",
        "    eps = 1e-6\n",
        "    idx_max = pool_idx.max(dim=0, keepdim=True)[0] + eps\n",
        "    idx_min = pool_idx.min(dim=0, keepdim=True)[0] - eps\n",
        "    idx_itv = idx_max - idx_min\n",
        "    n_samples = len(params.idx.reshape(-1, idx_dim))\n",
        "    sampled_idx = torch.rand((n_samples, idx_dim), device=pool.data.device)\n",
        "    sampled_idx = sampled_idx * idx_itv + idx_min\n",
        "    params.idx = sampled_idx\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    ###\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    n = x.data.shape[0]\n",
        "    param_sets = self._config[\"params\"][\"sets\"]\n",
        "    param_samples = self._config[\"params\"][\"samples\"]\n",
        "    feat_sets = self._config[\"features\"][\"sets\"]\n",
        "    feat_samples = self._config[\"features\"][\"samples\"]\n",
        "    self.all_pools = []\n",
        "    self.all_samples = []\n",
        "    pool = x\n",
        "    pool = MTensor.reshape(pool, (n, self._ch0, -1))\n",
        "    pool = MTensor.permute(pool, (0, 2, 1))\n",
        "    wl, wr = 0, param_sets[0] * param_samples[0]\n",
        "    for step in range(n_layers):\n",
        "      self.all_pools.append(pool[:4])\n",
        "      ###\n",
        "      if self._num_fwd == 0:\n",
        "        self._reinit_indices(pool, self.MW[wl: wr])\n",
        "      self._num_fwd += 1\n",
        "      ###\n",
        "      if self._is_conv[step]:\n",
        "        idx_slice = pool.idx[0, :, 0]\n",
        "      else:\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        # idx_slice = pool.idx[0]\n",
        "        idx_slice = pool.idx[0, :, :3]\n",
        "      # pool = MTensor.reshape(pool, (n, -1))\n",
        "      # idx_slice = pool.idx[0]\n",
        "      idxx = idxhood(\n",
        "          idx_slice,\n",
        "          feat_samples[step],\n",
        "          num_sets=feat_sets[step],\n",
        "          sample=True,\n",
        "      )\n",
        "      # pool: N x (num_windows * window_volume)\n",
        "      pool = pool[:, idxx]\n",
        "      self.all_samples.append(pool[:4])\n",
        "      ###\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[wl: wr],\n",
        "          (param_sets[step], param_samples[step])\n",
        "      )\n",
        "      ###\n",
        "      # pool: (N * num_windows) x sets\n",
        "      # pool = MTensor.reshape(pool, (-1, feat_samples[step])) @ mw\n",
        "      ### o 1 do bias parece que atrapalha\n",
        "      # pool = (\n",
        "      #     self._put_one(MTensor.reshape(\n",
        "      #         pool, (n * feat_sets[step], -1)\n",
        "      #     ))\n",
        "      #     @ mw\n",
        "      # )\n",
        "      ###\n",
        "      maromba_only = False\n",
        "      if maromba_only or (step < n_layers - 1):\n",
        "        pool = (\n",
        "            MTensor.reshape(\n",
        "                pool, (n * feat_sets[step], -1)\n",
        "            )\n",
        "            @ mw\n",
        "        )\n",
        "      else:\n",
        "        pool = self._probe(pool.data.reshape(n, -1))\n",
        "        pool = MTensor(\n",
        "            pool,\n",
        "            torch.zeros((*pool.shape, self._idx_dim)).to(pool.device)\n",
        "        )\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, -1))\n",
        "      # pool: N x (num_windows * sets)\n",
        "      if step < n_layers - 1:\n",
        "        pool.data = self.activation(pool.data)\n",
        "        pass\n",
        "      else:\n",
        "        break\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, feat_sets[step], param_sets[step]))\n",
        "      ###\n",
        "      nxt_step = step + 1\n",
        "      next_wr = wr + param_sets[nxt_step] * param_samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  # y_pred = 10.0 * y_pred.data ### WHY 10x?\n",
        "  y_pred = y_pred.data ### WHY 10x?\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 3 # 500 # 3 # 10\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, chs=channels, offset=1, d=idx_dim, mag=2.0) # 1000.0\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_m1YvjxBdj9"
      },
      "source": [
        "### Visualizações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UZ4DrI6mBn39"
      },
      "outputs": [],
      "source": [
        "def plot_features(x: MTensor):\n",
        "  \"\"\"\n",
        "  x.data: in_dim\n",
        "  x.idx:  in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  n, idx_dim = x.idx.shape\n",
        "  assert x.data.shape == (n,)\n",
        "  tidx = x.idx.cpu().detach().numpy()\n",
        "  tdata = x.data.cpu().detach().numpy()\n",
        "  plot_df = pd.DataFrame(\n",
        "      {\n",
        "          \"x\": tidx[:, 0],\n",
        "          \"y\": tidx[:, 1],\n",
        "          \"z\": tidx[:, 2],\n",
        "          \"val\": tdata,\n",
        "      }\n",
        "  )\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=\"val\")\n",
        "  fig.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQbo5JtHw3bq"
      },
      "source": [
        "### Treino (b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "xGn5VTZPw-1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dab733ef-c19a-4156-e78d-200dfef600f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b71aca92-9f17-4db3-a7d4-3a7bc9c70cf4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b71aca92-9f17-4db3-a7d4-3a7bc9c70cf4\")) {                    Plotly.newPlot(                        \"b71aca92-9f17-4db3-a7d4-3a7bc9c70cf4\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003eval=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[-0.40215539932250977,0.42935362458229065,-0.03681901842355728,0.40254855155944824,-0.3269279897212982,0.35864877700805664,0.4295039474964142,0.3921107053756714,0.38123688101768494,0.4295039474964142,0.4295039474964142,-0.3664020895957947,-0.39843106269836426,-0.4067482352256775,0.4295039474964142,0.4295039474964142,0.42818349599838257,-0.3806632161140442,0.4295039474964142,-0.2981153130531311,-0.4041523039340973,0.4295039474964142,-0.3654194474220276,-0.42174050211906433,-0.39822426438331604,0.4295039474964142,-0.33301571011543274,-0.30450549721717834,-0.24650324881076813,0.4295039474964142,-0.40666311979293823,0.14958131313323975,0.42903414368629456,-0.03513605147600174,0.4295039474964142,0.4295039474964142,-0.39352184534072876,0.4096641540527344,-0.3848012089729309,0.4295039474964142,0.4295039474964142,0.4290396571159363,0.0471595898270607,0.4295039474964142,0.4295039474964142,-0.08857370913028717,0.4295039474964142,0.4295039474964142,-0.31530240178108215,0.37299174070358276,-0.3068862855434418,0.4295039474964142,-0.3804919421672821,-0.3081128001213074,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.4211684465408325,0.3965776860713959,0.04075285792350769,0.4295039474964142,-0.17421400547027588,-0.45232248306274414,-0.40272215008735657,-0.31244948506355286,0.4295039474964142,-0.42470991611480713,-0.33537331223487854,-0.3726673126220703,-0.37662577629089355,0.4295039474964142,0.425016850233078,0.4295039474964142,0.2976948916912079,-0.3169330358505249,0.4295039474964142,-0.4492841958999634,-0.3380519151687622,-0.3752809762954712,0.4295039474964142,0.42595037817955017,-0.3482159376144409,-0.34246379137039185,-0.1292998194694519,0.07672044634819031,0.4295039474964142,0.3383697271347046,-0.3952193558216095,-0.38302576541900635,0.4295039474964142,-0.39342716336250305,0.4288969337940216,-0.3959736227989197,-0.40153324604034424,0.4295039474964142,0.287538081407547,0.4295039474964142,-0.37121033668518066,-0.40029340982437134,0.020104968920350075,-0.39209872484207153,-0.3779117465019226,0.299444317817688,-0.37625905871391296,-0.31381022930145264,0.35498613119125366,0.4295039474964142,-0.45058751106262207,-0.2707776427268982,-0.38567012548446655,0.34164971113204956,-0.40111398696899414,-0.10409501940011978,0.35444605350494385,-0.4231777787208557,-0.32246267795562744,-0.4140787422657013,0.29859793186187744,0.4295039474964142,0.41300588846206665,0.4295039474964142,-0.07284311950206757,0.12526430189609528,0.4295039474964142,0.4295039474964142,-0.3578329384326935,0.4295039474964142,0.4295039474964142,0.4279991090297699,-0.22060725092887878,0.4295039474964142,0.4295039474964142,0.4273567497730255,-0.36204567551612854,-0.2939949333667755,0.4295039474964142,0.0675126239657402,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.24141830205917358,-0.37708568572998047,0.4295039474964142,0.2966814339160919,0.42947208881378174,0.007787078619003296,0.4295039474964142,-0.06258915364742279,-0.1772741824388504,-0.3502638339996338,0.3858295977115631,-0.33228495717048645,0.4295039474964142,-0.36134129762649536,0.4295039474964142,-0.4638422131538391,0.4295039474964142,-0.020510587841272354,-0.07799704372882843,-0.4228250980377197,0.2734546661376953,-0.4119203984737396,-0.2685065269470215,0.4295039474964142,-0.4172455072402954,0.4295039474964142,0.4295039474964142,-0.3291357755661011,0.4295039474964142,0.429328590631485,-0.38966816663742065,0.2274145632982254,0.18993188440799713,-0.4027007222175598,0.4295039474964142,0.4295039474964142,0.14279456436634064,0.4293481707572937,0.4295039474964142,-0.40045344829559326,0.4295039474964142,-0.36033308506011963,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.3875737488269806,0.4295039474964142,0.011091096326708794,0.4295039474964142,-0.3924051523208618,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.23627202212810516,-0.027630111202597618,-0.3690739870071411,0.4295039474964142,0.24897807836532593,-0.3908662497997284,0.4295039474964142,-0.39044591784477234,-0.36446690559387207,-0.39521920680999756,-0.3662145733833313,-0.22500522434711456,-0.3336009085178375,-0.2819691002368927,0.4295039474964142,-0.41556522250175476,0.4295039474964142,0.4295039474964142,-0.4073469042778015,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.31261348724365234,0.4295039474964142,-0.3547651767730713,0.4295039474964142,0.4295039474964142,-0.39513099193573,-0.39168402552604675,0.4295039474964142,-0.17995071411132812,-0.4083960950374603,0.1144028753042221,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.3956969082355499,0.4295039474964142,0.04725608229637146,0.4295039474964142,0.19632135331630707,0.33771947026252747,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.3006719648838043,0.2517797350883484,0.4295039474964142,-0.3605325222015381,0.4284359812736511,0.4295039474964142,-0.3720133304595947,-0.2664736807346344,0.4295039474964142,0.4295039474964142,0.42931440472602844,0.005588816478848457,-0.3638375401496887,-0.3588595688343048,0.37748581171035767,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.3794933557510376,0.4295039474964142,-0.39797312021255493,-0.44663673639297485,0.027845880016684532,-0.40947020053863525,0.4295039474964142,0.4295039474964142,-0.44915342330932617,0.2827714681625366,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.29049989581108093,-0.2770422101020813,-0.3642542064189911,0.40367239713668823,0.1065230593085289,0.4295039474964142,0.4295039474964142,-0.3715745806694031,0.4295039474964142,-0.29081523418426514,0.2573055624961853,0.4070536196231842,-0.14350825548171997,0.4295039474964142,0.11534319818019867,0.4295039474964142,0.40393829345703125,-0.1647762656211853,-0.3991136848926544,0.4295039474964142,-0.4149717688560486,0.4295039474964142,-0.3109745979309082,0.19696159660816193,0.4295039474964142,0.4295039474964142,0.26420876383781433,-0.3633292317390442,-0.37388288974761963,0.4295039474964142,-0.38015860319137573,-0.40669217705726624,0.4295039474964142,-0.2986947298049927,0.09889177978038788,0.3997308611869812,0.4295039474964142,0.4295039474964142,-0.3154662251472473,0.4295039474964142,0.4295039474964142,0.4151001572608948,0.032343752682209015,0.19571708142757416,-0.4337506890296936,-0.3725595474243164,0.3860113024711609,0.4295039474964142,-0.1524267941713333,0.42798376083374023,0.4295039474964142,-0.4104325771331787,0.42887455224990845,-0.3121500611305237,-0.387607216835022,0.4292864501476288,0.05699775367975235,0.4295039474964142,-0.41390079259872437,-0.1719752699136734,0.42268261313438416,0.4295039474964142,-0.46230071783065796,-0.30449992418289185,0.4295039474964142,0.4295039474964142,-0.36553430557250977,-0.42309892177581787,0.42678025364875793,0.4295039474964142,-0.43479451537132263,0.4295039474964142,-0.22587405145168304,0.4112492799758911,0.4295039474964142,-0.4336619973182678,-0.41584035754203796,0.22819092869758606,0.4295039474964142,-0.3816099762916565,0.4295039474964142,0.3842143714427948,-0.39840027689933777,0.4227052330970764,-0.27196717262268066,0.4295039474964142,0.4295039474964142,-0.31247931718826294,-0.35426345467567444,-0.4144291281700134,0.1592726707458496,0.38807663321495056,0.42888718843460083,-0.3724249005317688,0.4295039474964142,0.3350245952606201,-0.4275052845478058,0.22969786822795868,0.4295039474964142,-0.36049214005470276,-0.35734736919403076,0.4295039474964142,0.25941941142082214,-0.1880773901939392,-0.38421693444252014,0.36778244376182556,0.4295039474964142,-0.4147034287452698,0.4295039474964142,0.0437127985060215,0.4295039474964142,0.4295039474964142,-0.33445414900779724,-0.3878719210624695,-0.42254820466041565,0.4295039474964142,0.3032473027706146,0.3626927435398102,0.4295039474964142,-0.3948647379875183,0.42915624380111694,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.32737424969673157,0.3620935082435608,0.4295039474964142,0.4290832281112671,-0.35808318853378296,-0.33964622020721436,-0.3677911162376404,0.1033894419670105,0.4295039474964142,0.4281797707080841,0.4295039474964142,0.3843662738800049,-0.3886793851852417,0.31973153352737427,0.4039176106452942,0.4295039474964142,-0.3211376965045929,0.21133702993392944,-0.14636960625648499,0.4295039474964142,0.4295039474964142,-0.11360649764537811,-0.15123392641544342,0.4135860204696655,0.3035529851913452,-0.4118216931819916,0.4295039474964142,0.059164270758628845,-0.0760219618678093,0.4295039474964142,0.0021680407226085663,0.27934008836746216,-0.42750000953674316,0.3824451267719269,0.4295039474964142,-0.1183810904622078,0.4295039474964142,-0.29917314648628235,-0.1812756210565567,0.025834308937191963,0.4295039474964142,0.4295039474964142,-0.4300796091556549,-0.2995258569717407,-0.36340567469596863,-0.2665405869483948,-0.3799492418766022,-0.45401349663734436,0.1780800223350525,0.40578794479370117,0.38376477360725403,-0.40792012214660645,0.2276991903781891,0.3972300589084625,0.4295039474964142,-0.3864755630493164,0.4295039474964142,0.1802307367324829,0.3942704200744629,-0.36835208535194397,0.10236528515815735,-0.36720162630081177,-0.38822320103645325,0.4288150668144226,-0.3484978675842285,0.4295039474964142,-0.3649633526802063,-0.2709007263183594,-0.398495614528656,0.03828496113419533,0.3149772882461548,0.4295039474964142,-0.4196719825267792,0.3294204771518707,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.2329275757074356,0.4295039474964142,-0.34835004806518555,-0.1010531634092331,0.4295039474964142,0.4295039474964142,-0.379751056432724,0.4295039474964142,0.4295039474964142,-0.129336416721344,-0.396659255027771,0.4295039474964142,0.4295039474964142,0.42932426929473877,-0.33144208788871765,0.36756038665771484,0.42776942253112793,0.37004342675209045,0.4295039474964142,-0.3810679614543915,-0.10111244022846222,-0.3156728744506836,-0.21730783581733704,-0.13214921951293945,0.4295039474964142,0.4295039474964142,0.42548954486846924,-0.18961907923221588,0.38085833191871643,-0.15349611639976501,0.25773540139198303,0.4295039474964142,-0.39638203382492065,-0.3955838084220886,0.4295039474964142,0.1200234591960907,-0.412508487701416,0.4295039474964142,0.4295039474964142,0.26956453919410706,0.4295039474964142,-0.4168287217617035,0.4295039474964142,-0.3964773714542389,-0.39323896169662476,0.1786527931690216,-0.1693817526102066,-0.3741794228553772,0.14191119372844696,-0.1893104463815689,-0.33219054341316223,0.23452912271022797,0.4295039474964142,-0.38246235251426697,-0.35406744480133057,-0.4031558930873871,0.20185597240924835,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.44655880331993103,0.4293929934501648,0.4295039474964142,0.3775416314601898,-0.4045441746711731,0.4295039474964142,-0.23408864438533783,-0.36743998527526855,-0.4179987907409668,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.3776656985282898,0.22718054056167603,0.16493046283721924,-0.29744982719421387,0.4295039474964142,-0.36592382192611694,-0.4086569845676422,0.35196369886398315,0.429299920797348,0.4295039474964142,0.3541198670864105,0.4295039474964142,-0.3804266154766083,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.3593454957008362,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4292435944080353,0.4295039474964142,0.4295039474964142,-0.4444230794906616,0.4247444272041321,0.4295039474964142,-0.3621585965156555,-0.29233571887016296,0.4295039474964142,0.4295039474964142,-0.3879703879356384,0.3733382821083069,-0.3958043158054352,-0.035885926336050034,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.4102327525615692,-0.3819308578968048,-0.4092266857624054,0.4294334948062897,0.4295039474964142,0.4295039474964142,-0.4116398096084595,0.379508912563324,-0.434309720993042,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.1058250144124031,-0.020977681502699852,-0.17802567780017853,-0.3127587139606476,0.3741986155509949,-0.2359524667263031,0.4295039474964142,-0.14064306020736694,0.4250291585922241,-0.3945830464363098,-0.39439183473587036,-0.24289534986019135,0.006567008793354034,0.4295039474964142,0.27178990840911865,0.4295039474964142,-0.3881171643733978,-0.1612924337387085,-0.36479541659355164,0.3654313087463379,-0.41058310866355896,-0.41668617725372314,0.4198662042617798,0.4295039474964142,-0.3607754707336426,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.21228070557117462,-0.15559102594852448,-0.36670956015586853,0.4295039474964142,-0.2704470753669739,-0.4071732461452484,0.28195691108703613,0.3850376307964325,0.20156696438789368,-0.37144380807876587,-0.40770456194877625,-0.05320630595088005,-0.046996910125017166,0.39163458347320557,-0.36126166582107544,-0.3759457468986511,0.4295039474964142,-0.39560234546661377,-0.4203229248523712,-0.28130918741226196,0.4295039474964142,-0.4104212522506714,-0.3682519495487213,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.0808584988117218,-0.45145177841186523,0.4292466640472412,0.2557808458805084,-0.15763536095619202,0.4173358678817749,0.4295039474964142,-0.2833838164806366,0.4295039474964142,0.4295039474964142,-0.4191170334815979,0.3088335394859314,0.4295039474964142,0.27472472190856934,-0.32666701078414917,0.4281826913356781,-0.4386581480503082,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.31323567032814026,-0.42698439955711365,0.4295039474964142,-0.41986221075057983,-0.4180334210395813,0.4295039474964142,0.4295039474964142,0.4295039474964142,0.35032981634140015,0.4295039474964142,0.4295039474964142,0.4192836880683899,0.4295039474964142,0.42583784461021423,0.4282277822494507,-0.39630603790283203,0.4295039474964142,-0.3654058873653412,0.4295039474964142,0.4295039474964142,-0.38040924072265625,-0.1987706869840622,-0.38899344205856323,0.40491658449172974,0.31780117750167847,0.4292586147785187,0.4294402003288269,-0.0673636794090271,0.4295039474964142,0.42920613288879395,0.4295039474964142,0.4295039474964142,-0.3987082540988922,0.4285275340080261,0.4295039474964142,-0.3709932565689087,0.1953597515821457,0.4295039474964142,-0.21891295909881592,-0.3188711702823639,0.1513863354921341,0.4295039474964142,0.2007930427789688,0.4295039474964142,0.32076185941696167,0.05851658433675766,0.4295039474964142,-0.40318563580513,-0.3524394631385803,-0.3911399841308594,-0.3612173795700073,0.3948762118816376,0.2573363482952118,-0.36600905656814575,0.16505639255046844,0.42796850204467773,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.23718437552452087,0.4295039474964142,0.4295039474964142,0.4295039474964142,-0.38422974944114685,-0.3960276246070862,-0.0889199748635292,0.4295039474964142,-0.3657749891281128,0.4293126165866852,0.4295039474964142,-0.4128738045692444,0.4198426604270935,0.4295039474964142,0.23206903040409088,0.28430455923080444,0.4295039474964142,0.4295039474964142,-0.35714271664619446,0.09164917469024658,-0.38626527786254883,0.39710986614227295,0.3577740490436554,0.4293520748615265,0.4295039474964142,0.39442721009254456,-0.4107503592967987,-0.34482264518737793,-0.33202695846557617,-0.37383532524108887,-0.4248242676258087,0.3828497529029846,0.4295039474964142,0.4295039474964142,-0.4180086553096771,0.4295039474964142,-0.4052305519580841,-0.38030311465263367,-0.4022691249847412,-0.16844651103019714,-0.3348972499370575,-0.3014335632324219,-0.27213171124458313,0.4295039474964142,0.4295039474964142,0.006589600816369057,-0.37445753812789917,-0.42121195793151855,0.4295039474964142,-0.3314239978790283,0.4295039474964142],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[-0.1279955357313156,0.06002639979124069,-0.14245876669883728,0.13234253227710724,0.13234253227710724,0.08895284682512283,0.11787930130958557,0.1468057632446289,0.06002639979124069,0.07448963075876236,-0.11353232711553574,0.016636721789836884,-0.16870681941509247,0.1468057632446289,-0.026752958074212074,0.002173495013266802,-0.14245876669883728,0.06002639979124069,-0.1715637743473053,-0.026752958074212074,0.1034160777926445,-0.16870684921741486,0.002173495013266802,0.13234253227710724,0.07448962330818176,-0.09906910359859467,-0.05567941069602966,-0.11353232711553574,0.08895284682512283,0.07448962330818176,-0.11353232711553574,-0.0846058651804924,-0.14245876669883728,-0.041216179728507996,-0.012289732694625854,-0.1708495318889618,0.016636721789836884,-0.09906908869743347,-0.09906910359859467,-0.09906908869743347,-0.14245876669883728,-0.09906908869743347,0.1468057632446289,0.016636721789836884,0.031099947169423103,-0.09906910359859467,-0.16870683431625366,0.07448962330818176,-0.11353232711553574,-0.012289732694625854,0.016636721789836884,-0.16870683431625366,0.18751706182956696,-0.14245876669883728,0.18751706182956696,0.08895284682512283,-0.0846058651804924,0.16126897931098938,-0.07014264166355133,0.17573221027851105,-0.11353232711553574,-0.14245876669883728,0.08895284682512283,0.1034160777926445,0.18751706182956696,0.11787930130958557,0.16126897931098938,-0.1279955357313156,-0.012289732694625854,-0.11353232711553574,0.16126897931098938,-0.16870683431625366,0.06002639979124069,0.18751706182956696,-0.15692198276519775,-0.1676354855298996,-0.15692198276519775,-0.07014263421297073,-0.04121618717908859,0.16126897931098938,-0.14245876669883728,-0.05567941069602966,-0.04121618717908859,0.016636721789836884,-0.16870683431625366,-0.14245876669883728,-0.0846058651804924,0.18751706182956696,0.04556317254900932,0.11787930130958557,-0.026752958074212074,-0.16870684921741486,0.08895284682512283,0.04556317999958992,-0.0846058651804924,0.08895284682512283,0.016636721789836884,0.16126897931098938,-0.11353232711553574,0.06002639979124069,0.18751706182956696,0.031099949032068253,0.031099949032068253,-0.012289732694625854,0.18751706182956696,-0.09906910359859467,-0.026752958074212074,-0.15692198276519775,-0.04121618717908859,-0.07014264166355133,0.06002639979124069,0.13234253227710724,0.1034160777926445,-0.012289733625948429,0.1468057632446289,0.1468057632446289,0.1468057632446289,-0.05567941069602966,-0.1279955357313156,0.08895284682512283,0.031099949032068253,0.18751706182956696,0.016636721789836884,0.04556317254900932,0.1468057632446289,-0.026752958074212074,-0.084605872631073,-0.026752958074212074,-0.16870683431625366,0.16126897931098938,-0.09906910359859467,0.06002639979124069,-0.09906908869743347,0.002173495013266802,-0.16870683431625366,0.031099947169423103,-0.16870683431625366,0.13234251737594604,-0.012289732694625854,0.1034160777926445,-0.05567941069602966,0.17573222517967224,0.18983832001686096,-0.16870683431625366,-0.127995565533638,-0.15692198276519775,-0.1604931503534317,-0.026752958074212074,0.13234251737594604,-0.07014264166355133,-0.15692198276519775,-0.0846058651804924,-0.041216179728507996,0.016636719927191734,-0.026752958074212074,-0.14245876669883728,0.002173495013266802,-0.041216179728507996,-0.012289732694625854,0.031099949032068253,-0.16870683431625366,-0.16870683431625366,0.18751706182956696,-0.1279955357313156,0.13234253227710724,-0.09906910359859467,-0.05567941069602966,0.04556317254900932,0.16126897931098938,-0.15692198276519775,0.11787930130958557,-0.1279955357313156,-0.09906910359859467,0.11787930130958557,-0.05567941069602966,-0.17084954679012299,-0.09906910359859467,-0.07014264166355133,0.031099949032068253,0.16126897931098938,-0.09906910359859467,0.18751706182956696,0.11787930130958557,0.18591003119945526,-0.127995565533638,0.04556317254900932,0.18591003119945526,-0.1279955357313156,0.0021734952460974455,-0.09906910359859467,-0.012289732694625854,-0.026752958074212074,0.031099947169423103,0.17555364966392517,0.18751706182956696,0.11787930130958557,0.04556317254900932,-0.14656560122966766,-0.04121618717908859,0.07448962330818176,-0.05567941069602966,0.1034160777926445,0.016636719927191734,-0.16870683431625366,0.031099947169423103,-0.05567941069602966,-0.07014264166355133,-0.012289732694625854,0.002173495013266802,-0.16870681941509247,-0.1279955506324768,-0.15692198276519775,0.13234253227710724,-0.012289732694625854,-0.1699567437171936,0.04556317254900932,0.13234253227710724,-0.012289732694625854,0.18751706182956696,0.08895284682512283,-0.04121618717908859,0.031099949032068253,0.18751706182956696,0.016636721789836884,-0.07014264166355133,0.07448963075876236,0.18751706182956696,0.06002639979124069,0.031099949032068253,0.002173495013266802,-0.1715637743473053,-0.07014264166355133,-0.05567941069602966,-0.05567941069602966,-0.127995565533638,0.016636721789836884,0.002173495013266802,0.06002639979124069,-0.07014264166355133,0.016636719927191734,0.18751706182956696,-0.14245876669883728,-0.05567941069602966,-0.16870683431625366,0.04556317254900932,0.1468057632446289,-0.14245876669883728,0.1034160777926445,0.18591003119945526,0.17519652843475342,-0.0846058651804924,-0.05567941069602966,-0.012289732694625854,-0.026752958074212074,0.18751706182956696,0.16126897931098938,-0.11353232711553574,-0.04121618717908859,0.08895284682512283,0.13234253227710724,0.08895284682512283,0.16126897931098938,0.18751706182956696,0.1034160777926445,0.11787930130958557,0.16234032809734344,0.13234253227710724,-0.15692198276519775,0.031099947169423103,-0.1669212281703949,-0.14245876669883728,-0.07014264166355133,-0.0846058651804924,-0.04121618717908859,-0.16870683431625366,-0.0846058651804924,-0.16013602912425995,0.18591003119945526,0.04556317254900932,0.18591004610061646,0.1034160777926445,0.17573222517967224,-0.05567941069602966,0.16126897931098938,0.031099947169423103,0.11787930130958557,-0.026752958074212074,0.1034160777926445,0.18751706182956696,-0.09906910359859467,-0.04121618717908859,0.11787930130958557,-0.14245876669883728,-0.07014264166355133,-0.11353232711553574,0.17573222517967224,0.016636721789836884,0.1034160777926445,0.07448962330818176,0.002173495013266802,-0.041216179728507996,0.08895284682512283,0.1468057632446289,0.1034160777926445,-0.09906910359859467,0.13234251737594604,0.031099947169423103,-0.1676354855298996,-0.14245876669883728,0.18751706182956696,0.04556317254900932,0.11787930130958557,-0.07014264166355133,-0.14245876669883728,0.002173495013266802,-0.1279955357313156,-0.07014264166355133,0.16126897931098938,0.031099947169423103,-0.16870683431625366,-0.1279955357313156,-0.05567941069602966,0.11787930130958557,-0.11353231221437454,0.11787930130958557,0.002173495013266802,-0.15692199766635895,-0.07014264166355133,-0.012289732694625854,0.17573222517967224,-0.15692198276519775,-0.0846058651804924,-0.05567941069602966,-0.14245876669883728,0.18751706182956696,0.18751706182956696,0.11787930130958557,-0.05567941069602966,-0.14245876669883728,-0.15692198276519775,0.06002639979124069,-0.1279955357313156,-0.0846058651804924,-0.16870681941509247,0.11787930130958557,-0.07014264166355133,-0.14245876669883728,0.13234253227710724,0.11787930130958557,0.13234253227710724,0.06002639979124069,-0.04121618717908859,-0.14245876669883728,0.11787930130958557,-0.127995565533638,-0.15692198276519775,-0.16013602912425995,-0.14245876669883728,-0.09906910359859467,-0.04121618717908859,-0.16870681941509247,0.1034160777926445,-0.05567941069602966,-0.16870683431625366,0.031099949032068253,-0.11353231221437454,0.1034160777926445,0.16126897931098938,0.18751706182956696,-0.09906908869743347,-0.16870683431625366,0.06002639979124069,0.18591003119945526,0.06002639979124069,0.1468057632446289,0.1034160777926445,-0.11353232711553574,-0.15692198276519775,0.11787930130958557,0.1034160777926445,-0.16870683431625366,0.002173495013266802,-0.07014264166355133,0.016636719927191734,0.06002639979124069,0.16126897931098938,0.04556317627429962,-0.16870683431625366,0.016636721789836884,0.18983832001686096,0.18751706182956696,-0.15692198276519775,-0.05567941069602966,-0.0846058651804924,-0.11353231221437454,-0.07014264166355133,0.11787930130958557,0.16126897931098938,0.1468057632446289,0.13234253227710724,-0.0846058651804924,0.002173495013266802,-0.05567941069602966,0.031099949032068253,-0.07014264166355133,0.11787930130958557,-0.11353232711553574,0.04556317254900932,-0.16870683431625366,0.17573222517967224,0.07448962330818176,-0.0846058651804924,-0.1669212281703949,-0.0846058651804924,-0.11353232711553574,-0.0846058651804924,-0.041216179728507996,-0.11353231221437454,0.17573222517967224,0.11787930130958557,0.002173495013266802,0.13234253227710724,0.1034160777926445,-0.05567941069602966,0.04556317254900932,-0.1279955357313156,0.04556317254900932,0.1034160777926445,-0.16870683431625366,-0.15692198276519775,0.13234253227710724,0.1034160777926445,0.002173495013266802,0.002173495013266802,-0.16870681941509247,0.17573222517967224,-0.15692198276519775,0.1034160777926445,-0.1676354855298996,0.04556317254900932,0.11787930130958557,0.18751706182956696,-0.026752958074212074,0.04556317254900932,0.1034160777926445,0.1468057781457901,0.016636721789836884,-0.026752958074212074,-0.1279955357313156,-0.026752958074212074,0.18751706182956696,0.002173495013266802,0.06002639979124069,-0.16870683431625366,0.031099947169423103,-0.04121618717908859,-0.05567941069602966,0.07448962330818176,-0.04121618717908859,-0.04121618717908859,-0.1279955357313156,-0.04121618717908859,-0.07014263421297073,-0.026752958074212074,0.17573222517967224,-0.026752958074212074,-0.16870683431625366,0.11787930130958557,-0.041216183453798294,0.13234253227710724,-0.09906910359859467,0.1034160777926445,-0.17084954679012299,0.1468057632446289,0.18751706182956696,0.06002639979124069,-0.026752958074212074,-0.012289732694625854,0.08895284682512283,-0.1604931503534317,0.002173495013266802,0.1468057632446289,-0.11353232711553574,-0.09906910359859467,-0.04121618717908859,0.18751706182956696,0.016636721789836884,-0.16870683431625366,0.18751706182956696,0.002173495013266802,-0.04121618717908859,-0.026752958074212074,-0.1279955357313156,-0.15692198276519775,0.002173495013266802,-0.09906910359859467,0.06002639979124069,0.016636721789836884,-0.012289732694625854,0.1894811987876892,0.08895284682512283,-0.07014264166355133,0.18751706182956696,0.18751706182956696,0.16126897931098938,0.07448962330818176,0.08895284682512283,0.08895284682512283,0.016636721789836884,0.18751706182956696,-0.16870683431625366,0.1034160777926445,-0.041216179728507996,-0.05567941069602966,0.18591003119945526,0.1468057632446289,-0.1279955506324768,0.17573222517967224,0.06002639979124069,0.17573222517967224,0.04556317254900932,0.04556317254900932,0.06002639979124069,0.04556317254900932,0.002173495013266802,0.07448963075876236,-0.14245876669883728,0.07448962330818176,-0.012289732694625854,0.06002639979124069,0.08895284682512283,0.13234253227710724,0.031099947169423103,0.04556317254900932,0.07448963075876236,-0.14245876669883728,-0.0846058651804924,-0.14245876669883728,0.08895284682512283,0.1468057781457901,0.18751706182956696,0.031099947169423103,0.11787930130958557,0.17876769602298737,0.002173495013266802,0.04556317254900932,0.18751706182956696,-0.026752958074212074,0.04556317254900932,-0.1279955357313156,0.08895284682512283,-0.07014264166355133,0.16126897931098938,0.16126897931098938,-0.14245876669883728,-0.127995565533638,0.04556317254900932,-0.0846058651804924,-0.05567941069602966,0.016636721789836884,0.08895284682512283,-0.14245876669883728,0.031099949032068253,-0.026752958074212074,0.07448962330818176,0.07448962330818176,0.1468057632446289,0.016636721789836884,0.06002639979124069,-0.05567941069602966,-0.026752958074212074,-0.07014264166355133,0.1468057632446289,-0.16870683431625366,-0.11353232711553574,-0.16870681941509247,0.002173495013266802,0.1034160777926445,0.18751706182956696,0.07448963075876236,0.07448962330818176,-0.084605872631073,-0.11353232711553574,0.04556317254900932,0.17573222517967224,-0.07014264166355133,0.18591006100177765,0.1034160777926445,0.016636721789836884,0.17573222517967224,-0.1279955357313156,0.016636721789836884,0.18983832001686096,0.002173495013266802,-0.04121618717908859,0.16126897931098938,0.1468057632446289,0.17876769602298737,0.13234253227710724,0.1468057632446289,0.07448963075876236,-0.1279955357313156,-0.012289732694625854,-0.1279955357313156,-0.09906910359859467,0.13234251737594604,0.031099947169423103,-0.026752958074212074,0.13234253227710724,-0.084605872631073,-0.04121618717908859,0.18751706182956696,0.1468057781457901,-0.15692198276519775,-0.09906908869743347,-0.012289733625948429,0.031099947169423103,-0.012289732694625854,-0.127995565533638,0.11787930130958557,-0.11353232711553574,0.07448963075876236,-0.09906908869743347,0.18751706182956696,-0.07014264166355133,0.16126897931098938,-0.026752958074212074,-0.16870683431625366,-0.16870681941509247,-0.09906910359859467,0.1034160777926445,0.18751706182956696,-0.012289732694625854,0.04556317254900932,0.17573222517967224,-0.012289732694625854,-0.026752958074212074,0.13234253227710724,0.13234253227710724,-0.026752958074212074,0.04556317254900932,-0.012289732694625854,0.016636721789836884,-0.1279955357313156,0.1468057632446289,0.1468057632446289,-0.11353232711553574,-0.07014264166355133,0.11787930130958557,0.016636721789836884,-0.07014264166355133,0.0021734952460974455,0.06002639979124069,-0.15692199766635895,0.18751706182956696,0.11787930130958557,-0.16870683431625366,-0.04121618717908859,-0.0846058651804924,-0.11353232711553574,0.16126897931098938,-0.1279955357313156,-0.11353231221437454,0.08895284682512283,0.13234253227710724,-0.0846058651804924,0.17573219537734985,0.1468057632446289,-0.0846058651804924,-0.15692199766635895,0.06002639979124069,0.016636719927191734,-0.11353231221437454,0.16126897931098938,0.18751706182956696,0.16126897931098938,-0.012289732694625854,0.1468057632446289,0.016636721789836884,0.08895284682512283,-0.16299296915531158,0.07448962330818176,-0.11353232711553574,-0.04121618717908859,-0.012289732694625854,-0.012289732694625854,-0.15977893769741058,0.06002639979124069,0.07448962330818176,-0.05567941069602966,-0.012289732694625854,-0.05567941069602966,0.18751706182956696,0.18983832001686096,-0.084605872631073,-0.11353232711553574,0.17573222517967224,-0.16870681941509247,0.016636719927191734,-0.16870683431625366,-0.07014264166355133,0.08895284682512283,-0.012289732694625854,-0.11353232711553574,0.17876769602298737,-0.0846058651804924,0.1034160777926445,-0.09906908869743347,-0.0846058651804924,0.06002639979124069,-0.012289732694625854,0.1468057632446289,0.07448962330818176,0.031099947169423103,0.16126897931098938,-0.1669212281703949,0.13234253227710724,0.16555438935756683,-0.07014264166355133,0.08895284682512283,-0.041216179728507996,0.1034160777926445,0.06002639979124069,0.07448962330818176,-0.16870681941509247,0.18751706182956696,-0.14245876669883728,-0.0846058651804924,0.002173495013266802,-0.16870683431625366,0.04556317254900932,-0.026752958074212074,-0.05567941069602966,0.07448962330818176,0.13234253227710724,0.06002639979124069,0.13234253227710724,0.031099949032068253,0.07448962330818176,-0.11353232711553574,0.002173495013266802,0.002173495013266802,-0.16870683431625366,0.07448962330818176,0.17573222517967224,0.1034160777926445,0.07448962330818176,-0.1279955357313156,0.18751706182956696,-0.16870683431625366,-0.09906908869743347,0.04556317999958992,0.031099947169423103,0.17573222517967224,-0.15692198276519775,0.17573222517967224,0.031099949032068253,0.17876769602298737,0.11787930130958557,0.08895284682512283,0.1468057632446289,0.17573222517967224,-0.026752958074212074,0.16126897931098938,-0.16870683431625366,-0.05567941069602966,-0.09906910359859467,0.17573222517967224,0.08895284682512283,-0.15692198276519775,0.17573222517967224,0.08895284682512283,0.06002639979124069,0.031099947169423103,-0.14245876669883728,0.08895284682512283,-0.15977893769741058,-0.026752958074212074,0.16126897931098938,0.07448963075876236,0.1468057632446289,0.08895284682512283,0.18751706182956696,-0.0846058651804924],\"y\":[0.019100837409496307,0.14926989376544952,-0.05321529507637024,0.14926989376544952,0.09141696244478226,0.13480666279792786,-0.12553143501281738,0.14926989376544952,0.13480666279792786,-0.16624273359775543,0.18998117744922638,0.004637609701603651,0.019100835546851158,0.07695373892784119,0.1637330949306488,0.18998117744922638,0.13480666279792786,0.033564064651727676,0.1810532659292221,-0.05321529507637024,0.06249050796031952,-0.13999463617801666,0.048027291893959045,0.019100837409496307,0.019100837409496307,0.18998117744922638,0.062490515410900116,-0.03875207155942917,-0.06767851859331131,-0.16624273359775543,0.004637609701603651,-0.06767851859331131,0.14926989376544952,0.09141696989536285,0.1637330949306488,0.16462589800357819,-0.0242888443171978,0.120343416929245,0.004637609701603651,-0.11106821149587631,-0.13999463617801666,0.14926989376544952,0.12034342437982559,0.14926989376544952,-0.12553143501281738,0.07695373892784119,-0.12553143501281738,-0.13999465107917786,0.03356406092643738,-0.09660498052835464,0.07695373892784119,-0.12553143501281738,0.048027291893959045,0.048027291893959045,-0.13999465107917786,-0.16624271869659424,-0.15445786714553833,0.07695373892784119,0.12034342437982559,0.12034342437982559,-0.11106819659471512,0.06249050796031952,-0.03875207155942917,0.033564064651727676,0.07695373892784119,0.17819632589817047,0.019100837409496307,0.033564064651727676,-0.00982561707496643,0.019100837409496307,-0.13999465107917786,0.12034342437982559,0.17819632589817047,0.13480666279792786,0.048027291893959045,0.18819560110569,0.004637609701603651,0.048027291893959045,-0.00982561707496643,0.18998117744922638,0.120343416929245,0.048027291893959045,0.033564064651727676,0.09141696989536285,0.07695373892784119,0.18998117744922638,-0.08214174956083298,0.019100837409496307,-0.00982561707496643,0.1637330949306488,-0.0242888443171978,-0.09660496562719345,-0.05321529507637024,-0.0242888443171978,0.17819632589817047,-0.09660498052835464,-0.13999465107917786,0.09141696989536285,-0.02428884245455265,0.10588020086288452,0.03356406092643738,-0.00982561707496643,0.12034342437982559,0.019100837409496307,-0.02428884245455265,-0.08214174956083298,-0.16624273359775543,-0.00982561707496643,-0.05321529507637024,-0.009825615212321281,-0.09660498052835464,0.07695373892784119,0.10588018596172333,0.12034342437982559,0.062490515410900116,-0.06767851859331131,-0.05321529507637024,-0.08214174956083298,0.17819632589817047,-0.11106821149587631,0.14926989376544952,0.10588020086288452,0.10588020086288452,-0.15445786714553833,0.17819632589817047,0.062490515410900116,0.18998117744922638,-0.16624271869659424,0.13480664789676666,0.10588020086288452,-0.15445786714553833,0.1637330949306488,-0.09660498052835464,0.062490515410900116,0.048027291893959045,-0.16624271869659424,0.07695373892784119,0.17819632589817047,-0.16624271869659424,0.18998117744922638,-0.05321529507637024,-0.05321529507637024,0.1812318116426468,-0.06767851859331131,-0.11106821149587631,-0.05321529507637024,0.1921238899230957,0.09141696989536285,0.10588018596172333,-0.03875207155942917,0.10588020086288452,-0.03875207155942917,-0.15445786714553833,0.033564064651727676,-0.11106821149587631,0.004637609701603651,0.1637330949306488,-0.06767851859331131,0.09141696989536285,-0.03875207155942917,0.09141696244478226,0.004637609701603651,-0.05321529507637024,-0.13999463617801666,0.06249050796031952,0.18998117744922638,-0.12553143501281738,0.07695373892784119,-0.15445786714553833,-0.11106819659471512,0.004637609701603651,0.09141696244478226,-0.06767851859331131,-0.009825615212321281,-0.16624273359775543,-0.15802903473377228,0.09141696989536285,0.13480666279792786,0.17819632589817047,0.004637609701603651,-0.16624271869659424,0.004637609701603651,0.18998117744922638,-0.1649928092956543,0.18998117744922638,0.019100837409496307,-0.1649928092956543,0.07695373892784119,0.18998117744922638,-0.0242888443171978,-0.12553143501281738,-0.12553143501281738,0.18998117744922638,0.18123182654380798,-0.09660498052835464,-0.08214174956083298,0.004637609701603651,-0.16838540136814117,0.10588020086288452,0.062490515410900116,0.18998117744922638,0.07695373892784119,0.062490515410900116,0.019100837409496307,0.03356406092643738,0.07695373892784119,0.033564064651727676,0.07695373892784119,-0.15445786714553833,-0.00982561707496643,-0.16624273359775543,-0.1255314201116562,-0.00982561707496643,-0.16624273359775543,0.1842672973871231,0.14926989376544952,-0.06767851859331131,0.18998117744922638,0.062490515410900116,0.18998117744922638,-0.11106821149587631,-0.02428884245455265,0.033564064651727676,0.18998117744922638,-0.05321529507637024,-0.02428884245455265,0.12034342437982559,-0.16624271869659424,-0.13999465107917786,0.14926989376544952,0.1810532659292221,-0.02428884245455265,0.14926989376544952,-0.06767851859331131,-0.1255314201116562,-0.08214174956083298,0.12034342437982559,-0.15445786714553833,-0.12553143501281738,-0.16624273359775543,-0.03875207155942917,0.09141696989536285,0.1637330949306488,-0.02428884245455265,-0.11106821149587631,-0.13999465107917786,0.033564064651727676,-0.06767851859331131,-0.1649928092956543,-0.15802903473377228,0.14926989376544952,0.09141696989536285,0.048027291893959045,0.048027291893959045,-0.11106821149587631,0.1637331247329712,-0.1255314201116562,0.1637330949306488,0.004637609701603651,-0.12553143501281738,0.062490515410900116,-0.03875207155942917,-0.08214174956083298,0.048027291893959045,-0.15445786714553833,-0.16820687055587769,-0.03875207155942917,-0.06767851859331131,0.18998117744922638,-0.16517135500907898,-0.16624273359775543,0.10588020086288452,0.062490515410900116,0.004637609701603651,-0.08214174956083298,0.09141696244478226,-0.15445786714553833,0.18837416172027588,0.033564064651727676,-0.1649928092956543,0.09141696989536285,0.13480666279792786,-0.09660498052835464,-0.08214174956083298,-0.15445786714553833,0.120343416929245,-0.13999465107917786,-0.11106821149587631,-0.06767851859331131,-0.00982561707496643,0.14926989376544952,0.048027291893959045,0.17819632589817047,0.062490515410900116,0.09141696989536285,-0.13999465107917786,-0.12553143501281738,-0.09660498052835464,0.07695373892784119,-0.00982561707496643,-0.16624271869659424,0.07695373892784119,-0.00982561707496643,-0.15445786714553833,0.048027291893959045,0.12034342437982559,0.13480666279792786,0.1881955862045288,0.18998117744922638,-0.0242888443171978,-0.12553143501281738,-0.13999463617801666,-0.09660498052835464,0.07695373892784119,-0.08214174956083298,0.004637609701603651,0.004637609701603651,0.14926989376544952,0.1637330949306488,0.06249050796031952,0.13480664789676666,-0.16624271869659424,0.03356406092643738,0.14926989376544952,0.09141696989536285,-0.02428884245455265,0.14926989376544952,0.09141696989536285,0.14926989376544952,0.033564064651727676,0.06249050796031952,-0.09660498052835464,0.17819632589817047,-0.00982561707496643,-0.03875207155942917,-0.12553143501281738,0.18998117744922638,-0.03875207155942917,-0.0242888443171978,0.12034342437982559,-0.12553143501281738,-0.00982561707496643,-0.13999465107917786,-0.03875207155942917,0.14926989376544952,-0.16624273359775543,0.019100837409496307,0.033564064651727676,-0.09660498052835464,0.16373310983181,0.062490515410900116,-0.12553143501281738,-0.08214174956083298,0.07695373892784119,0.12034342437982559,-0.03875207155942917,0.17819632589817047,-0.12553143501281738,0.033564064651727676,0.048027291893959045,0.004637609701603651,0.120343416929245,0.12034342437982559,-0.09660498052835464,0.048027291893959045,0.17819632589817047,0.13480666279792786,-0.05321529507637024,-0.09660498052835464,-0.16624273359775543,0.03356406092643738,-0.05321529507637024,0.18837416172027588,0.12034342437982559,0.10588020086288452,0.004637609701603651,-0.08214174956083298,-0.13999463617801666,-0.02428884245455265,-0.16624273359775543,-0.05321529507637024,-0.16624273359775543,-0.11106821149587631,-0.05321529507637024,0.048027291893959045,0.033564064651727676,-0.16624273359775543,-0.06767851859331131,-0.09660498052835464,-0.1578504741191864,0.019100837409496307,-0.09660498052835464,-0.13999465107917786,-0.12553143501281738,-0.13999465107917786,0.18998117744922638,-0.16624273359775543,-0.06767851859331131,-0.11106819659471512,-0.15445786714553833,0.13480666279792786,0.03356406092643738,0.033564064651727676,0.004637609701603651,-0.06767851859331131,-0.16624273359775543,0.13480664789676666,0.1637330949306488,0.10588020086288452,0.07695373892784119,-0.09660498052835464,0.12034342437982559,-0.16517135500907898,0.048027291893959045,-0.06767851859331131,0.07695373892784119,-0.13999465107917786,0.18998117744922638,-0.08214174956083298,0.10588020086288452,0.13480666279792786,0.13480666279792786,-0.0242888443171978,0.18998117744922638,0.10588020086288452,-0.05321529507637024,-0.16624271869659424,-0.08214174211025238,0.09141696989536285,0.019100837409496307,-0.11106821149587631,-0.16624273359775543,-0.06767851859331131,-0.11106821149587631,0.048027291893959045,0.10588020086288452,0.07695373892784119,-0.13999465107917786,0.18819560110569,-0.03875207155942917,-0.06767851859331131,0.004637609701603651,0.07695373892784119,0.048027291893959045,-0.03875207155942917,-0.09660498052835464,0.13480666279792786,-0.09660498052835464,-0.0242888443171978,0.10588020086288452,0.14926989376544952,-0.12553143501281738,-0.00982561707496643,-0.13999465107917786,-0.08214174956083298,-0.09660498052835464,0.004637609701603651,-0.08214174956083298,0.019100837409496307,-0.03875207155942917,0.14926989376544952,0.06249050796031952,0.1637330949306488,0.004637609701603651,-0.06767851859331131,-0.03875207155942917,-0.05321529507637024,0.13480664789676666,-0.16624273359775543,0.048027291893959045,0.10588020086288452,0.1637330949306488,-0.15802903473377228,0.18998117744922638,0.09141696989536285,0.18998117744922638,0.033564064651727676,-0.06767851859331131,0.18998117744922638,0.1921238899230957,0.019100837409496307,0.1637330949306488,-0.16624273359775543,-0.05321529507637024,-0.0242888443171978,0.1637330949306488,0.18998117744922638,-0.11106821149587631,-0.00982561707496643,-0.09660498052835464,0.13480666279792786,0.12034342437982559,0.1637331247329712,0.033564064651727676,0.09141696989536285,-0.03875207155942917,0.09141696989536285,-0.06767851859331131,-0.11106821149587631,0.16480445861816406,0.14926989376544952,0.07695373892784119,-0.11106821149587631,-0.06767851859331131,0.13480666279792786,-0.15445786714553833,0.03356406092643738,-0.00982561707496643,0.1637330949306488,0.120343416929245,-0.00982561707496643,0.17819632589817047,0.18998117744922638,0.10588020086288452,0.18837416172027588,0.033564064651727676,-0.15445786714553833,-0.03875207155942917,0.019100835546851158,-0.09660498052835464,-0.06767851859331131,0.062490515410900116,-0.08214174956083298,0.09141696989536285,-0.05321529507637024,0.12034342437982559,-0.15445786714553833,-0.05321529507637024,0.033564064651727676,-0.0242888443171978,0.12034342437982559,-0.16624273359775543,-0.16624273359775543,0.18998117744922638,-0.03875207155942917,-0.11106819659471512,-0.11106821149587631,0.10588020086288452,0.019100837409496307,-0.15445786714553833,0.09141696244478226,0.062490515410900116,0.019100837409496307,-0.16892109811306,0.17819632589817047,0.18998117744922638,0.048027291893959045,-0.08214174211025238,-0.08214174956083298,-0.03875207155942917,-0.15445786714553833,0.019100835546851158,-0.00982561707496643,-0.11106821149587631,-0.09660498052835464,-0.16624273359775543,-0.09660498052835464,0.18998117744922638,-0.00982561707496643,-0.15445786714553833,-0.16624273359775543,0.1637330949306488,-0.09660498052835464,0.14926989376544952,0.17819632589817047,-0.12553143501281738,-0.16624273359775543,0.17819632589817047,-0.13999465107917786,0.13480666279792786,0.18998117744922638,-0.15445786714553833,-0.03875207155942917,0.12034342437982559,-0.16624273359775543,-0.02428884245455265,0.07695373892784119,-0.12553143501281738,-0.13999465107917786,0.03356406092643738,0.13480666279792786,-0.02428884245455265,0.07695373892784119,0.17819632589817047,0.1637330949306488,0.17819632589817047,0.18837416172027588,0.019100837409496307,0.019100837409496307,0.062490515410900116,-0.09660498052835464,-0.16624271869659424,0.18123182654380798,-0.03875207155942917,0.12034342437982559,0.062490515410900116,-0.12553143501281738,0.19230243563652039,-0.16624271869659424,-0.08214174956083298,0.10588018596172333,0.062490515410900116,-0.05321529507637024,-0.08214174211025238,0.062490515410900116,0.18998117744922638,-0.06767851859331131,0.13480666279792786,0.004637609701603651,-0.009825615212321281,0.07695374637842178,-0.08214174956083298,-0.16624273359775543,0.09141696989536285,0.17819632589817047,-0.02428884245455265,0.09141696989536285,0.004637609701603651,0.10588020086288452,-0.05321529507637024,-0.009825615212321281,-0.11106821149587631,-0.12553143501281738,0.062490515410900116,-0.13999465107917786,-0.1255314201116562,-0.15445786714553833,-0.03875207155942917,0.06249050796031952,0.019100837409496307,0.18998117744922638,-0.05321529507637024,-0.03875207155942917,0.12034342437982559,0.14926986396312714,-0.08214174956083298,0.019100837409496307,-0.05321529507637024,-0.08214174956083298,-0.06767851859331131,0.13480666279792786,0.062490515410900116,-0.00982561707496643,0.18998117744922638,0.004637609701603651,0.048027291893959045,0.048027291893959045,0.18998117744922638,0.06249050796031952,0.048027291893959045,-0.16624273359775543,-0.16624271869659424,-0.16624273359775543,0.1637330949306488,0.10588020086288452,-0.03875207155942917,0.14926989376544952,-0.08214174211025238,-0.05321529507637024,0.120343416929245,-0.16624273359775543,0.048027291893959045,0.1637330949306488,0.17819632589817047,-0.0242888443171978,0.10588020086288452,-0.12553143501281738,0.13480664789676666,0.03356406092643738,0.13480664789676666,-0.03875207155942917,-0.11106821149587631,-0.15445786714553833,0.17819632589817047,0.07695373892784119,0.048027291893959045,0.17819632589817047,-0.0242888443171978,-0.03875207155942917,-0.12553143501281738,-0.16749262809753418,0.18998117744922638,0.10588020086288452,0.17819632589817047,-0.15445786714553833,0.13480666279792786,-0.16909964382648468,-0.11106821149587631,0.14926989376544952,-0.02428884245455265,-0.13999465107917786,0.019100835546851158,-0.12553143501281738,-0.1578504741191864,0.004637609701603651,0.062490515410900116,0.004637609701603651,-0.08214174956083298,0.12034342437982559,0.14926989376544952,0.14926989376544952,0.10588020086288452,0.18998117744922638,-0.09660498052835464,-0.16892109811306,0.1637330949306488,-0.00982561707496643,0.13480666279792786,-0.16624273359775543,0.004637609701603651,0.10588020086288452,0.18998117744922638,-0.06767851859331131,0.07695373892784119,-0.09660498052835464,-0.16517135500907898,-0.09660498052835464,0.19194531440734863,-0.08214174956083298,-0.08214174956083298,0.18998117744922638,-0.05321529507637024,0.07695373892784119,-0.009825615212321281,0.03356406092643738,0.14926989376544952,-0.06767851859331131,0.019100837409496307,0.10588020086288452,0.13480666279792786,-0.13999465107917786,0.17819632589817047,-0.11106821149587631,0.09141696989536285,-0.13999465107917786,0.18998117744922638,0.18998117744922638,0.019100835546851158,0.048027291893959045,-0.05321529507637024,-0.13999465107917786,0.004637609701603651,-0.11106821149587631,0.18998117744922638,0.048027291893959045,0.14926989376544952,0.1637330949306488,-0.06767851859331131,0.13480666279792786,0.1637330949306488,0.1637330949306488,-0.05321529507637024,0.10588020086288452,-0.0242888443171978,-0.08214174211025238,-0.11106821149587631,-0.11106821149587631,0.19230243563652039,-0.11106821149587631,-0.02428884245455265,0.09141696989536285,0.09141696989536285,-0.00982561707496643,-0.0242888443171978,0.10588020086288452,-0.15445786714553833,-0.13999463617801666,0.019100837409496307,0.1637330949306488,-0.0242888443171978,-0.00982561707496643,0.048027291893959045,-0.0676785260438919,-0.05321529507637024,-0.03875207155942917,0.09141696989536285,-0.16909965872764587,0.18998117744922638,0.120343416929245,0.004637609701603651,0.019100837409496307,-0.13999465107917786,-0.00982561707496643,-0.16624273359775543],\"z\":[-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344,-0.005942010786384344],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"val\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b71aca92-9f17-4db3-a7d4-3a7bc9c70cf4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _after_fork at 0x7893984f5b40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1622, in _after_fork\n",
            "      File \"/usr/lib/python3.10/_weakrefset.py\", line 66, in __iter__\n",
            "threads.update(_dangling)\n",
            "    item = itemref()\n",
            "KeyboardInterrupt: "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-90b34f29830d>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m   \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_handling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_SIGCHLD_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_pids_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# prime the prefetch loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0midx_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0midx_in_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "hidden_dim = 10\n",
        "start_mode = True\n",
        "valid_mode = False\n",
        "index_mode = True\n",
        "\n",
        "# TODO: Visualize conv layer output\n",
        "###\n",
        "# config = {\n",
        "#     \"features\": {\n",
        "#         \"pre-set\": channels,\n",
        "#         \"sets\":    [1024,    1024,    1,],\n",
        "#         \"samples\": [25,      1,       1024,],\n",
        "#     },\n",
        "#     \"params\": {\n",
        "#         \"sets\":    [4,       1,       num_classes,],\n",
        "#         \"samples\": [(76, 1), (1, 5),  (1025, 1),],\n",
        "#         \"is conv\": [True,    True,    False,]\n",
        "#     },\n",
        "# }\n",
        "###\n",
        "# config = {\n",
        "#     \"features\": {\n",
        "#         \"pre-set\": channels,\n",
        "#         \"sets\":    [784,     784,     784,     1,],\n",
        "#         \"samples\": [9,       9,       1,       784,],\n",
        "#     },\n",
        "#     \"params\": {\n",
        "#         \"sets\":    [8,       16,      1,       num_classes,],\n",
        "#         \"samples\": [(3, 1),  (9, 1),  (1, 4),  (28, 1),],\n",
        "#         \"is conv\": [True,    True,    True,    False,]\n",
        "#     },\n",
        "# }\n",
        "###\n",
        "config = {\n",
        "    \"features\": {\n",
        "        \"pre-set\": channels,\n",
        "        \"sets\":    [784, 784, 1,],\n",
        "        \"samples\": [9, 9, 784,],\n",
        "    },\n",
        "    \"params\": {\n",
        "        \"sets\":    [1, 1, num_classes,],\n",
        "        \"samples\": [9, 9, 784,],\n",
        "        \"is conv\": [False, False, False,]\n",
        "    },\n",
        "}\n",
        "\n",
        "tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "conv_params = int(np.dot(config[\"params\"][\"sets\"][:-1], tot_samples[:-1]))\n",
        "n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "\n",
        "if start_mode:\n",
        "  model = MModule4(\n",
        "      config=config,\n",
        "      idx_dim=idx_dim,\n",
        "      device=device,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  # optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "  widx0 = model.MW.idx[:, :conv_params].clone().detach()\n",
        "  train_log = {\n",
        "      \"train loss\": [],\n",
        "      \"eval loss\": [],\n",
        "      \"acc\": [],\n",
        "      \"set\": [],\n",
        "      \"epoch\": [],\n",
        "  }\n",
        "  epoch = 0\n",
        "\n",
        "num_epochs = 720 * 4\n",
        "epoch_len = 10 # 60\n",
        "\n",
        "while epoch < num_epochs:\n",
        "  epoch += 1\n",
        "  ###\n",
        "  # widx_diff = (model.MW.idx[:, :conv_params] - widx0)\n",
        "  # print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    ###\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    _loss = loss.item()\n",
        "    # loss += 1e-1 * torch.cat(model._penalties, dim=0).sum()\n",
        "    ###\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(_loss)\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    metric_cols = [\"eval loss\", \"acc\"]\n",
        "    set_val = \"eval\"\n",
        "  else:\n",
        "    metric_cols = [\"train loss\",]\n",
        "    set_val = \"train\"\n",
        "  if index_mode:\n",
        "    _layer = 2\n",
        "    _batchidx = 0\n",
        "    pool = model.all_pools[_layer]\n",
        "    pool = MTensor.reshape(pool[_batchidx], (-1,))\n",
        "    display.clear_output(wait=True)\n",
        "    plot_features(pool)\n",
        "    from time import sleep\n",
        "    sleep(3)\n",
        "    #\n",
        "    # pool = model.all_samples[_layer - 1]\n",
        "    # _shape = (\n",
        "    #     config[\"features\"][\"sets\"][_layer - 1],\n",
        "    #     config[\"features\"][\"samples\"][_layer - 1],\n",
        "    # )\n",
        "    # _set = (config[\"features\"][\"sets\"][_layer - 1]) // 2\n",
        "    # pool = MTensor.reshape(pool[_batchidx], _shape)[_set]\n",
        "    # display.clear_output(wait=True)\n",
        "    # plot_features(pool)\n",
        "    # from time import sleep\n",
        "    # sleep(3)\n",
        "  else:\n",
        "    group_cols = [\"epoch\"] + metric_cols\n",
        "    df_train = pd.DataFrame(train_log)\n",
        "    df_train = df_train[df_train[\"set\"] == set_val]\n",
        "    display.clear_output(wait=True)\n",
        "    (\n",
        "      df_train[group_cols]\n",
        "      .groupby(\"epoch\")\n",
        "      .agg(lambda x: x.median(skipna=True))\n",
        "      .reset_index()\n",
        "      .sort_values(\"epoch\", ascending=True)\n",
        "      .tail(30)[metric_cols]\n",
        "      .plot(figsize=(16, 3), grid=True)\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Hm-pCJqjTm"
      },
      "outputs": [],
      "source": [
        "# tidx = idxu.reshape(32, -1, 3)[0].cpu().detach().numpy()\n",
        "# tidx = idxu.reshape(32, -1, 18, 3)[0, 0].cpu().detach().numpy()\n",
        "# tidx = idxv.reshape(-1, 3).cpu().detach().numpy()\n",
        "## tidx = model.W_idx[0, :18].cpu().detach().numpy()\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "##\n",
        "# phi = idxu[0] @ idxv[0].T\n",
        "# import seaborn as sns\n",
        "# sns.heatmap(phi.cpu().detach().numpy()); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-K_7fUh2anJ"
      },
      "source": [
        "### Visualização dos índices dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNJnesCt2f1p"
      },
      "outputs": [],
      "source": [
        "soft_W_idx = scaled_idx # MTensor._soft_kernel(model.MW.idx, img_dim)\n",
        "threshold = 100 # rows * cols * hidden_dim\n",
        "# First layer\n",
        "soft_W_idx = soft_W_idx[:, :threshold].reshape(1, -1, idx_dim)\n",
        "# Last layer\n",
        "# soft_W_idx = soft_W_idx[:, threshold:].reshape(1, -1, idx_dim)\n",
        "soft_W_idx = soft_W_idx.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gaTHY1L2it1"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "sample_idx = np.random.choice(\n",
        "    len(soft_W_idx),\n",
        "    min(len(soft_W_idx), 10000),\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "W_idx_tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=10,\n",
        ").fit_transform(soft_W_idx[sample_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows6icrE2unf"
      },
      "outputs": [],
      "source": [
        "plot_df = pd.DataFrame(\n",
        "    {\n",
        "        \"W_idx x tsne\": W_idx_tsne[:, 0],\n",
        "        \"W_idx y tsne\": W_idx_tsne[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "plot_df.plot.scatter(\n",
        "    x=\"W_idx x tsne\",\n",
        "    y=\"W_idx y tsne\",\n",
        "    figsize=(24, 4),\n",
        "    grid=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUDvxRS3yZu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "RGCfrrmCXap_",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "kTfYY3SQXNJF",
        "1SknOTQ7O9BS",
        "QQRFtDATXUmH",
        "039kGqbPXp4d",
        "8_m1YvjxBdj9",
        "Y-K_7fUh2anJ"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyN7rvdbgD+AigdBnq4SnvOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}