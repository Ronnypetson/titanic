{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "j6dxGxcHAx5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb3f994-e1e3-4031-8ae7-fbb0e3781009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "tr = ToTensor()\n",
        "\n",
        "img_dim = 32\n",
        "\n",
        "def _transform(x):\n",
        "  # x = x.resize((img_dim, img_dim))\n",
        "  # return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "  return (tr(x).mean(dim=0)).reshape(-1)\n",
        "\n",
        "bsize = 4 ###\n",
        "\n",
        "MNIST_train_data = CIFAR10(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = CIFAR10(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def _cat2d(rows, cols, d=32):\n",
        "  \"\"\"\n",
        "  Index in the log-softmax scale.\n",
        "  After sotmax (in the partition dimension)\n",
        "  -inf --> 0\n",
        "  1.0  --> 1\n",
        "  \"\"\"\n",
        "  assert rows + cols <= d\n",
        "  inf = 1.0\n",
        "  idx = np.zeros((rows, cols, d)) - inf\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      idx[row, col, row] = 1.0\n",
        "      idx[row, col, rows + col] = 1.0\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx\n",
        "\n",
        "def cartesian_idx(rows, cols, d=2):\n",
        "  idx = np.zeros((rows, cols, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      # idx[row, col, 0] = (1 + row) / rows\n",
        "      # idx[row, col, 1] = (1 + col) / cols\n",
        "      idx[row, col, 0] = 2.0 * ((row) / rows) - 1.0 ### (row + 1)\n",
        "      idx[row, col, 1] = 2.0 * ((col) / cols) - 1.0 ### (col + 1)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye\n",
        "\n",
        "def log_sinkhorn(log_alpha, n_iter):\n",
        "    \"\"\" https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/sampling/permutations.html\n",
        "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
        "    Concerning nonnegative matrices and doubly stochastic\n",
        "    matrices. Pacific Journal of Mathematics, 1967\n",
        "    Args:\n",
        "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
        "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
        "      n_iters: number of sinkhorn iterations\n",
        "    \"\"\"\n",
        "    for _ in range(n_iter):\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "    return log_alpha.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _sgbmd(u, v, idxu, idxv, sim=None, f=None, normalize=True) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Slow General Batch Maromba Dot\"\n",
        "  Slower, more general, implementation for the \"batch maromba dot\" operation.\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  sim: index similarity function\n",
        "  f: value function\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  sim = Pairwise(sim)\n",
        "  f = Pairwise(f)\n",
        "  ###\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  ###\n",
        "  # sims: (M * N) x 1 x (d_u * d_v)\n",
        "  # vals: (M * N) x (d_u * d_v) x d_val\n",
        "  sims = sim(idxu, idxv).reshape(m * n, 1, d_u * d_v) ###\n",
        "  norm = 1.0\n",
        "  if normalize:\n",
        "    # norm: (M * N) x 1\n",
        "    norm = sims.sum(dim=-1)\n",
        "  vals = f(u, v)\n",
        "  vals = vals.reshape(m * n, d_u * d_v, d_val)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.bmm(sims, vals).squeeze(1)\n",
        "  eps = 1e-8\n",
        "  dot = (dot / (norm + eps)).reshape(m, n, d_val)\n",
        "  return dot\n",
        "\n",
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  ###\n",
        "  # idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # mag = 80.0\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv, dim=2)\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), 6)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  from random import randint\n",
        "  bsidx = randint(0, m - 1)\n",
        "  m_ = 1 # m,\n",
        "  idxuv = (\n",
        "      idxu[bsidx].reshape(m_ * d_u, d_idx)\n",
        "      @ idxv.reshape(n * d_v, d_idx).T\n",
        "  )\n",
        "  idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  mag = 80.0\n",
        "  idxuv = (\n",
        "      log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), 6)\n",
        "      .permute(0, 2, 3, 1)\n",
        "  )\n",
        "  idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # from random import randint\n",
        "  # bsidx = randint(0, m - 1)\n",
        "  # m_ = 1\n",
        "  # mag = 80.0\n",
        "  # # idxu = log_sinkhorn(mag * idxu[bsidx].unsqueeze(0), 6)\n",
        "  # # idxv = log_sinkhorn(mag * idxv, 6)\n",
        "  # idxu = nn.functional.softmax(mag * idxu[bsidx].unsqueeze(0), dim=1)\n",
        "  # idxv = nn.functional.softmax(mag * idxv, dim=1)\n",
        "  # idxuv = idxu.reshape(m_ * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9hF3Uoi3tF"
      },
      "source": [
        "#### Índices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UrPFWDtli55C"
      },
      "outputs": [],
      "source": [
        "def _fast_kernel_idx_sum(idxu, idxv, k, idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, idx_part)\n",
        "  kidxv = k(idxv, idx_part)\n",
        "  d_idx_k = kidxu.shape[-1]\n",
        "  assert kidxu.shape[:-1] == idxu.shape[:-1]\n",
        "  assert kidxv.shape[:-1] == idxv.shape[:-1]\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx_k)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx_k)\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  # sidx: (M * d_idx) x N + (N * d_idx) x M\n",
        "  sidx = (\n",
        "      (iTki @ skj.T).reshape(m, d_idx, n).permute(0, 2, 1)\n",
        "      + (jTkj @ ski.T).reshape(n, d_idx, m).permute(2, 0, 1)\n",
        "  )\n",
        "  sidx = sidx / norm\n",
        "  sidx = sidx.repeat(batch_m, 1, 1)\n",
        "  return sidx\n",
        "\n",
        "def _fast_kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx)\n",
        "  # iTki_kjTj: M x N x d_idx x d_idx\n",
        "  iTki_kjTj = (iTki @ jTkj.T).reshape(m, d_idx, n, d_idx).permute(0, 2, 1, 3)\n",
        "  diag = torch.diagonal(iTki_kjTj, dim1=2, dim2=3)\n",
        "  ###\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  diag = diag / norm\n",
        "  ###\n",
        "  diag = diag.repeat(batch_m, 1, 1)\n",
        "  return diag\n",
        "\n",
        "def _kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # ski: (M * N) x d_idx\n",
        "  # skj: (M * N) x d_idx\n",
        "  # norm: M x N x 1\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  ski = ski.unsqueeze(1).repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "  skj = skj.unsqueeze(1).repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "  # idxu, kidxu: (M * d_u) x d_idx x 1\n",
        "  # idxv, kidxv: (N * d_v) x d_idx x 1\n",
        "  idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "  idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "  kidxu = kidxu.reshape(m * d_u, d_idx, 1)\n",
        "  kidxv = kidxv.reshape(n * d_v, d_idx, 1)\n",
        "  # sikiT: M x d_idx x d_idx\n",
        "  # sjkjT: N x d_idx x d_idx\n",
        "  sikiT = torch.bmm(idxu, kidxu.permute(0, 2, 1))\n",
        "  sikiT = sikiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "  sjkjT = torch.bmm(idxv, kidxv.permute(0, 2, 1))\n",
        "  sjkjT = sjkjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1)\n",
        "  del kidxu\n",
        "  del kidxv\n",
        "  del idxu\n",
        "  del idxv\n",
        "  # sikiT: (M * N) x d_idx x d_idx\n",
        "  # sjkjT: (M * N) x d_idx x d_idx\n",
        "  sikiT = sikiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  sjkjT = sjkjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  # diag_sikiT_skjjT: (M * N) x d_idx\n",
        "  # skjjT = sjkjT.permute(0, 2, 1)\n",
        "  # diag_sikiT_skjjT = torch.diagonal(torch.bmm(sikiT, skjjT), dim1=1, dim2=2)\n",
        "  # diag_sikiT_skjjT = diag_sikiT_skjjT.unsqueeze(-1)\n",
        "  xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski)\n",
        "  # xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski) - diag_sikiT_skjjT\n",
        "  # xor_idx = diag_sikiT_skjjT\n",
        "  xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "  xor_idx = xor_idx / norm\n",
        "  return xor_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    kernel = _soft_kernel\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    ###\n",
        "    # mdot = _rdot(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    # )\n",
        "    ###\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    ###\n",
        "    onesa = torch.ones(self.idx.shape).to(self.idx.device)\n",
        "    onesb = torch.ones(b.idx.shape).to(b.idx.device)\n",
        "    ###\n",
        "    midx = (\n",
        "        _nsbmd(aidx, onesb, aidx, bidx)\n",
        "        + _nsbmd(onesa, bidx, aidx, bidx)\n",
        "    ) / 2.0\n",
        "    ###\n",
        "    # midx = (\n",
        "    #     _nsbmd(aidx, 1.0 - bidx, aidx, bidx)\n",
        "    #     + _nsbmd(1.0 - aidx, bidx, aidx, bidx)\n",
        "    # )\n",
        "    # midx = _nsbmd(aidx, bidx, aidx, bidx)\n",
        "    ###\n",
        "    # midx = (\n",
        "    #     _rdot(aidx, onesb, aidx, bidx)\n",
        "    #     + _rdot(onesa, bidx, aidx, bidx)\n",
        "    # ) / 2.0\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "def idx2d(\n",
        "    channels: int,\n",
        "    rows: int,\n",
        "    cols: int,\n",
        "    w: int,\n",
        "    h: int,\n",
        "    stride: int=2,\n",
        "    dilation: int=1,\n",
        "    device=\"cpu\"\n",
        "  ):\n",
        "  idx = []\n",
        "  dilh = 1 + dilation * (h - 1)\n",
        "  dilw = 1 + dilation * (w - 1)\n",
        "  for row in range(0, rows - (dilh - 1), stride):\n",
        "    for col in range(0, cols - (dilw - 1), stride):\n",
        "      for ch in range(channels):\n",
        "        for drow in range(0, dilh, dilation):\n",
        "          for dcol in range(0, dilw, dilation):\n",
        "            idx.append(\n",
        "                cols * rows * ch\n",
        "                + cols * (row + drow)\n",
        "                + (col + dcol)\n",
        "            )\n",
        "  idx = torch.tensor(idx).long().to(device)\n",
        "  return idx\n",
        "\n",
        "def unsort(idxs):\n",
        "  ridxs = [0 for _ in idxs]\n",
        "  for i, idx in enumerate(idxs):\n",
        "    ridxs[idx] = i\n",
        "  ridxs = torch.tensor(ridxs).long().to(idxs.device)\n",
        "  return ridxs\n",
        "\n",
        "def get_perms(tmp_idx):\n",
        "  idxs, _idxs = [], []\n",
        "  for dim in range(tmp_idx.shape[-1]):\n",
        "    ordering = torch.argsort(tmp_idx[:, dim], stable=True)\n",
        "    idxs.append(ordering.cpu().detach())\n",
        "    _idxs.append(unsort(ordering).cpu().detach())\n",
        "  return idxs, _idxs\n",
        "\n",
        "def resort(k, src, tgt):\n",
        "  assert src == 0 or tgt == 0\n",
        "  global idxs, _idxs\n",
        "  if tgt == 0:\n",
        "    return idxs[src][k]\n",
        "  return _idxs[tgt][k]\n",
        "\n",
        "def hoods(dims, k0, w, _min=0, _max=None):\n",
        "  assert len(dims) == len(w), f\"{len(dims)} != {len(w)}\"\n",
        "  if len(dims) == 0:\n",
        "    return [k0] # [k0.item()]\n",
        "  _hoods = []\n",
        "  global idxs, _idxs\n",
        "  _k0d = resort(k0, 0, dims[-1]) #, idxs, _idxs)\n",
        "  for _w in range(-(w[-1] // 2), (w[-1] // 2) + (w[-1] % 2)):\n",
        "    # k0d = min(_max, max(_min, _k0d + _w))\n",
        "    k0d = torch.clip(_k0d + _w, min=_min, max=_max)\n",
        "    _hoods += hoods(\n",
        "        dims[:-1],\n",
        "        resort(\n",
        "            k0d,\n",
        "            dims[-1], 0,\n",
        "            # idxs, _idxs\n",
        "        ),\n",
        "        w[:-1],\n",
        "        # idxs, _idxs,\n",
        "        _min, _max\n",
        "    )\n",
        "  return _hoods\n",
        "\n",
        "idxs, _idxs = None, None\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def idxhood(xidx, ws, stride=None, num_sets=None, sample=False):\n",
        "  xidx = xidx.reshape(-1, xidx.shape[-1]).cpu().detach().numpy()\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  if sample:\n",
        "    if num_sets is None:\n",
        "      num_sets = (len(xidx) + (len(xidx) % stride)) // stride\n",
        "    subidx = np.random.choice(len(xidx), size=num_sets, replace=False)\n",
        "    all_hoods = neigh.kneighbors(\n",
        "        xidx[subidx], return_distance=False\n",
        "    ).reshape(-1)\n",
        "  else:\n",
        "    all_hoods = neigh.kneighbors(\n",
        "        xidx[::stride], return_distance=False\n",
        "    ).reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods\n",
        "\n",
        "def _idxhood(xidx, ws, stride):\n",
        "  \"\"\"\n",
        "  xidx: in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  dims = tuple(range(xidx.shape[-1]))\n",
        "  global idxs, _idxs\n",
        "  idxs, _idxs = get_perms(xidx)\n",
        "  pivots = torch.tensor([piv for piv in range(0, len(xidx), stride)]).long()\n",
        "  all_hoods = hoods(dims, pivots, ws, 0, len(xidx) - 1)\n",
        "  # all_hoods = torch.tensor(all_hoods).long().T.reshape(-1)\n",
        "  all_hoods = torch.cat(all_hoods, dim=0).reshape(len(all_hoods), -1).T\n",
        "  all_hoods = all_hoods.reshape(-1)\n",
        "  # Pdb().set_trace()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MModule III"
      ],
      "metadata": {
        "id": "4NH27yFEuqtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pandas.core.arrays.categorical import Shape\n",
        "\n",
        "class MModule3(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=3, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self._samples = samples\n",
        "    self.samples = [int(np.prod(samp)) for samp in samples]\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (1, n_params), idx_dim, device\n",
        "    )\n",
        "    if probe_dim:\n",
        "      n_classes = 10\n",
        "      self._pw, self._pw_idx, self.probe = self._make_pmt(\n",
        "          (n_classes, probe_dim), idx_dim, device\n",
        "      )\n",
        "    self.activation = nn.ELU()\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _W = nn.Parameter(\n",
        "        2.0 * torch.rand(shape, device=device) - 1.0\n",
        "    )\n",
        "    _W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((*shape, idxdim), device=device) - 1.0\n",
        "    )\n",
        "    # _W_idx = (\n",
        "    #     2.0 * torch.rand((*shape, idxdim), device=device) - 1.0\n",
        "    # )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    n = x.data.shape[0]\n",
        "    filter_whs = [(samp[1], samp[2], samp[0]) for samp in self._samples[:-1]]\n",
        "    strides = self.sets[:-1]\n",
        "    stride = strides[0]\n",
        "    filter_volume = np.prod(filter_whs[0])\n",
        "    self.all_pools = [x[:4]]\n",
        "    idxx = idxhood(x.idx[0], filter_whs[0], strides[0], sample=True) ### FIX\n",
        "    # pool: N x (num_windows * window_volume)\n",
        "    pool = x[:, idxx]\n",
        "    ###\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[0, wl: wr],\n",
        "          (self.sets[step], self.samples[step])\n",
        "      )\n",
        "      if conv:\n",
        "        # pool: (N * num_windows) x  sets\n",
        "        pool = MTensor.reshape(pool, (-1, filter_volume)) @ mw\n",
        "        pool = MTensor.reshape(pool, (n, -1, self.sets[step]))\n",
        "      else:\n",
        "        # pool.data = self.probe(pool.data)\n",
        "        # pool: N x n_classes\n",
        "        pool = pool @ self.probe\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_volume = np.prod(filter_whs[nxt_conv_step])\n",
        "        if nxt_conv:\n",
        "          idxx = idxhood(\n",
        "              pool.idx[0],\n",
        "              filter_whs[nxt_conv_step],\n",
        "              strides[nxt_conv_step],\n",
        "              sample=True,\n",
        "          ) ### FIX\n",
        "          pool = pool[:, idxx]\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ],
      "metadata": {
        "id": "VvlcR_tmuyy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MModule IV"
      ],
      "metadata": {
        "id": "vCh8kNiFl15G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _count_params(config):\n",
        "  tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "  n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "  return n_params\n",
        "\n",
        "class MModule4(nn.Module):\n",
        "  def __init__(self, config, idx_dim=3, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    assert len(config[\"features\"][\"sets\"]) > 0\n",
        "    assert len(config[\"params\"][\"sets\"]) > 0\n",
        "    assert len(config[\"features\"][\"sets\"]) == len(config[\"params\"][\"sets\"])\n",
        "    for key in [\"sets\", \"samples\"]:\n",
        "      assert len(config[\"features\"][key]) == len(config[\"features\"][\"sets\"])\n",
        "      assert len(config[\"params\"][key]) == len(config[\"params\"][\"sets\"])\n",
        "    self._idx_dim = idx_dim\n",
        "    self.device = device\n",
        "    self._config = config\n",
        "    self._config[\"params\"][\"samples\"] = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "    self._feat_samples = config[\"features\"][\"samples\"]\n",
        "    self._n_params = _count_params(config)\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (self._n_params,), idx_dim, device\n",
        "    )\n",
        "    self.activation = nn.ELU()\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _W = nn.Parameter(\n",
        "        2.0 * torch.rand(shape, device=device) - 1.0\n",
        "    )\n",
        "    _W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((*shape, idxdim), device=device) - 1.0\n",
        "    )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    n = x.data.shape[0]\n",
        "    param_sets = self._config[\"params\"][\"sets\"]\n",
        "    param_samples = self._config[\"params\"][\"samples\"]\n",
        "    feat_sets = self._config[\"features\"][\"sets\"]\n",
        "    feat_samples = self._config[\"features\"][\"samples\"]\n",
        "    self.all_pools = [x[:4]]\n",
        "    idxx = idxhood(\n",
        "        x.idx[0],\n",
        "        feat_samples[0],\n",
        "        num_sets=feat_sets[0],\n",
        "        sample=True\n",
        "    )\n",
        "    # pool: N x (num_windows * window_volume)\n",
        "    pool = x[:, idxx]\n",
        "    ###\n",
        "    wl, wr = 0, param_sets[0] * param_samples[0]\n",
        "    for step in range(n_layers):\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[wl: wr],\n",
        "          (param_sets[step], param_samples[step])\n",
        "      )\n",
        "      # pool: (N * num_windows) x sets\n",
        "      pool = MTensor.reshape(pool, (-1, feat_samples[step])) @ mw\n",
        "      pool = MTensor.reshape(pool, (n, -1))\n",
        "      if step < n_layers - 1:\n",
        "        pool.data = self.activation(pool.data)\n",
        "      else:\n",
        "        break\n",
        "      # pool: N x num_windows x numw_windows\n",
        "      self.all_pools.append(pool[:4])\n",
        "      nxt_step = step + 1\n",
        "      idxx = idxhood(\n",
        "          pool.idx[0],\n",
        "          feat_samples[nxt_step],\n",
        "          num_sets=feat_sets[nxt_step],\n",
        "          sample=True,\n",
        "      )\n",
        "      pool = pool[:, idxx]\n",
        "      next_wr = wr + param_sets[nxt_step] * param_samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ],
      "metadata": {
        "id": "PAqc0d7Kl6Ud"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  y_pred = 10.0 * y_pred.data\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 3 # rows + cols + hidden_dim + clf_dim # 3\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, d=idx_dim)\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyzd22RQX-Yg"
      },
      "source": [
        "### Treino (a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNheVxvNNK30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "a715741a-0caa-4df5-e31c-dff9c944cde0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e5120073ada2>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# model = MModule(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# model = MModule2(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   model = MModule3(\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mn_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0midx_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MModule3' is not defined"
          ]
        }
      ],
      "source": [
        "hidden_dim = 50\n",
        "start_mode = True\n",
        "valid_mode = False\n",
        "# TODO: Visualize conv layer output\n",
        "samples = [\n",
        "    # in_ch * h * w,\n",
        "    (2, 3, 3),\n",
        "    (2, 3, 3),\n",
        "    # (2, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    # (1, 3, 3),\n",
        "    hidden_dim,\n",
        "]\n",
        "\n",
        "sets = [samp[0] for samp in samples[1:-1]] + [1, num_classes]\n",
        "_samples = [int(np.prod(samp)) for samp in samples]\n",
        "conv_params = int(np.array(_samples[:-1]).dot(np.array(sets[:-1])))\n",
        "# conv_params = int(np.prod(np.array(samples[:-1])))\n",
        "n_params = int(np.array(_samples).dot(np.array(sets)))\n",
        "# n_params = conv_params + hidden_dim * num_classes\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "if start_mode:\n",
        "  # model = MModule(\n",
        "  # model = MModule2(\n",
        "  model = MModule3(\n",
        "      n_params=n_params,\n",
        "      idx_dim=idx_dim,\n",
        "      samples=samples,\n",
        "      sets=sets,\n",
        "      device=device,\n",
        "      probe_dim=hidden_dim,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  widx0 = model.W_idx[:, :conv_params].clone().detach()\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "\n",
        "num_epochs = 720 * 4\n",
        "epoch_len = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  ###\n",
        "  widx_diff = (model.W_idx[:, :conv_params] - widx0)\n",
        "  print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  else:\n",
        "    group_cols = [\"epoch\", \"train loss\"]\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  (\n",
        "    df_train[group_cols]\n",
        "    .groupby(\"epoch\")\n",
        "    .agg(lambda x: x.median(skipna=True))\n",
        "    # .tail(200)\n",
        "    .plot(figsize=(16, 3), grid=True)\n",
        "  )\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tidx = model.W_idx[0, :18].cpu().detach().numpy()\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();"
      ],
      "metadata": {
        "id": "8CcZxz9MYMwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG5gdtyeWnjJ"
      },
      "outputs": [],
      "source": [
        "# tidx = aidx.reshape(32, -1, 9, 3)[0, 100].cpu().detach().numpy(); tidx = tidx.reshape(-1, 3)\n",
        "# tidx = aidx.reshape(32, -1, 9, 2)[0].cpu().detach().numpy(); tidx = tidx.reshape(-1, 2)\n",
        "\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 1] * 0.0})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "# midx.reshape(32, -1, 3)[0, 100:105].cpu().detach().numpy()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tJHxWRO_xoX"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0uL2kM1Okfd"
      },
      "outputs": [],
      "source": [
        "imgs = [img.data.cpu().detach() for img in model.all_pools]\n",
        "idxs = [img.idx.cpu().detach() for img in model.all_pools]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj5tP_tfMAjw"
      },
      "outputs": [],
      "source": [
        "[img.shape for img in imgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRWVQRznvaer"
      },
      "outputs": [],
      "source": [
        "shapes = [\n",
        "    (img_dim, img_dim, 1),\n",
        "    ((img_dim - 0) // 1, (img_dim - 0) // 1, 1),\n",
        "    # ((img_dim - 0) // 1, (img_dim - 0) // 1, 4),\n",
        "    # ((img_dim - 0) // 1, (img_dim - 0) // 1, 8),\n",
        "    # (((img_dim - 2) // 2 - 2) // 1, ((img_dim - 2) // 2 - 2) // 1, 4),\n",
        "    # (((img_dim - 4) // 1 - 4) // 2, ((img_dim - 4) // 1 - 4) // 2, 8),\n",
        "    # ((img_dim - 4) // 2 - 2, (img_dim - 4) // 2 - 2, 4),\n",
        "    # ((img_dim - 4) // 2 - 4, (img_dim - 4) // 2 - 4, 8),\n",
        "]\n",
        "# display.clear_output(wait=True)\n",
        "plt.close()\n",
        "for idx, img in enumerate(imgs[:len(shapes)]):\n",
        "  img = img[0]\n",
        "  print(img.shape)\n",
        "  img = img.reshape(*shapes[idx])\n",
        "  rows, cols = 1, shapes[idx][2]\n",
        "  fig, ax = plt.subplots(rows, cols, figsize=(min(18, 3 * cols), 3))\n",
        "  for ch in range(cols):\n",
        "    img_ = img[:, :, ch].numpy()\n",
        "    if cols > 1:\n",
        "      ax[ch].imshow(img_)\n",
        "    else:\n",
        "      ax.imshow(img_)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgY4NUoRagWO"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def scatter3d(x, y, z):\n",
        "  plot_df = pd.DataFrame({\"x\": x, \"y\": y, \"z\": z})\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None)\n",
        "  return fig\n",
        "\n",
        "# plt.clf(); plt.cla()\n",
        "# plt.close()\n",
        "for i, idx in enumerate(idxs[:len(shapes)]):\n",
        "  idx = idx[0]\n",
        "  print(idx.shape)\n",
        "  idx = idx.reshape(-1, shapes[i][2], idx_dim)\n",
        "  rows, cols = 1 + (idx.shape[1] - 1) // 2, 2\n",
        "  # fig = plt.figure(figsize=(min(18, 3 * cols), 3))\n",
        "  fig = make_subplots(\n",
        "    rows=rows, cols=cols,\n",
        "    specs=[[{\"type\": \"scene\"} for _ in range(cols)] for _ in range(rows)],\n",
        "    # row_heights=[10 for _ in range(rows)],\n",
        "    vertical_spacing=0.05\n",
        "  )\n",
        "  fig.update_layout(\n",
        "    height=600 * rows,\n",
        "    width=600 * cols\n",
        ")\n",
        "  for ch in range(idx.shape[1]):\n",
        "    idx_ = idx[:, ch].numpy()\n",
        "    # ax = fig.add_subplot(rows, cols, ch + 1, projection=\"3d\")\n",
        "    # ax.scatter(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2], marker=\"+\")\n",
        "    # fig = scatter3d(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2])\n",
        "    row, col = (ch // cols) + 1, (ch % cols) + 1\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=idx_[::, 0],\n",
        "            y=idx_[::, 1],\n",
        "            z=idx_[::, 2],\n",
        "            # z=idx_[::, 1] * 0.0,\n",
        "            # color=None,\n",
        "            # colorscale=\"Viridis\",\n",
        "            # showscale=False\n",
        "        ),\n",
        "        row=row,\n",
        "        col=col,\n",
        "    )\n",
        "  fig.show()\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFWbwRwuE9lu"
      },
      "outputs": [],
      "source": [
        "# model.MW.idx[0, 0]\n",
        "threshold = rows * cols * hidden_dim\n",
        "param_id = 0\n",
        "layer_idx = model.MW.idx[:, param_id: param_id + 100] # threshold\n",
        "n_parts = idx_dim // img_dim\n",
        "scaled_idx = (\n",
        "    MTensor\n",
        "    ._soft_kernel(layer_idx, img_dim)[0]\n",
        "    .reshape(-1, n_parts, img_dim)\n",
        ") * (n_parts ** 0.5)\n",
        "idx_att = torch.argmax(\n",
        "    scaled_idx,\n",
        "    dim=-1\n",
        ")[:, :2]\n",
        "idx_att = idx_att.cpu().detach().numpy()\n",
        "grid = np.zeros((rows, cols))\n",
        "# for pos in range(len(idx_att)):\n",
        "#   idxx, idxy = idx_att[pos]\n",
        "#   idxx, idxy = int(idxx), int(idxy)\n",
        "#   # grid[int(idxx), int(idxy)] += 1\n",
        "#   grid[idxx, idxy] += scaled_idx[pos, 0, idxx] * scaled_idx[pos, 1, idxy]\n",
        "for idxx in range(rows):\n",
        "  for idxy in range(cols):\n",
        "    grid[idxx, idxy] = (scaled_idx[:, 0, idxx] * scaled_idx[:, 1, idxy]).sum()\n",
        "grid = grid / grid.max()\n",
        "\n",
        "# plt.imshow(grid)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.matshow(grid, cmap=\"seismic\")\n",
        "\n",
        "for (i, j), z in np.ndenumerate(grid):\n",
        "    ax.text(j, i, \"{:0.2f}\".format(z), ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSCHvt7xQq6q"
      },
      "outputs": [],
      "source": [
        "MTensor._soft_kernel(y_pred.idx, img_dim)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino (b)"
      ],
      "metadata": {
        "id": "lQbo5JtHw3bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 200\n",
        "start_mode = False\n",
        "valid_mode = False\n",
        "\n",
        "# TODO: Visualize conv layer output\n",
        "# config = {\n",
        "#     # 2 Convs + 1 fully-connected\n",
        "#     \"features\": {\n",
        "#         \"sets\":    [392,    392,    784,    1,],\n",
        "#         \"samples\": [9,      18,     18,     4 * 784,],\n",
        "#     },\n",
        "#     \"params\": {\n",
        "#         \"sets\":    [2,      4,      4,      num_classes,],\n",
        "#         \"samples\": [(3, 3), (3, 6), (3, 6), (hidden_dim, 1),],\n",
        "#     },\n",
        "# }\n",
        "\n",
        "config = {\n",
        "    # 2 Convs + 1 fully-connected\n",
        "    \"features\": {\n",
        "        \"sets\":    [1024,   1024,   2048,   4096,   1,],\n",
        "        \"samples\": [9,      9,      9,      9,      8192,],\n",
        "    },\n",
        "    \"params\": {\n",
        "        \"sets\":    [1,      2,      2,      2,      num_classes,],\n",
        "        \"samples\": [(3, 3), (3, 3), (3, 3), (3, 3), (hidden_dim, 1),],\n",
        "    },\n",
        "}\n",
        "\n",
        "tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "conv_params = int(np.dot(config[\"params\"][\"sets\"][:-1], tot_samples[:-1]))\n",
        "n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "\n",
        "if start_mode:\n",
        "  model = MModule4(\n",
        "      config=config,\n",
        "      idx_dim=idx_dim,\n",
        "      device=device,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  widx0 = model.W_idx[:, :conv_params].clone().detach()\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "\n",
        "num_epochs = 720 * 4\n",
        "epoch_len = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  ###\n",
        "  widx_diff = (model.W_idx[:, :conv_params] - widx0)\n",
        "  print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  else:\n",
        "    group_cols = [\"epoch\", \"train loss\"]\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  (\n",
        "    df_train[group_cols]\n",
        "    .groupby(\"epoch\")\n",
        "    .agg(lambda x: x.median(skipna=True))\n",
        "    # .tail(200)\n",
        "    .plot(figsize=(16, 3), grid=True)\n",
        "  )\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xGn5VTZPw-1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "f05aa232-1aa2-442a-b190-a07bc9ffdd6a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.028225397691130638 -0.14381295442581177 0.17387515306472778\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-38adf69d7879>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaromba_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mtrain_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-K_7fUh2anJ"
      },
      "source": [
        "### Visualização dos índices dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNJnesCt2f1p"
      },
      "outputs": [],
      "source": [
        "soft_W_idx = scaled_idx # MTensor._soft_kernel(model.MW.idx, img_dim)\n",
        "threshold = 100 # rows * cols * hidden_dim\n",
        "# First layer\n",
        "soft_W_idx = soft_W_idx[:, :threshold].reshape(1, -1, idx_dim)\n",
        "# Last layer\n",
        "# soft_W_idx = soft_W_idx[:, threshold:].reshape(1, -1, idx_dim)\n",
        "soft_W_idx = soft_W_idx.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gaTHY1L2it1"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "sample_idx = np.random.choice(\n",
        "    len(soft_W_idx),\n",
        "    min(len(soft_W_idx), 10000),\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "W_idx_tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=10,\n",
        ").fit_transform(soft_W_idx[sample_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows6icrE2unf"
      },
      "outputs": [],
      "source": [
        "plot_df = pd.DataFrame(\n",
        "    {\n",
        "        \"W_idx x tsne\": W_idx_tsne[:, 0],\n",
        "        \"W_idx y tsne\": W_idx_tsne[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "plot_df.plot.scatter(\n",
        "    x=\"W_idx x tsne\",\n",
        "    y=\"W_idx y tsne\",\n",
        "    figsize=(24, 4),\n",
        "    grid=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUDvxRS3yZu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "_T9hF3Uoi3tF",
        "kTfYY3SQXNJF",
        "1SknOTQ7O9BS",
        "4NH27yFEuqtg",
        "vCh8kNiFl15G",
        "QQRFtDATXUmH",
        "039kGqbPXp4d",
        "Lyzd22RQX-Yg"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOr19J4KzOcqnrtkrxWMDvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}