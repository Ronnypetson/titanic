{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6dxGxcHAx5P",
        "outputId": "6defda97-750c-435a-ddba-1c32974f8974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_root/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 237179741.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_root/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_root/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 99618169.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_root/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_root/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 64928192.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_root/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_root/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20888737.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_root/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_root_test/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 228839070.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_root_test/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_root_test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20075521.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_root_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_root_test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 63792085.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_root_test/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_root_test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15995406.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_root_test/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_root_test/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tr = ToTensor()\n",
        "\n",
        "img_dim = 28\n",
        "\n",
        "def _transform(x):\n",
        "  x = x.resize((img_dim, img_dim))\n",
        "  return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "\n",
        "bsize = 32\n",
        "\n",
        "MNIST_train_data = MNIST(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = MNIST(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def _cat2d(rows, cols, d=32):\n",
        "  \"\"\"\n",
        "  Index in the log-softmax scale.\n",
        "  After sotmax (in the partition dimension)\n",
        "  -inf --> 0\n",
        "  1.0  --> 1\n",
        "  \"\"\"\n",
        "  assert rows + cols <= d\n",
        "  inf = 1.0\n",
        "  idx = np.zeros((rows, cols, d)) - inf\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      idx[row, col, row] = 1.0\n",
        "      idx[row, col, rows + col] = 1.0\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx\n",
        "\n",
        "def cartesian_idx(rows, cols, d=2):\n",
        "  idx = np.zeros((rows, cols, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      # idx[row, col, 0] = (1 + row) / rows\n",
        "      # idx[row, col, 1] = (1 + col) / cols\n",
        "      idx[row, col, 0] = 2.0 * ((row) / rows) - 1.0 ### (row + 1)\n",
        "      idx[row, col, 1] = 2.0 * ((col) / cols) - 1.0 ### (col + 1)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _sgbmd(u, v, idxu, idxv, sim=None, f=None, normalize=True) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Slow General Batch Maromba Dot\"\n",
        "  Slower, more general, implementation for the \"batch maromba dot\" operation.\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  sim: index similarity function\n",
        "  f: value function\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  sim = Pairwise(sim)\n",
        "  f = Pairwise(f)\n",
        "  ###\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  ###\n",
        "  # sims: (M * N) x 1 x (d_u * d_v)\n",
        "  # vals: (M * N) x (d_u * d_v) x d_val\n",
        "  sims = sim(idxu, idxv).reshape(m * n, 1, d_u * d_v) ###\n",
        "  norm = 1.0\n",
        "  if normalize:\n",
        "    # norm: (M * N) x 1\n",
        "    norm = sims.sum(dim=-1)\n",
        "  vals = f(u, v)\n",
        "  vals = vals.reshape(m * n, d_u * d_v, d_val)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.bmm(sims, vals).squeeze(1)\n",
        "  eps = 1e-8\n",
        "  dot = (dot / (norm + eps)).reshape(m, n, d_val)\n",
        "  return dot\n",
        "\n",
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  # Pdb().set_trace()\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  # normalizer: M x N x 1\n",
        "  idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv ** 6.0\n",
        "  # idxuv = nn.functional.relu(idxuv - bias) ###\n",
        "  mag = 2.0 # 10.0 # 200.0\n",
        "  idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # idxuv = nn.functional.gumbel_softmax(\n",
        "  #     6.0 * idxuv - 3.0, dim=2, hard=False, tau=0.2\n",
        "  # )\n",
        "  # idxuv = get_eye(m, d_u, d_v, n, idxuv.device)\n",
        "  # idxuv = idxuv / (idxuv.sum(dim=2).unsqueeze(2) + 1e-6)\n",
        "  # Pdb().set_trace()\n",
        "  ###\n",
        "  # normalizer = idxuv.reshape(m, d_u * d_v, n).sum(dim=1).reshape(m, n, 1)\n",
        "  ###\n",
        "  # normalizer = 1.0 - 1e-6\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  # dot = dot / (normalizer + 1e-6)\n",
        "  return dot\n",
        "\n",
        "def _gbmd(u, v, idxu, idxv, kernel=None, idx_part=None) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"General Batch Maromba Dot\"\n",
        "  Shorter implementation for the \"batch maromba dot\" operation.\n",
        "  u: M x d_u\n",
        "  v: N x d_v\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u = u.shape\n",
        "  n, d_v = v.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  assert (m, d_u, d_idx) == idxu.shape\n",
        "  assert (n, d_v, d_idx) == idxv.shape\n",
        "  if kernel:\n",
        "    idxu = kernel(idxu, idx_part)\n",
        "    idxv = kernel(idxv, idx_part)\n",
        "  # uidxu: M x d_idx\n",
        "  # vidxv: N x d_idx\n",
        "  uidxu = torch.bmm(u.reshape(m, 1, d_u), idxu).squeeze(1)\n",
        "  vidxv = torch.bmm(v.reshape(n, 1, d_v), idxv).squeeze(1)\n",
        "  dot = uidxu @ vidxv.T\n",
        "  ### Under experimentation\n",
        "  normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "  dot = dot / (normalizer + 1e-8) ###\n",
        "  ###\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9hF3Uoi3tF"
      },
      "source": [
        "#### Índices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UrPFWDtli55C"
      },
      "outputs": [],
      "source": [
        "def _fast_kernel_idx_sum(idxu, idxv, k, idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, idx_part)\n",
        "  kidxv = k(idxv, idx_part)\n",
        "  d_idx_k = kidxu.shape[-1]\n",
        "  assert kidxu.shape[:-1] == idxu.shape[:-1]\n",
        "  assert kidxv.shape[:-1] == idxv.shape[:-1]\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx_k)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx_k)\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  # sidx: (M * d_idx) x N + (N * d_idx) x M\n",
        "  sidx = (\n",
        "      (iTki @ skj.T).reshape(m, d_idx, n).permute(0, 2, 1)\n",
        "      + (jTkj @ ski.T).reshape(n, d_idx, m).permute(2, 0, 1)\n",
        "  )\n",
        "  sidx = sidx / norm\n",
        "  sidx = sidx.repeat(batch_m, 1, 1)\n",
        "  return sidx\n",
        "\n",
        "def _fast_kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  ### idxu MUST be the input mini-batch\n",
        "  batch_m = 1 # idxu.shape[0]\n",
        "  # idxu = idxu.mean(dim=0).unsqueeze(0)\n",
        "  ###\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # kiTi: (M * d_idx) x d_idx(k)\n",
        "  # kjTj: (N * d_idx) x d_idx(k)\n",
        "  iTki = torch.bmm(idxu.permute(0, 2, 1), kidxu).reshape(m * d_idx, d_idx)\n",
        "  jTkj = torch.bmm(idxv.permute(0, 2, 1), kidxv).reshape(n * d_idx, d_idx)\n",
        "  # iTki_kjTj: M x N x d_idx x d_idx\n",
        "  iTki_kjTj = (iTki @ jTkj.T).reshape(m, d_idx, n, d_idx).permute(0, 2, 1, 3)\n",
        "  diag = torch.diagonal(iTki_kjTj, dim1=2, dim2=3)\n",
        "  ###\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  diag = diag / norm\n",
        "  ###\n",
        "  diag = diag.repeat(batch_m, 1, 1)\n",
        "  return diag\n",
        "\n",
        "def _kernel_idx(idxu, idxv, k, _idx_part):\n",
        "  \"\"\"\n",
        "  k: callable: A x B x C -> A x B x C\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  n, d_v, _ = idxv.shape\n",
        "  assert d_idx == idxv.shape[-1]\n",
        "  # kidxu: M x d_u x d_idx\n",
        "  # kidxv: N x d_v x d_idx\n",
        "  kidxu = k(idxu, _idx_part)\n",
        "  kidxv = k(idxv, _idx_part)\n",
        "  assert kidxu.shape == idxu.shape\n",
        "  assert kidxv.shape == idxv.shape\n",
        "  # ski: (M * N) x d_idx\n",
        "  # skj: (M * N) x d_idx\n",
        "  # norm: M x N x 1\n",
        "  ski = kidxu.sum(dim=1)\n",
        "  skj = kidxv.sum(dim=1)\n",
        "  norm = (ski @ skj.T).unsqueeze(-1)\n",
        "  ski = ski.unsqueeze(1).repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "  skj = skj.unsqueeze(1).repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "  # idxu, kidxu: (M * d_u) x d_idx x 1\n",
        "  # idxv, kidxv: (N * d_v) x d_idx x 1\n",
        "  idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "  idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "  kidxu = kidxu.reshape(m * d_u, d_idx, 1)\n",
        "  kidxv = kidxv.reshape(n * d_v, d_idx, 1)\n",
        "  # sikiT: M x d_idx x d_idx\n",
        "  # sjkjT: N x d_idx x d_idx\n",
        "  sikiT = torch.bmm(idxu, kidxu.permute(0, 2, 1))\n",
        "  sikiT = sikiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "  sjkjT = torch.bmm(idxv, kidxv.permute(0, 2, 1))\n",
        "  sjkjT = sjkjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1)\n",
        "  del kidxu\n",
        "  del kidxv\n",
        "  del idxu\n",
        "  del idxv\n",
        "  # sikiT: (M * N) x d_idx x d_idx\n",
        "  # sjkjT: (M * N) x d_idx x d_idx\n",
        "  sikiT = sikiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  sjkjT = sjkjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "  # diag_sikiT_skjjT: (M * N) x d_idx\n",
        "  # skjjT = sjkjT.permute(0, 2, 1)\n",
        "  # diag_sikiT_skjjT = torch.diagonal(torch.bmm(sikiT, skjjT), dim1=1, dim2=2)\n",
        "  # diag_sikiT_skjjT = diag_sikiT_skjjT.unsqueeze(-1)\n",
        "  xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski)\n",
        "  # xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski) - diag_sikiT_skjjT\n",
        "  # xor_idx = diag_sikiT_skjjT\n",
        "  xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "  xor_idx = xor_idx / norm\n",
        "  return xor_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    kernel = _soft_kernel\n",
        "    # kernel = _cosine_kernel\n",
        "    # mdot = _gbmd(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1]),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1]),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     kernel=kernel,\n",
        "    #     idx_part=self._idx_part,\n",
        "    # )\n",
        "    # mdot = _sgbmd(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     sim=relu_cosine,\n",
        "    #     # sim=squared_cosine,\n",
        "    #     f=vecprod,\n",
        "    # )\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    ###\n",
        "    # mdot = _rdot(\n",
        "    #     self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "    #     b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    # )\n",
        "    ###\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    # _kernel_idx # _fast_kernel_idx # _fast_kernel_idx_sum\n",
        "    # midx = _fast_kernel_idx_sum(\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     kernel,\n",
        "    #     self._idx_part,\n",
        "    # )\n",
        "    # midx = _sgbmd(\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     aidx,\n",
        "    #     bidx,\n",
        "    #     sim=relu_cosine,\n",
        "    #     # sim=squared_cosine,\n",
        "    #     # f=vecsum,\n",
        "    #     f=vecmean,\n",
        "    # )\n",
        "    ###\n",
        "    onesa = torch.ones(self.idx.shape).to(self.idx.device)\n",
        "    onesb = torch.ones(b.idx.shape).to(b.idx.device)\n",
        "    # midx = norm_normalize(\n",
        "    #     norm_normalize(_nsbmd(aidx, onesb, aidx, bidx))\n",
        "    #     + norm_normalize(_nsbmd(onesa, bidx, aidx, bidx))\n",
        "    # )\n",
        "    # midx = norm_normalize(_nsbmd(aidx, bidx, aidx, bidx))\n",
        "    midx = (\n",
        "        _nsbmd(aidx, onesb, aidx, bidx)\n",
        "        + _nsbmd(onesa, bidx, aidx, bidx)\n",
        "    ) / 2.0\n",
        "    # Pdb().set_trace()\n",
        "    ###\n",
        "    # midx = norm_normalize(\n",
        "    #     norm_normalize(_rdot(aidx, onesb, aidx, bidx))\n",
        "    #     + norm_normalize(_rdot(onesa, bidx, aidx, bidx))\n",
        "    # )\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "def idx2d(\n",
        "    channels: int,\n",
        "    rows: int,\n",
        "    cols: int,\n",
        "    w: int,\n",
        "    h: int,\n",
        "    stride: int=2,\n",
        "    dilation: int=1,\n",
        "    device=\"cpu\"\n",
        "  ):\n",
        "  idx = []\n",
        "  dilh = 1 + dilation * (h - 1)\n",
        "  dilw = 1 + dilation * (w - 1)\n",
        "  for row in range(0, rows - (dilh - 1), stride):\n",
        "    for col in range(0, cols - (dilw - 1), stride):\n",
        "      for ch in range(channels):\n",
        "        for drow in range(0, dilh, dilation):\n",
        "          for dcol in range(0, dilw, dilation):\n",
        "            idx.append(\n",
        "                cols * rows * ch\n",
        "                + cols * (row + drow)\n",
        "                + (col + dcol)\n",
        "            )\n",
        "  idx = torch.tensor(idx).long().to(device)\n",
        "  return idx\n",
        "\n",
        "def unsort(idxs):\n",
        "  ridxs = [0 for _ in idxs]\n",
        "  for i, idx in enumerate(idxs):\n",
        "    ridxs[idx] = i\n",
        "  ridxs = torch.tensor(ridxs).long().to(idxs.device)\n",
        "  return ridxs\n",
        "\n",
        "def get_perms(tmp_idx):\n",
        "  idxs, _idxs = [], []\n",
        "  for dim in range(tmp_idx.shape[-1]):\n",
        "    ordering = torch.argsort(tmp_idx[:, dim], stable=True)\n",
        "    idxs.append(ordering.cpu().detach())\n",
        "    _idxs.append(unsort(ordering).cpu().detach())\n",
        "  return idxs, _idxs\n",
        "\n",
        "def resort(k, src, tgt):\n",
        "  assert src == 0 or tgt == 0\n",
        "  global idxs, _idxs\n",
        "  if tgt == 0:\n",
        "    return idxs[src][k]\n",
        "  return _idxs[tgt][k]\n",
        "\n",
        "def hoods(dims, k0, w, _min=0, _max=None):\n",
        "  assert len(dims) == len(w), f\"{len(dims)} != {len(w)}\"\n",
        "  if len(dims) == 0:\n",
        "    return [k0] # [k0.item()]\n",
        "  _hoods = []\n",
        "  global idxs, _idxs\n",
        "  _k0d = resort(k0, 0, dims[-1]) #, idxs, _idxs)\n",
        "  for _w in range(-(w[-1] // 2), (w[-1] // 2) + (w[-1] % 2)):\n",
        "    # k0d = min(_max, max(_min, _k0d + _w))\n",
        "    k0d = torch.clip(_k0d + _w, min=_min, max=_max)\n",
        "    _hoods += hoods(\n",
        "        dims[:-1],\n",
        "        resort(\n",
        "            k0d,\n",
        "            dims[-1], 0,\n",
        "            # idxs, _idxs\n",
        "        ),\n",
        "        w[:-1],\n",
        "        # idxs, _idxs,\n",
        "        _min, _max\n",
        "    )\n",
        "  return _hoods\n",
        "\n",
        "idxs, _idxs = None, None\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def idxhood(xidx, ws, stride):\n",
        "  xidx = xidx.reshape(-1, xidx.shape[-1]).cpu().detach().numpy()\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  all_hoods = neigh.kneighbors(xidx[::stride], return_distance=False).reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods\n",
        "\n",
        "def _idxhood(xidx, ws, stride):\n",
        "  \"\"\"\n",
        "  xidx: in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  dims = tuple(range(xidx.shape[-1]))\n",
        "  global idxs, _idxs\n",
        "  idxs, _idxs = get_perms(xidx)\n",
        "  pivots = torch.tensor([piv for piv in range(0, len(xidx), stride)]).long()\n",
        "  all_hoods = hoods(dims, pivots, ws, 0, len(xidx) - 1)\n",
        "  # all_hoods = torch.tensor(all_hoods).long().T.reshape(-1)\n",
        "  all_hoods = torch.cat(all_hoods, dim=0).reshape(len(all_hoods), -1).T\n",
        "  all_hoods = all_hoods.reshape(-1)\n",
        "  # Pdb().set_trace()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZ8zHIcPQPS"
      },
      "source": [
        "#### MModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tQoFxrDIPScK"
      },
      "outputs": [],
      "source": [
        "class MModule(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    # self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    self.W = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params), device=device) - 1.0\n",
        "    )\n",
        "    self.W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params, idx_dim), device=device) - 1.0\n",
        "    )\n",
        "    # self.W_idx = _W_idx\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    ###\n",
        "    if probe_dim:\n",
        "      self.probe = nn.Linear(probe_dim, 10).to(device) # 288, 400, 512\n",
        "    ###\n",
        "    # self.activation = nn.ReLU()\n",
        "    self.activation = nn.ELU()\n",
        "\n",
        "  def _W_step(\n",
        "      self,\n",
        "      x: MTensor,\n",
        "      W: MTensor,\n",
        "      sets,\n",
        "      samples,\n",
        "      random=True,\n",
        "      conv=False,\n",
        "      filter_size=4,\n",
        "      activation=True,\n",
        "      regular_dot=False):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # Put 1 into x\n",
        "    if not conv:\n",
        "      filter_size = in_dim\n",
        "    assert (in_dim % filter_size) == 0\n",
        "    num_windows = (in_dim // filter_size)\n",
        "    # one = MTensor(\n",
        "    #     torch.ones((n * num_windows), 1).to(self.device),\n",
        "    #     torch.ones((n * num_windows), 1, idx_dim).to(self.device),\n",
        "    # )\n",
        "    x = MTensor.reshape(x, (n * num_windows, filter_size))\n",
        "    # Sample W\n",
        "    if conv:\n",
        "      ### filter_size + 1\n",
        "      assert (sets * samples) % (filter_size) == 0\n",
        "      numw_windows = (sets * samples) // (filter_size)\n",
        "      sets, samples = numw_windows, (filter_size)\n",
        "    W_sets = MTensor.reshape(W, (sets, samples))\n",
        "    ## mdot: N x sets\n",
        "    # mdot: (N * num_windows) x numw_windows\n",
        "    mdot = x @ W_sets\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    # mdot: N x num_windows x numw_windows\n",
        "    if conv:\n",
        "      ### Várias \"imagens\" coladas em um sentido\n",
        "      mdot = MTensor.reshape(mdot, (n, num_windows, numw_windows))\n",
        "    return mdot\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    channels = 1\n",
        "    img_h, img_w = img_dim, img_dim\n",
        "    filter_whs = [(3, 3), (3, 3)]\n",
        "    strides = [2, 1]\n",
        "    filter_w, filter_h = filter_whs[0]\n",
        "    stride = strides[0]\n",
        "    filter_area = filter_w * filter_h\n",
        "    filter_volume = channels * filter_area\n",
        "    self.all_pools = [x[:4]]\n",
        "    idx = idx2d(\n",
        "        channels,\n",
        "        img_h, img_w,\n",
        "        filter_w, filter_h,\n",
        "        stride=stride,\n",
        "        device=self.device\n",
        "    )\n",
        "    x = x[:, idx]\n",
        "    ###\n",
        "    pool = x\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      if conv:\n",
        "        pool = self._W_step(\n",
        "            pool,\n",
        "            self.MW[:, wl: wr],\n",
        "            self.sets[step],\n",
        "            self.samples[step],\n",
        "            random=False,\n",
        "            conv=conv,\n",
        "            filter_size=filter_volume,\n",
        "            activation=activate,\n",
        "        )\n",
        "      else:\n",
        "        pool.data = self.probe(pool.data)\n",
        "      ###\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      ###\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        filter_volume = channels * filter_area\n",
        "        pool = MTensor.permute(pool, (0, 2, 1))\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        # assert img_dim % stride == 0\n",
        "        img_h = (img_h - filter_h + stride) // stride\n",
        "        img_w = (img_w - filter_w + stride) // stride\n",
        "        assert img_h * img_w == img_area\n",
        "        # cols = pool.data.shape[1] // rows\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        filter_w, filter_h = filter_whs[nxt_conv_step]\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_area = filter_w * filter_h\n",
        "        filter_volume = channels * filter_area\n",
        "        if nxt_conv:\n",
        "          idx = idx2d(\n",
        "              channels,\n",
        "              img_h, img_w,\n",
        "              filter_w, filter_h,\n",
        "              stride=stride,\n",
        "              device=self.device\n",
        "          )\n",
        "          pool = pool[:, idx]\n",
        "      ###\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mlldpkcPFvk"
      },
      "source": [
        "#### MModule II"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Oipx_P9qYUUb"
      },
      "outputs": [],
      "source": [
        "class MModule2(nn.Module):\n",
        "  def __init__(\n",
        "      self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\",\n",
        "      probe_dim=None,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    # self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    self.W = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params), device=device) - 1.0\n",
        "    )\n",
        "    self.W_idx = nn.Parameter(\n",
        "        2.0 * torch.rand((1, n_params, idx_dim), device=device) - 1.0\n",
        "    )\n",
        "    # self.W_idx = _W_idx\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    ###\n",
        "    if probe_dim:\n",
        "      self.probe = nn.Linear(probe_dim, 10).to(device) # 288, 400, 512\n",
        "    ###\n",
        "    # self.activation = nn.ReLU()\n",
        "    self.activation = nn.ELU()\n",
        "    self._prev_idx = None\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    ### Under experimentation\n",
        "    n = x.data.shape[0]\n",
        "    filter_whs = [(3, 3, 1), (3, 3, 2)]\n",
        "    strides = [2, 2]\n",
        "    stride = strides[0]\n",
        "    filter_volume = np.prod(filter_whs[0])\n",
        "    self.all_pools = [x[:4]]\n",
        "    idxx = idxhood(x.idx[0], filter_whs[0], strides[0]) ### FIX\n",
        "    # pool: N x (num_windows * window_volume)\n",
        "    pool = x[:, idxx]\n",
        "    ###\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    # idxw = idxhood(\n",
        "    #     mw.idx,\n",
        "    #     filter_whs[0],\n",
        "    #     strides[0]\n",
        "    # ) ### FIX\n",
        "    # mw = mw[:, idxw]\n",
        "    for step in range(n_sets):\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[0, wl: wr],\n",
        "          (self.sets[step], self.samples[step])\n",
        "      )\n",
        "      activate = (step < n_sets - 1)\n",
        "      conv = activate\n",
        "      if conv:\n",
        "        # pool: (N * num_windows) x  sets\n",
        "        # Pdb().set_trace()\n",
        "        pool = MTensor.reshape(pool, (-1, filter_volume)) @ mw\n",
        "        pool = MTensor.reshape(pool, (n, -1, self.sets[step]))\n",
        "      else:\n",
        "        pool.data = self.probe(pool.data)\n",
        "      ###\n",
        "      nxt_conv = (step + 1 < n_sets - 1)\n",
        "      ###\n",
        "      if conv:\n",
        "        # pool: N x num_windows x numw_windows\n",
        "        self.all_pools.append(pool[:4])\n",
        "        n, img_area, channels = pool.data.shape\n",
        "        # pool = MTensor.permute(pool, (0, 2, 1))\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        nxt_conv_step = (step + 1) % len(strides)\n",
        "        stride = strides[nxt_conv_step]\n",
        "        filter_volume = np.prod(filter_whs[nxt_conv_step])\n",
        "        if nxt_conv:\n",
        "          idxx = idxhood(\n",
        "              pool.idx[0],\n",
        "              filter_whs[nxt_conv_step],\n",
        "              strides[nxt_conv_step]\n",
        "          ) ### FIX\n",
        "          pool = pool[:, idxx]\n",
        "      ###\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "      # param_stride = (wl + wr) // 2 # wr\n",
        "      # next_wr = param_stride + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      # wl, wr = param_stride, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  y_pred = 10.0 * y_pred.data\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 3 # rows + cols + hidden_dim + clf_dim # 3\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, d=idx_dim)\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OisDCAuLCmQ8"
      },
      "outputs": [],
      "source": [
        "# tmp_idx = template_x_idx[0].reshape(-1, 3)[:, :2]\n",
        "# idxs, _idxs = get_perms(tmp_idx)\n",
        "# sampled = hoods([0, 1], 14 * 28 + 14, [3, 3], idxs, _idxs, 0, 783)\n",
        "# sampled = np.array([[idx // 28, idx % 28] for idx in sampled])\n",
        "# # print(sampled)\n",
        "# plt.scatter(sampled[:, 0], sampled[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OoGmgn1K--XO"
      },
      "outputs": [],
      "source": [
        "# tmp_idx = template_x_idx[0].reshape(-1, 2).cpu().detach().numpy()\n",
        "# # set desired number of neighbors\n",
        "# neigh = NearestNeighbors(n_neighbors=9)\n",
        "# neigh.fit(tmp_idx)\n",
        "# # select indices of k nearest neighbors of the vectors in the input list\n",
        "# neighbors = neigh.kneighbors(tmp_idx, return_distance=False)\n",
        "# print(neighbors[0])\n",
        "# print(neighbors[1])\n",
        "# print(neighbors[28])\n",
        "# print(neighbors[29])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyzd22RQX-Yg"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNheVxvNNK30"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 784\n",
        "start_mode = False\n",
        "valid_mode = False\n",
        "# TODO: Visualize conv layer output\n",
        "samples = [\n",
        "    # in_ch * h * w,\n",
        "    1 * 3 * 3,\n",
        "    2 * 3 * 3,\n",
        "    # 2 * 3 * 3,\n",
        "    # 4 * 8 * 3 * 3,\n",
        "    hidden_dim,\n",
        "]\n",
        "sets = [\n",
        "    # out_ch\n",
        "    2,\n",
        "    2,\n",
        "    # 2,\n",
        "    # 1,\n",
        "    num_classes\n",
        "]\n",
        "conv_params = int(np.array(samples[:-1]).dot(np.array(sets[:-1])))\n",
        "n_params = int(np.array(samples).dot(np.array(sets)))\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "if start_mode:\n",
        "  # model = MModule(\n",
        "  model = MModule2(\n",
        "      n_params=n_params,\n",
        "      idx_dim=idx_dim,\n",
        "      samples=samples,\n",
        "      sets=sets,\n",
        "      device=device,\n",
        "      probe_dim=hidden_dim,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3) # 1e-2\n",
        "  widx0 = model.W_idx[:, :conv_params].clone().detach()\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "\n",
        "num_epochs = 720 * 4\n",
        "epoch_len = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  ###\n",
        "  widx_diff = (model.W_idx[:, :conv_params] - widx0)\n",
        "  print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  else:\n",
        "    group_cols = [\"epoch\", \"train loss\"]\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  (\n",
        "    df_train[group_cols]\n",
        "    .groupby(\"epoch\")\n",
        "    .agg(lambda x: x.median(skipna=True))\n",
        "    # .tail(200)\n",
        "    .plot(figsize=(16, 3), grid=True)\n",
        "  )\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tidx = model.W_idx[0, :18].cpu().detach().numpy()\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();"
      ],
      "metadata": {
        "id": "8CcZxz9MYMwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG5gdtyeWnjJ"
      },
      "outputs": [],
      "source": [
        "# tidx = aidx.reshape(32, -1, 9, 3)[0, 100].cpu().detach().numpy(); tidx = tidx.reshape(-1, 3)\n",
        "# tidx = aidx.reshape(32, -1, 9, 2)[0].cpu().detach().numpy(); tidx = tidx.reshape(-1, 2)\n",
        "\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 1] * 0.0})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "# midx.reshape(32, -1, 3)[0, 100:105].cpu().detach().numpy()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tJHxWRO_xoX"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t0uL2kM1Okfd"
      },
      "outputs": [],
      "source": [
        "imgs = [img.data.cpu().detach() for img in model.all_pools]\n",
        "idxs = [img.idx.cpu().detach() for img in model.all_pools]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj5tP_tfMAjw",
        "outputId": "aabde05b-0082-49f5-eb56-7e150c8e0d70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([4, 784]), torch.Size([4, 784, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "[img.shape for img in imgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "ZRWVQRznvaer",
        "outputId": "25d38b32-07a9-4e28-c610-1430e0737f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784, 2])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "shapes = [\n",
        "    (img_dim, img_dim, 1),\n",
        "    ((img_dim - 0) // 1, (img_dim - 0) // 1, 2),\n",
        "    # ((img_dim - 0) // 1, (img_dim - 0) // 1, 4),\n",
        "    # ((img_dim - 0) // 1, (img_dim - 0) // 1, 8),\n",
        "    # (((img_dim - 2) // 2 - 2) // 1, ((img_dim - 2) // 2 - 2) // 1, 4),\n",
        "    # (((img_dim - 4) // 1 - 4) // 2, ((img_dim - 4) // 1 - 4) // 2, 8),\n",
        "    # ((img_dim - 4) // 2 - 2, (img_dim - 4) // 2 - 2, 4),\n",
        "    # ((img_dim - 4) // 2 - 4, (img_dim - 4) // 2 - 4, 8),\n",
        "]\n",
        "# display.clear_output(wait=True)\n",
        "plt.close()\n",
        "for idx, img in enumerate(imgs[:len(shapes)]):\n",
        "  img = img[0]\n",
        "  print(img.shape)\n",
        "  img = img.reshape(*shapes[idx])\n",
        "  rows, cols = 1, shapes[idx][2]\n",
        "  fig, ax = plt.subplots(rows, cols, figsize=(min(18, 3 * cols), 3))\n",
        "  for ch in range(cols):\n",
        "    img_ = img[:, :, ch].numpy()\n",
        "    if cols > 1:\n",
        "      ax[ch].imshow(img_)\n",
        "    else:\n",
        "      ax.imshow(img_)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LgY4NUoRagWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb23e999-e848-4ebc-8f75-0efadc903f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"7c740489-2d30-4e9e-a409-2623c1bc0c41\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7c740489-2d30-4e9e-a409-2623c1bc0c41\")) {                    Plotly.newPlot(                        \"7c740489-2d30-4e9e-a409-2623c1bc0c41\",                        [{\"x\":[-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.9285714030265808,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.8571428656578064,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7857142686843872,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.7142857313156128,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.6428571343421936,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5714285969734192,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.4285714328289032,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.3571428656578064,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2857142984867096,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.2142857164144516,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.1428571492433548,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,-0.0714285746216774,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.0714285746216774,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.1428571492433548,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2142857164144516,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.2857142984867096,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.3571428656578064,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.4285714328289032,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.5714285969734192,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.6428571343421936,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7142857313156128,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.7857142686843872,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.8571428656578064,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808,0.9285714030265808],\"y\":[-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808,-1.0,-0.9285714030265808,-0.8571428656578064,-0.7857142686843872,-0.7142857313156128,-0.6428571343421936,-0.5714285969734192,-0.5,-0.4285714328289032,-0.3571428656578064,-0.2857142984867096,-0.2142857164144516,-0.1428571492433548,-0.0714285746216774,0.0,0.0714285746216774,0.1428571492433548,0.2142857164144516,0.2857142984867096,0.3571428656578064,0.4285714328289032,0.5,0.5714285969734192,0.6428571343421936,0.7142857313156128,0.7857142686843872,0.8571428656578064,0.9285714030265808],\"z\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"type\":\"scatter3d\",\"scene\":\"scene\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]}},\"scene2\":{\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]}},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7c740489-2d30-4e9e-a409-2623c1bc0c41');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784, 2, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"9950453b-734a-4332-934a-0aa905b083f2\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9950453b-734a-4332-934a-0aa905b083f2\")) {                    Plotly.newPlot(                        \"9950453b-734a-4332-934a-0aa905b083f2\",                        [{\"x\":[-5.421514511108398,-5.909268379211426,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.517286777496338,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.51728630065918,-6.133905410766602,-5.421514511108398,-5.615042686462402,-5.421514511108398,-5.421514511108398,-5.421514987945557,-5.421514511108398,-5.421514987945557,-5.421514511108398,-5.421514987945557,-5.421514511108398,-5.421514511108398,-5.421514987945557,-5.421514987945557,-5.421514511108398,-5.421514511108398,-5.421514511108398,-5.421514511108398,-5.421514987945557,-5.421514987945557,-5.421514987945557,-5.421514511108398,-5.421514511108398,-5.421514987945557,-5.421514511108398,-5.421514511108398,-5.421514511108398,-5.421514511108398,-5.421514987945557,-5.742749214172363,-5.150052070617676,-5.1000871658325195,-5.100086212158203,-5.1000871658325195,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.100086688995361,-5.1000871658325195,-5.100086688995361,-5.1000871658325195,-5.100086212158203,-5.1000871658325195,-5.468772888183594,-4.8286237716674805,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.778657913208008,-4.778657913208008,-4.77865743637085,-4.77865743637085,-4.778657913208008,-4.778657913208008,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.778657913208008,-4.778657913208008,-4.778657913208008,-4.77865743637085,-4.77865743637085,-4.778657913208008,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.77865743637085,-4.77865743637085,-5.147345542907715,-4.507194519042969,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.457230091094971,-4.457229137420654,-4.457229137420654,-4.4572296142578125,-4.457230091094971,-4.457229137420654,-4.4572296142578125,-4.457229137420654,-4.4572296142578125,-4.457230091094971,-4.4572296142578125,-4.457230091094971,-4.457229137420654,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.4572296142578125,-4.825915813446045,-4.185766696929932,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.135800361633301,-4.504487991333008,-3.864337921142578,-3.8143727779388428,-3.8143725395202637,-3.8143727779388428,-3.8143725395202637,-3.8143727779388428,-3.8143723011016846,-3.814373016357422,-3.8143727779388428,-3.8143725395202637,-3.8143725395202637,-3.814373016357422,-3.8143725395202637,-3.8143725395202637,-3.8143727779388428,-3.8143727779388428,-3.814373016357422,-3.8143725395202637,-3.814373016357422,-3.8143725395202637,-3.8143725395202637,-3.8143725395202637,-3.8143727779388428,-3.8143725395202637,-3.8143727779388428,-3.8143725395202637,-3.8143727779388428,-4.183059215545654,-3.542909622192383,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943286895752,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943286895752,-3.492943286895752,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943286895752,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.492943286895752,-3.492943525314331,-3.86163067817688,-3.2214808464050293,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715152263641357,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715152263641357,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.1715149879455566,-3.5402021408081055,-2.900052070617676,-2.8500866889953613,-2.850086212158203,-2.8500866889953613,-2.850086212158203,-2.8500866889953613,-2.850086212158203,-2.8500866889953613,-2.850086212158203,-2.850086212158203,-2.8500864505767822,-2.8500866889953613,-2.850086212158203,-2.850086212158203,-2.850086212158203,-2.850086212158203,-2.8500864505767822,-2.8500864505767822,-2.8500866889953613,-2.8500866889953613,-2.8500864505767822,-2.8500864505767822,-2.8500866889953613,-2.850086212158203,-2.8500866889953613,-2.850086212158203,-2.8500866889953613,-3.218773603439331,-2.5786232948303223,-2.528658151626587,-2.528657913208008,-2.528658151626587,-2.528657913208008,-2.528658151626587,-2.528657913208008,-2.528658390045166,-2.528657913208008,-2.528658151626587,-2.528657913208008,-2.528658151626587,-2.528658151626587,-2.528657913208008,-2.528657913208008,-2.528658151626587,-2.528658390045166,-2.528658151626587,-2.528658390045166,-2.528658151626587,-2.528657913208008,-2.528657913208008,-2.528658151626587,-2.528657913208008,-2.528658151626587,-2.528657913208008,-2.528658151626587,-2.8973450660705566,-2.257194995880127,-2.2072293758392334,-2.2072291374206543,-2.2072293758392334,-2.2072291374206543,-2.2072293758392334,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072291374206543,-2.2072293758392334,-2.2072291374206543,-2.2072293758392334,-2.2072291374206543,-2.2072293758392334,-2.575916290283203,-1.935766339302063,-1.885800838470459,-1.8858006000518799,-1.885800838470459,-1.8858006000518799,-1.885800838470459,-1.8858006000518799,-1.8858009576797485,-1.8858009576797485,-1.8858006000518799,-1.8858007192611694,-1.8858009576797485,-1.8858006000518799,-1.8858006000518799,-1.8858006000518799,-1.8858007192611694,-1.8858009576797485,-1.8858007192611694,-1.8858009576797485,-1.8858007192611694,-1.8858007192611694,-1.8858007192611694,-1.885800838470459,-1.8858006000518799,-1.885800838470459,-1.8858006000518799,-1.885800838470459,-2.254487991333008,-1.6143379211425781,-1.5643723011016846,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.564371943473816,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643720626831055,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643723011016846,-1.5643720626831055,-1.5643723011016846,-1.9330594539642334,-1.2929091453552246,-1.2429436445236206,-1.2429434061050415,-1.2429436445236206,-1.2429434061050415,-1.2429436445236206,-1.2429434061050415,-1.242943525314331,-1.2429434061050415,-1.242943525314331,-1.242943525314331,-1.2429436445236206,-1.242943525314331,-1.242943525314331,-1.2429436445236206,-1.2429434061050415,-1.2429436445236206,-1.242943525314331,-1.2429436445236206,-1.242943525314331,-1.2429434061050415,-1.242943286895752,-1.2429436445236206,-1.2429434061050415,-1.2429436445236206,-1.2429434061050415,-1.2429436445236206,-1.6116306781768799,-0.9714805483818054,-0.9215150475502014,-0.9215148091316223,-0.9215150475502014,-0.9215148091316223,-0.9215150475502014,-0.9215148091316223,-0.9215150475502014,-0.9215148687362671,-0.9215149283409119,-0.9215149879455566,-0.921515166759491,-0.9215148091316223,-0.9215148091316223,-0.9215149283409119,-0.9215148091316223,-0.921515166759491,-0.9215149879455566,-0.921515166759491,-0.9215148091316223,-0.9215149879455566,-0.9215147495269775,-0.9215150475502014,-0.9215148091316223,-0.9215150475502014,-0.9215148091316223,-0.9215150475502014,-1.2902021408081055,-0.6500520706176758,-0.6000864505767822,-0.6000862717628479,-0.6000864505767822,-0.6000862717628479,-0.6000864505767822,-0.6000862717628479,-0.6000862717628479,-0.6000862717628479,-0.6000862717628479,-0.6000863909721375,-0.6000863909721375,-0.6000862717628479,-0.6000861525535583,-0.6000862717628479,-0.6000863909721375,-0.6000863909721375,-0.6000863909721375,-0.6000863909721375,-0.6000862717628479,-0.6000862717628479,-0.6000861525535583,-0.6000864505767822,-0.6000862717628479,-0.6000864505767822,-0.6000862717628479,-0.6000864505767822,-0.9687734842300415,-0.3286234140396118,-0.27865803241729736,-0.27865779399871826,-0.27865803241729736,-0.27865779399871826,-0.27865803241729736,-0.27865779399871826,-0.2786581516265869,-0.27865803241729736,-0.27865803241729736,-0.27865803241729736,-0.2786581516265869,-0.27865803241729736,-0.2786579132080078,-0.27865803241729736,-0.27865803241729736,-0.2786581516265869,-0.27865779399871826,-0.2786581516265869,-0.27865803241729736,-0.27865803241729736,-0.27865779399871826,-0.27865803241729736,-0.27865779399871826,-0.27865803241729736,-0.27865779399871826,-0.27865803241729736,-0.6473449468612671,-0.007194876670837402,0.04277074337005615,0.0427708625793457,0.04277074337005615,0.0427708625793457,0.04277074337005615,0.0427708625793457,0.042770981788635254,0.042770981788635254,0.042771100997924805,0.04277074337005615,0.0427706241607666,0.042770981788635254,0.042770981788635254,0.042770981788635254,0.042770981788635254,0.04277074337005615,0.04277074337005615,0.0427706241607666,0.042770981788635254,0.042771100997924805,0.042770981788635254,0.04277074337005615,0.0427708625793457,0.04277074337005615,0.0427708625793457,0.04277074337005615,-0.3259161710739136,0.314233660697937,0.36419928073883057,0.36419951915740967,0.36419928073883057,0.36419951915740967,0.36419928073883057,0.36419951915740967,0.3641993999481201,0.36419951915740967,0.36419951915740967,0.3641993999481201,0.36419928073883057,0.36419951915740967,0.3641993999481201,0.36419951915740967,0.36419951915740967,0.36419928073883057,0.3641993999481201,0.36419928073883057,0.3641993999481201,0.3641993999481201,0.3641996383666992,0.36419928073883057,0.36419951915740967,0.36419928073883057,0.36419951915740967,0.36419928073883057,-0.004487514495849609,0.6356620788574219,0.685627818107605,0.6856280565261841,0.685627818107605,0.6856279373168945,0.685627818107605,0.6856280565261841,0.6856274604797363,0.6856274604797363,0.6856279373168945,0.685627818107605,0.6856276988983154,0.6856279373168945,0.685627818107605,0.6856279373168945,0.6856279373168945,0.6856276988983154,0.685627818107605,0.6856276988983154,0.6856276988983154,0.6856276988983154,0.6856276988983154,0.685627818107605,0.6856279373168945,0.6856276988983154,0.6856280565261841,0.685627818107605,0.31694066524505615,0.9570910930633545,1.0070561170578003,1.007056474685669,1.0070561170578003,1.007056474685669,1.0070561170578003,1.007056474685669,1.0070559978485107,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070559978485107,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070561170578003,1.0070562362670898,1.007056474685669,1.0070561170578003,1.007056474685669,1.0070561170578003,1.007056474685669,1.0070561170578003,0.6383697986602783,1.2785195112228394,1.328485131263733,1.3284852504730225,1.3284850120544434,1.3284856081008911,1.3284850120544434,1.3284856081008911,1.3284850120544434,1.328485131263733,1.328485131263733,1.3284854888916016,1.3284850120544434,1.328485369682312,1.328485131263733,1.328485131263733,1.328485131263733,1.3284850120544434,1.3284854888916016,1.3284850120544434,1.3284852504730225,1.328485369682312,1.3284854888916016,1.3284850120544434,1.328485131263733,1.3284850120544434,1.3284856081008911,1.328485131263733,0.9597980976104736,1.5999480485916138,1.6499130725860596,1.6499131917953491,1.6499130725860596,1.6499133110046387,1.6499130725860596,1.6499133110046387,1.64991295337677,1.6499130725860596,1.6499131917953491,1.6499131917953491,1.6499131917953491,1.6499131917953491,1.6499130725860596,1.6499130725860596,1.6499131917953491,1.6499131917953491,1.6499131917953491,1.6499131917953491,1.6499131917953491,1.6499130725860596,1.6499130725860596,1.6499130725860596,1.6499131917953491,1.6499130725860596,1.6499131917953491,1.6499130725860596,1.2812265157699585,1.92137610912323,1.9713423252105713,1.9713420867919922,1.9713422060012817,1.9713420867919922,1.9713423252105713,1.9713422060012817,1.9713423252105713,1.9713423252105713,1.9713424444198608,1.9713423252105713,1.9713423252105713,1.9713424444198608,1.9713423252105713,1.9713424444198608,1.9713424444198608,1.9713423252105713,1.9713423252105713,1.9713423252105713,1.9713423252105713,1.9713424444198608,1.9713420867919922,1.9713423252105713,1.9713420867919922,1.9713424444198608,1.9713422060012817,1.9713423252105713,1.6026549339294434,2.242805004119873,2.2927701473236084,2.2927703857421875,2.2927701473236084,2.2927703857421875,2.2927701473236084,2.2927703857421875,2.2927703857421875,2.2927701473236084,2.2927701473236084,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927703857421875,2.2927701473236084,2.2927701473236084,2.2927701473236084,2.2927703857421875,2.2927703857421875,2.2927701473236084,2.2927703857421875,2.2927703857421875,1.9240837097167969,3.014638900756836,2.61419939994812,2.614199161529541,2.61419939994812,2.614199161529541,2.61419939994812,2.614199161529541,2.614199638366699,2.61419939994812,2.614199638366699,2.614199638366699,2.61419939994812,2.614199638366699,2.61419939994812,2.61419939994812,2.614199638366699,2.61419939994812,2.614199638366699,2.61419939994812,2.61419939994812,2.61419939994812,2.61419939994812,2.61419939994812,2.614199161529541,2.61419939994812,2.614199161529541,2.61419939994812,2.3703866004943848,2.61419939994812,3.095067024230957,3.5063345432281494,3.5063347816467285,3.5063345432281494,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063345432281494,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063345432281494,3.5063347816467285,3.5063347816467285,3.5063347816467285,3.5063345432281494,3.5063347816467285,3.5063345432281494,3.0235958099365234,2.614199638366699],\"y\":[-4.487550735473633,-4.402522563934326,-3.9588019847869873,-3.637373685836792,-3.3159451484680176,-2.994516611099243,-2.6730878353118896,-2.3516595363616943,-2.030230760574341,-1.7088022232055664,-1.3873738050460815,-1.0659451484680176,-0.7445166110992432,-0.423088014125824,-0.1016593798995018,0.2197691947221756,0.5411977767944336,0.862626314163208,1.184054970741272,1.505483627319336,1.8269119262695312,2.1483404636383057,2.469769239425659,2.7911980152130127,3.112626314163208,3.4340548515319824,3.8965044021606445,3.548163414001465,-4.673306465148926,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.30897966027259827,0.012448877096176147,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981631755828857,1.619591474533081,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.548163414001465,3.932795524597168,-4.990222454071045,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232653617858887,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304081678390503,-0.3089795708656311,0.01244896650314331,0.33387771248817444,0.6553061604499817,0.9767348766326904,1.2981634140014648,1.6195919513702393,1.9410203695297241,2.262449026107788,2.5838773250579834,2.905306577682495,3.2267343997955322,3.5481631755828857,4.239802360534668,-4.990222930908203,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232653617858887,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.30897969007492065,0.012448877096176147,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981631755828857,1.6195915937423706,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232653617858887,-3.201836347579956,-2.880408525466919,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304081082344055,-0.3089795708656311,0.01244896650314331,0.33387771248817444,0.6553061604499817,0.9767348766326904,1.2981634140014648,1.6195917129516602,1.9410203695297241,2.262449264526367,2.5838773250579834,2.905306339263916,3.2267343997955322,3.5481631755828857,4.239802360534668,-4.990222930908203,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161226749420166,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.3089796304702759,0.012448877096176147,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981632947921753,1.619591474533081,1.9410202503204346,2.262448787689209,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.880408525466919,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304081678390503,-0.30897951126098633,0.012449026107788086,0.33387771248817444,0.6553061604499817,0.9767348766326904,1.2981634140014648,1.6195918321609497,1.9410203695297241,2.262449264526367,2.5838775634765625,2.905306100845337,3.2267346382141113,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.8804080486297607,-2.5589795112609863,-2.237550973892212,-1.916122317314148,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082274436951,-0.30897966027259827,0.012448817491531372,0.3338775038719177,0.6553060412406921,0.9767345190048218,1.2981631755828857,1.6195917129516602,1.9410202503204346,2.262448787689209,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.8804080486297607,-2.5589795112609863,-2.237550973892212,-1.9161226749420166,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082274436951,-0.30897966027259827,0.012448936700820923,0.3338775336742401,0.6553060412406921,0.9767348170280457,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449264526367,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.8804080486297607,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304081678390503,-0.3089796304702759,0.012448906898498535,0.3338775038719177,0.6553060412406921,0.9767346382141113,1.2981634140014648,1.6195918321609497,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.1661224365234375,-3.844693899154663,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.5946940183639526,-1.2732653617858887,-0.9518367648124695,-0.6304082274436951,-0.3089796304702759,0.012448996305465698,0.33387765288352966,0.6553061604499817,0.9767347574234009,1.2981632947921753,1.6195915937423706,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082274436951,-0.30897969007492065,0.01244884729385376,0.3338775634765625,0.6553061008453369,0.9767345190048218,1.2981631755828857,1.6195918321609497,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.5946941375732422,-1.2732653617858887,-0.9518367648124695,-0.6304081678390503,-0.3089795708656311,0.012449026107788086,0.3338775038719177,0.6553060412406921,0.9767348170280457,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304081678390503,-0.3089795708656311,0.012448996305465698,0.3338775634765625,0.6553060412406921,0.9767346382141113,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.5946941375732422,-1.2732653617858887,-0.9518367648124695,-0.6304082274436951,-0.3089796304702759,0.012448936700820923,0.33387747406959534,0.6553060412406921,0.9767346382141113,1.2981634140014648,1.6195917129516602,1.9410202503204346,2.262449264526367,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.8804080486297607,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518367648124695,-0.6304082870483398,-0.30897966027259827,0.012448906898498535,0.3338775634765625,0.6553060412406921,0.9767348170280457,1.2981631755828857,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.3089796006679535,0.012448906898498535,0.3338775038719177,0.6553060412406921,0.9767347574234009,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.1661224365234375,-3.844693899154663,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304082274436951,-0.3089796304702759,0.012448996305465698,0.33387765288352966,0.6553061008453369,0.9767347574234009,1.2981634140014648,1.6195919513702393,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161226749420166,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082274436951,-0.30897966027259827,0.012448877096176147,0.3338775634765625,0.6553060412406921,0.9767345190048218,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304081678390503,-0.3089796006679535,0.01244896650314331,0.3338775038719177,0.6553060412406921,0.9767346382141113,1.2981634140014648,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304081678390503,-0.30897966027259827,0.01244896650314331,0.33387744426727295,0.6553060412406921,0.9767346978187561,1.2981632947921753,1.6195917129516602,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267346382141113,3.5481629371643066,4.239802360534668,-4.990222454071045,-4.487551212310791,-4.1661224365234375,-3.844693660736084,-3.5232653617858887,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550735473633,-1.9161224365234375,-1.5946940183639526,-1.2732653617858887,-0.9518367052078247,-0.6304081082344055,-0.30897951126098633,0.012448996305465698,0.3338776230812073,0.6553061008453369,0.9767347574234009,1.2981631755828857,1.6195918321609497,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306339263916,3.2267348766326904,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.8804080486297607,-2.5589795112609863,-2.237550735473633,-1.916122555732727,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.30897969007492065,0.012448936700820923,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981631755828857,1.6195915937423706,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.548163414001465,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.880408525466919,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304081082344055,-0.30897951126098633,0.01244896650314331,0.33387771248817444,0.6553061604499817,0.9767348766326904,1.2981634140014648,1.6195919513702393,1.9410202503204346,2.262449264526367,2.5838775634765625,2.905306577682495,3.2267343997955322,3.5481631755828857,4.239802360534668,-4.990222930908203,-4.487551212310791,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161226749420166,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.30897969007492065,0.012448906898498535,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981631755828857,1.6195915937423706,1.9410202503204346,2.262448787689209,2.5838773250579834,2.905306100845337,3.2267348766326904,3.5481631755828857,4.239802360534668,-4.990222454071045,-4.487550735473633,-4.1661224365234375,-3.844693660736084,-3.5232653617858887,-3.201836585998535,-2.880408525466919,-2.5589795112609863,-2.237550973892212,-1.9161224365234375,-1.594693899154663,-1.2732652425765991,-0.9518367648124695,-0.6304081678390503,-0.3089795708656311,0.01244896650314331,0.33387771248817444,0.6553061604499817,0.9767348766326904,1.2981634140014648,1.6195919513702393,1.9410202503204346,2.262449264526367,2.5838775634765625,2.905306100845337,3.2267343997955322,3.548163414001465,4.239802360534668,-4.8055572509765625,-4.487550735473633,-4.166122913360596,-3.844693660736084,-3.5232656002044678,-3.201836585998535,-2.88040828704834,-2.5589795112609863,-2.237550973892212,-1.9161226749420166,-1.5946941375732422,-1.2732653617858887,-0.9518368244171143,-0.6304082870483398,-0.30897969007492065,0.012448877096176147,0.3338775336742401,0.6553059816360474,0.9767347574234009,1.2981631755828857,1.619591474533081,1.9410202503204346,2.262449026107788,2.5838773250579834,2.905306100845337,3.2267348766326904,3.548163414001465,4.045125961303711,-4.487551212310791,-4.482553005218506,-4.172479629516602,-3.8510518074035645,-3.529622793197632,-3.2081942558288574,-2.886765480041504,-2.5653371810913086,-2.243908405303955,-1.9224799871444702,-1.6010513305664062,-1.2796227931976318,-0.9581941962242126,-0.636765718460083,-0.31533706188201904,0.0060915350914001465,0.32752010226249695,0.6489486694335938,0.9703773856163025,1.2918059825897217,1.613234281539917,1.9346630573272705,2.256091833114624,2.5775201320648193,2.8989486694335938,3.220377206802368,3.7701048851013184,3.548163414001465],\"z\":[-1.184543490409851,-1.4320076704025269,-1.878760576248169,-1.8787603378295898,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.8787603378295898,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.8787603378295898,-1.8787603378295898,-1.878760576248169,-1.878760576248169,-1.8787603378295898,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.878760576248169,-1.7405860424041748,-1.1845436096191406,-1.3470171689987183,-1.1845436096191406,-1.1845431327819824,-1.184543490409851,-1.184543251991272,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.6516265869140625,-1.092991590499878,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.800607442855835,-1.092991828918457,-1.1845433712005615,-1.184543251991272,-1.1845436096191406,-1.1845431327819824,-1.1845436096191406,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845431327819824,-1.184543490409851,-1.8006075620651245,-1.092991590499878,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845440864562988,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.8006072044372559,-1.092991828918457,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.184543490409851,-1.1845431327819824,-1.184543490409851,-1.184543251991272,-1.184543490409851,-1.8006075620651245,-1.0929917097091675,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845438480377197,-1.800607442855835,-1.092991828918457,-1.184543490409851,-1.184543251991272,-1.184543490409851,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543490409851,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.8006075620651245,-1.0929917097091675,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845438480377197,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845438480377197,-1.1845438480377197,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.8006075620651245,-1.092991590499878,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845438480377197,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.1845438480377197,-1.1845437288284302,-1.1845437288284302,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845437288284302,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.184543490409851,-1.1845433712005615,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.800607442855835,-1.0929917097091675,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.8006075620651245,-1.092991590499878,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.1845437288284302,-1.1845433712005615,-1.1845437288284302,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.184543490409851,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.1845436096191406,-1.1845436096191406,-1.1845433712005615,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.800607442855835,-1.092991590499878,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845437288284302,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.184543490409851,-1.1845437288284302,-1.800607442855835,-1.092991590499878,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.184543490409851,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.1845433712005615,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.184543251991272,-1.1845436096191406,-1.800607442855835,-1.0929917097091675,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845438480377197,-1.184543490409851,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.8006073236465454,-1.0929917097091675,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845438480377197,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.184543490409851,-1.184543490409851,-1.184543490409851,-1.1845436096191406,-1.1845436096191406,-1.184543490409851,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.1845433712005615,-1.1845436096191406,-1.8006075620651245,-1.092991590499878,-1.1845438480377197,-1.184543490409851,-1.1845438480377197,-1.184543490409851,-1.1845438480377197,-1.184543490409851,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845436096191406,-1.1845438480377197,-1.184543490409851,-1.1845438480377197,-1.184543490409851,-1.1845438480377197,-1.800607442855835,-1.092991590499878,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.1845431327819824,-1.184543490409851,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.800607442855835,-1.092991590499878,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.8006072044372559,-1.092991828918457,-1.1845433712005615,-1.1845431327819824,-1.1845436096191406,-1.1845431327819824,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.184543490409851,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.184543490409851,-1.8006075620651245,-1.092991590499878,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845439672470093,-1.1845438480377197,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.1845436096191406,-1.1845438480377197,-1.1845436096191406,-1.1845440864562988,-1.800607442855835,-0.8092432022094727,-1.1845433712005615,-1.1845431327819824,-1.184543490409851,-1.184543251991272,-1.1845433712005615,-1.1845431327819824,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.184543251991272,-1.1845433712005615,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.184543251991272,-1.1845433712005615,-1.549118161201477,-1.1845436096191406,-0.9587478637695312,-0.8780468702316284,-0.8780466318130493,-0.8780468702316284,-0.8780466318130493,-0.8780467510223389,-0.8780466318130493,-0.8780469298362732,-0.8780466318130493,-0.8780467510223389,-0.8780466318130493,-0.8780467510223389,-0.8780466318130493,-0.8780466914176941,-0.8780466914176941,-0.8780466318130493,-0.8780466914176941,-0.8780467510223389,-0.8780468106269836,-0.8780466318130493,-0.8780466914176941,-0.8780467510223389,-0.8780468702316284,-0.8780466318130493,-0.8780468702316284,-1.2546055316925049,-1.1845433712005615],\"type\":\"scatter3d\",\"scene\":\"scene\"},{\"x\":[-3.2215323448181152,-3.625343084335327,-4.058604717254639,-4.058605194091797,-4.058605194091797,-4.058605194091797,-4.058605194091797,-4.058604717254639,-4.058604717254639,-4.058605194091797,-4.058605194091797,-4.058604717254639,-4.058605194091797,-4.058605194091797,-4.058604717254639,-4.058604717254639,-4.058604717254639,-4.058605194091797,-4.058604717254639,-4.058604717254639,-4.058604717254639,-4.0586042404174805,-4.058605194091797,-4.058605194091797,-4.058605194091797,-4.058604717254639,-3.6268258094787598,-3.221532106399536,-3.2373833656311035,-3.2215323448181152,-3.2215323448181152,-3.2215323448181152,-3.2215325832366943,-3.2215323448181152,-3.2215323448181152,-3.2215325832366943,-3.221532106399536,-3.2215325832366943,-3.2215325832366943,-3.221532106399536,-3.2215325832366943,-3.221532106399536,-3.221532106399536,-3.221532106399536,-3.221532106399536,-3.2215325832366943,-3.221532106399536,-3.2215325832366943,-3.2215325832366943,-3.2215325832366943,-3.2215323448181152,-3.2215325832366943,-3.2215323448181152,-3.2215323448181152,-3.2215323448181152,-3.264495849609375,-2.80568265914917,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.90010404586792,-2.850092649459839,-2.4842543601989746,-2.578674793243408,-2.5786750316619873,-2.578674793243408,-2.578674793243408,-2.578674793243408,-2.578674793243408,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.5786750316619873,-2.578674793243408,-2.5786750316619873,-2.578674793243408,-2.578674793243408,-2.578674793243408,-2.5286643505096436,-2.1628260612487793,-2.25724720954895,-2.257246971130371,-2.257246732711792,-2.257246971130371,-2.257246732711792,-2.257246971130371,-2.2572474479675293,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246971130371,-2.257246732711792,-2.257246971130371,-2.2572474479675293,-2.257246971130371,-2.2572474479675293,-2.207235097885132,-1.8413975238800049,-1.9358175992965698,-1.9358179569244385,-1.9358175992965698,-1.935817837715149,-1.9358175992965698,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358179569244385,-1.9358175992965698,-1.935817837715149,-1.9358175992965698,-1.9358179569244385,-1.9358175992965698,-1.8858070373535156,-1.5199689865112305,-1.6143901348114014,-1.6143901348114014,-1.6143901348114014,-1.6143901348114014,-1.614390254020691,-1.6143901348114014,-1.614390254020691,-1.614390254020691,-1.614390254020691,-1.6143901348114014,-1.6143901348114014,-1.614390254020691,-1.614390254020691,-1.614390254020691,-1.614390254020691,-1.6143901348114014,-1.6143901348114014,-1.6143901348114014,-1.614390254020691,-1.614390254020691,-1.6143898963928223,-1.6143901348114014,-1.6143898963928223,-1.6143901348114014,-1.6143901348114014,-1.6143901348114014,-1.5643783807754517,-1.198540210723877,-1.2929610013961792,-1.2929608821868896,-1.2929610013961792,-1.2929608821868896,-1.2929610013961792,-1.2929608821868896,-1.2929608821868896,-1.2929611206054688,-1.2929610013961792,-1.2929610013961792,-1.2929611206054688,-1.2929610013961792,-1.2929610013961792,-1.2929610013961792,-1.2929610013961792,-1.2929611206054688,-1.2929610013961792,-1.2929611206054688,-1.2929610013961792,-1.2929610013961792,-1.2929608821868896,-1.2929610013961792,-1.2929608821868896,-1.2929610013961792,-1.2929608821868896,-1.2929610013961792,-1.2429497241973877,-0.8771116733551025,-0.9715324640274048,-0.9715324640274048,-0.9715323448181152,-0.9715324640274048,-0.9715323448181152,-0.9715324640274048,-0.9715325832366943,-0.9715325832366943,-0.9715325832366943,-0.9715325832366943,-0.9715324640274048,-0.9715325832366943,-0.9715325832366943,-0.9715325832366943,-0.9715325832366943,-0.9715325236320496,-0.9715325832366943,-0.9715324640274048,-0.9715325832366943,-0.9715325832366943,-0.9715325832366943,-0.9715323448181152,-0.9715324640274048,-0.9715324640274048,-0.9715324640274048,-0.9715323448181152,-0.9215209484100342,-0.5556831359863281,-0.6501036882400513,-0.6501036882400513,-0.6501036882400513,-0.6501036882400513,-0.6501036882400513,-0.6501036882400513,-0.6501040458679199,-0.6501038074493408,-0.6501038074493408,-0.6501038074493408,-0.6501038074493408,-0.6501038074493408,-0.6501040458679199,-0.6501038074493408,-0.6501039862632751,-0.6501040458679199,-0.6501038074493408,-0.6501040458679199,-0.6501040458679199,-0.6501040458679199,-0.6501038074493408,-0.6501038074493408,-0.6501036882400513,-0.6501036882400513,-0.6501036882400513,-0.6501039266586304,-0.6000926494598389,-0.2342545986175537,-0.32867538928985596,-0.3286755084991455,-0.32867538928985596,-0.3286755084991455,-0.32867538928985596,-0.3286755084991455,-0.3286755681037903,-0.32867562770843506,-0.3286755681037903,-0.3286755681037903,-0.3286755681037903,-0.3286755681037903,-0.3286755681037903,-0.3286755681037903,-0.3286755681037903,-0.3286755084991455,-0.3286755681037903,-0.3286755084991455,-0.32867562770843506,-0.32867562770843506,-0.3286755084991455,-0.32867538928985596,-0.3286755084991455,-0.32867538928985596,-0.3286755084991455,-0.32867538928985596,-0.27866411209106445,0.08717399835586548,-0.007246732711791992,-0.007246792316436768,-0.007246732711791992,-0.007246792316436768,-0.007246732711791992,-0.007246792316436768,-0.007246792316436768,-0.007246911525726318,-0.007246911525726318,-0.007246792316436768,-0.007246911525726318,-0.007246851921081543,-0.007246851921081543,-0.007246851921081543,-0.007246851921081543,-0.007246851921081543,-0.007246792316436768,-0.007246851921081543,-0.007246851921081543,-0.007246911525726318,-0.007246911525726318,-0.007246732711791992,-0.007246792316436768,-0.007246732711791992,-0.007246792316436768,-0.007246732711791992,0.042764484882354736,0.40860259532928467,0.31418174505233765,0.3141818642616272,0.31418174505233765,0.3141818642616272,0.31418174505233765,0.3141818642616272,0.31418168544769287,0.31418168544769287,0.3141816258430481,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418168544769287,0.31418174505233765,0.3141818642616272,0.31418174505233765,0.3141818642616272,0.31418174505233765,0.3641930818557739,0.7300312519073486,0.6356103420257568,0.6356104016304016,0.6356103420257568,0.6356104016304016,0.6356104612350464,0.6356104016304016,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356102824211121,0.6356103420257568,0.6356104016304016,0.6356104612350464,0.6356104016304016,0.6356103420257568,0.6856216788291931,1.051459789276123,0.9570388793945312,0.9570389986038208,0.9570388793945312,0.9570389986038208,0.9570388793945312,0.9570389986038208,0.957038938999176,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.957038938999176,0.9570388793945312,0.9570388793945312,0.9570388793945312,0.9570389986038208,0.9570389986038208,0.9570388793945312,0.9570389986038208,0.9570389986038208,1.0070502758026123,1.3728883266448975,1.2784674167633057,1.2784675359725952,1.2784675359725952,1.2784675359725952,1.2784674167633057,1.2784675359725952,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784674167633057,1.2784675359725952,1.2784675359725952,1.2784675359725952,1.2784675359725952,1.2784674167633057,1.3284788131713867,1.6943169832229614,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998960733413696,1.59989595413208,1.59989595413208,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.59989595413208,1.5998961925506592,1.59989595413208,1.59989595413208,1.5998961925506592,1.5998961925506592,1.5998960733413696,1.5998960733413696,1.59989595413208,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.5998961925506592,1.6499073505401611,2.0157456398010254,1.9213244915008545,1.921324610710144,1.9213244915008545,1.921324610710144,1.9213244915008545,1.921324610710144,1.9213244915008545,1.9213244915008545,1.9213244915008545,1.9213244915008545,1.921324610710144,1.9213244915008545,1.9213244915008545,1.9213244915008545,1.9213244915008545,1.921324610710144,1.9213244915008545,1.921324610710144,1.9213244915008545,1.9213244915008545,1.921324610710144,1.9213244915008545,1.921324610710144,1.9213244915008545,1.921324610710144,1.9213244915008545,1.9713361263275146,2.3371739387512207,2.242753267288208,2.242753505706787,2.242753267288208,2.242753505706787,2.242753267288208,2.242753505706787,2.242753267288208,2.242753267288208,2.242753267288208,2.242753505706787,2.242753267288208,2.242753267288208,2.242753267288208,2.242753028869629,2.242753028869629,2.242753028869629,2.242753267288208,2.242753267288208,2.242753028869629,2.242753028869629,2.242753267288208,2.242753267288208,2.242753505706787,2.242753267288208,2.242753505706787,2.242753267288208,2.292764663696289,2.658602714538574,2.5641815662384033,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641815662384033,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.5641818046569824,2.6141934394836426,2.9800312519073486,2.8856101036071777,2.885610580444336,2.8856101036071777,2.885610580444336,2.885610342025757,2.885610580444336,2.885610342025757,2.885610342025757,2.885610342025757,2.8856101036071777,2.885610342025757,2.885610342025757,2.885610342025757,2.885610342025757,2.885610342025757,2.8856101036071777,2.885610342025757,2.885610342025757,2.885610342025757,2.885610342025757,2.885610580444336,2.8856101036071777,2.885610580444336,2.8856101036071777,2.885610580444336,2.8856101036071777,2.935621738433838,3.301459789276123,3.207038402557373,3.2070388793945312,3.207038402557373,3.2070388793945312,3.207038402557373,3.2070388793945312,3.2070388793945312,3.2070388793945312,3.207038640975952,3.207038640975952,3.2070388793945312,3.2070388793945312,3.207038640975952,3.207038640975952,3.207038640975952,3.2070388793945312,3.207038640975952,3.2070388793945312,3.207038640975952,3.207038640975952,3.2070388793945312,3.207038402557373,3.2070388793945312,3.207038402557373,3.2070388793945312,3.207038402557373,3.2570505142211914,3.6228885650634766,3.528468132019043,3.528468132019043,3.528468132019043,3.528468132019043,3.528468132019043,3.528467893600464,3.528467893600464,3.5284676551818848,3.528467893600464,3.5284676551818848,3.528467893600464,3.5284676551818848,3.528467893600464,3.5284676551818848,3.5284676551818848,3.528467893600464,3.5284676551818848,3.528467893600464,3.5284676551818848,3.5284676551818848,3.528467893600464,3.528468132019043,3.528468132019043,3.528468132019043,3.528468132019043,3.528468132019043,3.578479051589966,3.944316864013672,3.849895715713501,3.84989595413208,3.849895477294922,3.84989595413208,3.849895477294922,3.84989595413208,3.849895477294922,3.849895477294922,3.849895477294922,3.849895715713501,3.849895715713501,3.849895477294922,3.849895477294922,3.849895477294922,3.849895477294922,3.849895715713501,3.849895715713501,3.849895715713501,3.849895477294922,3.849895477294922,3.849895715713501,3.849895477294922,3.84989595413208,3.849895477294922,3.84989595413208,3.849895477294922,3.8999075889587402,4.265745639801025,4.171325206756592,4.171324729919434,4.171325206756592,4.171324729919434,4.171325206756592,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171324729919434,4.171325206756592,4.171324729919434,4.171325206756592,4.171324729919434,4.171325206756592,4.221336364746094,4.587174415588379,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.492753028869629,4.542765140533447,5.253429412841797,4.814181804656982,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.814182281494141,4.720423698425293,4.814181804656982,5.161438465118408,5.583327770233154,5.583327293395996,5.583327770233154,5.583327293395996,5.583327293395996,5.583327293395996,5.583327770233154,5.583327293395996,5.583327770233154,5.583327293395996,5.583327770233154,5.583327293395996,5.583327293395996,5.583327293395996,5.583327293395996,5.583327293395996,5.583327293395996,5.583327770233154,5.583327770233154,5.583327770233154,5.583327293395996,5.583327770233154,5.583327293395996,5.583327770233154,5.161153316497803,4.814181804656982],\"y\":[-5.327062129974365,-5.499529838562012,-5.197632312774658,-4.876203536987305,-4.554775238037109,-4.233346939086914,-3.9119179248809814,-3.590489387512207,-3.2690606117248535,-2.9476318359375,-2.6262035369873047,-2.3047749996185303,-1.9833464622497559,-1.6619179248809814,-1.340489149093628,-1.0190606117248535,-0.6976321339607239,-0.376203715801239,-0.05477488040924072,0.26665377616882324,0.588081955909729,0.9095107316970825,1.230939507484436,1.5523678064346313,1.873795986175537,2.1952247619628906,2.8621323108673096,2.7086517810821533,-5.664051532745361,-5.327062606811523,-5.005634307861328,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.7443664073944092,2.0657944679260254,2.387223243713379,2.7086524963378906,2.9504008293151855,-6.044220447540283,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.719919443130493,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056335926055908,-0.18420523405075073,0.13722360134124756,0.458652138710022,0.7800807952880859,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.216653347015381,-6.044219970703125,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.7443664073944092,2.0657944679260254,2.3872230052948,2.7086524963378906,3.216653347015381,-6.044220447540283,-5.327062606811523,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.755633592605591,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056335926055908,-0.18420523405075073,0.13722360134124756,0.458652138710022,0.7800807952880859,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.216653347015381,-6.044219970703125,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199196815490723,-3.3984909057617188,-3.0770626068115234,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.74436616897583,2.0657944679260254,2.387223243713379,2.7086524963378906,3.216653347015381,-6.044220447540283,-5.327062606811523,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056335926055908,-0.18420523405075073,0.13722360134124756,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086522579193115,3.2166528701782227,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.719919443130493,-3.3984909057617188,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056336522102356,-0.18420517444610596,0.137223482131958,0.4586517810821533,0.7800805568695068,1.1015090942382812,1.4229378700256348,1.74436616897583,2.0657947063446045,2.387223243713379,2.7086522579193115,3.21665358543396,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.469919204711914,-1.1484907865524292,-0.8270620703697205,-0.5056336522102356,-0.1842050552368164,0.1372237205505371,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.1842050552368164,0.13722360134124756,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.74436616897583,2.0657947063446045,2.387223243713379,2.7086522579193115,3.21665358543396,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.18420511484146118,0.137223482131958,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.21665358543396,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770621299743652,-2.755633592605591,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.18420517444610596,0.13722360134124756,0.45865190029144287,0.7800806760787964,1.1015092134475708,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327061653137207,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.505633533000946,-0.1842050552368164,0.1372237205505371,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.505633533000946,-0.1842050552368164,0.13722360134124756,0.4586520195007324,0.7800807952880859,1.1015090942382812,1.4229378700256348,1.7443660497665405,2.0657947063446045,2.387223243713379,2.7086522579193115,3.2166531085968018,-6.044219970703125,-5.327061653137207,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.1842050552368164,0.13722360134124756,0.458652138710022,0.7800805568695068,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327061653137207,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.4699194431304932,-1.1484907865524292,-0.82706218957901,-0.505633533000946,-0.1842050552368164,0.1372237205505371,0.458652138710022,0.7800807952880859,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.755633592605591,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.8270623087882996,-0.5056336522102356,-0.18420517444610596,0.13722360134124756,0.45865190029144287,0.7800806760787964,1.1015092134475708,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.18420511484146118,0.137223482131958,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.2166531085968018,-6.044219970703125,-5.327061653137207,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.755633592605591,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484909057617188,-0.8270623087882996,-0.5056336522102356,-0.18420517444610596,0.13722360134124756,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044219970703125,-5.327061653137207,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770621299743652,-2.755633592605591,-2.4342050552368164,-2.112776279449463,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056336522102356,-0.1842050552368164,0.13722360134124756,0.458652138710022,0.7800805568695068,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.216653347015381,-6.044220447540283,-5.327061653137207,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.505633533000946,-0.1842050552368164,0.13722360134124756,0.458652138710022,0.7800807952880859,1.1015090942382812,1.4229378700256348,1.7443662881851196,2.0657947063446045,2.387223243713379,2.7086522579193115,3.21665358543396,-6.044219970703125,-5.327062606811523,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984906673431396,-3.0770621299743652,-2.755633592605591,-2.4342050552368164,-2.112776517868042,-1.7913479804992676,-1.469919204711914,-1.1484906673431396,-0.8270620703697205,-0.5056336522102356,-0.18420511484146118,0.13722360134124756,0.4586522579193115,0.7800806760787964,1.1015092134475708,1.4229378700256348,1.74436616897583,2.0657947063446045,2.387223243713379,2.7086520195007324,3.21665358543396,-6.044220447540283,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199196815490723,-3.3984909057617188,-3.0770626068115234,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.74436616897583,2.0657944679260254,2.3872230052948,2.7086524963378906,3.216653347015381,-6.044220447540283,-5.327062606811523,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056335926055908,-0.18420523405075073,0.13722360134124756,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.216653347015381,-6.044219970703125,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199199199676514,-3.3984909057617188,-3.0770626068115234,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.7443664073944092,2.0657944679260254,2.387223243713379,2.7086524963378906,3.21665358543396,-6.044220447540283,-5.327062606811523,-5.005633354187012,-4.684205055236816,-4.362776756286621,-4.041347503662109,-3.7199196815490723,-3.3984909057617188,-3.0770623683929443,-2.75563383102417,-2.4342050552368164,-2.112776517868042,-1.791347861289978,-1.4699193239212036,-1.1484907865524292,-0.82706218957901,-0.5056335926055908,-0.18420523405075073,0.13722360134124756,0.458652138710022,0.7800806760787964,1.1015090942382812,1.4229378700256348,1.7443664073944092,2.0657947063446045,2.387223243713379,2.7086520195007324,3.216653347015381,-5.704230308532715,-5.327062606811523,-5.00563383102417,-4.684205055236816,-4.362776756286621,-4.041347980499268,-3.7199199199676514,-3.3984909057617188,-3.0770626068115234,-2.75563383102417,-2.4342050552368164,-2.112776756286621,-1.7913479804992676,-1.4699194431304932,-1.1484909057617188,-0.8270623087882996,-0.5056337118148804,-0.18420517444610596,0.13722336292266846,0.4586520195007324,0.7800805568695068,1.1015090942382812,1.4229377508163452,1.7443664073944092,2.0657944679260254,2.387223243713379,2.7086524963378906,3.062133312225342,-5.327061653137207,-5.50719690322876,-5.246162414550781,-4.924734592437744,-4.603305816650391,-4.281877517700195,-3.9604485034942627,-3.6390199661254883,-3.317591428756714,-2.9961631298065186,-2.674734592437744,-2.3533058166503906,-2.031877279281616,-1.7104488611221313,-1.3890202045440674,-1.067591667175293,-0.7461631298065186,-0.42473459243774414,-0.10330569744110107,0.21812283992767334,0.5395510196685791,0.8609797954559326,1.182408332824707,1.5038368701934814,1.825265645980835,2.1466939449310303,2.8276538848876953,2.7086522579193115],\"z\":[-0.6987741589546204,-0.976157546043396,-0.9919840097427368,-0.9919842481613159,-0.9919840097427368,-0.9919843673706055,-0.9919840693473816,-0.9919840097427368,-0.991983950138092,-0.9919840693473816,-0.9919840693473816,-0.9919840693473816,-0.9919840693473816,-0.9919841289520264,-0.991983950138092,-0.991983950138092,-0.9919840693473816,-0.9919840693473816,-0.991983950138092,-0.991983950138092,-0.9919842481613159,-0.9919840097427368,-0.9919840693473816,-0.9919840097427368,-0.9919843673706055,-0.9919840097427368,-0.6481969356536865,-0.6987742185592651,-0.9198154211044312,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987743973731995,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.6393002271652222,-1.107728362083435,-0.6987740993499756,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987743973731995,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987741589546204,-0.5025411248207092,-1.1077284812927246,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.502541184425354,-1.1077284812927246,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.5025411248207092,-1.107728362083435,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.502541184425354,-1.1077284812927246,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987743973731995,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.502541184425354,-1.107728362083435,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987744569778442,-0.6987742781639099,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987744569778442,-0.6987742781639099,-0.6987744569778442,-0.6987742781639099,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987744569778442,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987742781639099,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.5025412440299988,-1.107728362083435,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987744569778442,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.502541184425354,-1.1077284812927246,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.5025411248207092,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742781639099,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987742781639099,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987744569778442,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.502541184425354,-1.107728362083435,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.5025411248207092,-1.107728123664856,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.502541184425354,-1.1077282428741455,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.502541184425354,-1.1077286005020142,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743973731995,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742781639099,-0.6987742185592651,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.6987743377685547,-0.6987742185592651,-0.5025412440299988,-1.1077284812927246,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987742185592651,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742185592651,-0.6987742781639099,-0.6987743377685547,-0.6987742781639099,-0.6987742185592651,-0.6987743377685547,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987740993499756,-0.502541184425354,-1.107728362083435,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.502541184425354,-1.1077282428741455,-0.6987740993499756,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987743973731995,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987742185592651,-0.5025411248207092,-1.107728362083435,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.502541184425354,-1.1077282428741455,-0.6987741589546204,-0.6987743973731995,-0.6987740993499756,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987743377685547,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987743377685547,-0.6987743377685547,-0.6987743973731995,-0.6987742185592651,-0.6987743973731995,-0.6987741589546204,-0.6987743973731995,-0.6987741589546204,-0.5025411248207092,-0.8420241475105286,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987742781639099,-0.6987742781639099,-0.6987742781639099,-0.6987743973731995,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.6987744569778442,-0.6987741589546204,-0.5700779557228088,-0.6987741589546204,-0.8810641765594482,-0.8200318813323975,-0.8200321793556213,-0.8200318813323975,-0.8200321793556213,-0.8200321793556213,-0.8200320601463318,-0.8200320601463318,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200321793556213,-0.8200320601463318,-0.820032000541687,-0.8200322985649109,-0.8200320601463318,-0.8200321793556213,-0.8200320601463318,-0.8200322985649109,-0.820032000541687,-0.5926142930984497,-0.6987741589546204],\"type\":\"scatter3d\",\"scene\":\"scene2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]}},\"scene2\":{\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]}},\"height\":600,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9950453b-734a-4332-934a-0aa905b083f2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def scatter3d(x, y, z):\n",
        "  plot_df = pd.DataFrame({\"x\": x, \"y\": y, \"z\": z})\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None)\n",
        "  return fig\n",
        "\n",
        "# plt.clf(); plt.cla()\n",
        "# plt.close()\n",
        "for i, idx in enumerate(idxs[:len(shapes)]):\n",
        "  idx = idx[0]\n",
        "  print(idx.shape)\n",
        "  idx = idx.reshape(-1, shapes[i][2], idx_dim)\n",
        "  rows, cols = 1 + (idx.shape[1] - 1) // 2, 2\n",
        "  # fig = plt.figure(figsize=(min(18, 3 * cols), 3))\n",
        "  fig = make_subplots(\n",
        "    rows=rows, cols=cols,\n",
        "    specs=[[{\"type\": \"scene\"} for _ in range(cols)] for _ in range(rows)],\n",
        "    # row_heights=[10 for _ in range(rows)],\n",
        "    vertical_spacing=0.05\n",
        "  )\n",
        "  fig.update_layout(\n",
        "    height=600 * rows,\n",
        "    width=600 * cols\n",
        ")\n",
        "  for ch in range(idx.shape[1]):\n",
        "    idx_ = idx[:, ch].numpy()\n",
        "    # ax = fig.add_subplot(rows, cols, ch + 1, projection=\"3d\")\n",
        "    # ax.scatter(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2], marker=\"+\")\n",
        "    # fig = scatter3d(idx_[::4, 0], idx_[::4, 1], idx_[::4, 2])\n",
        "    row, col = (ch // cols) + 1, (ch % cols) + 1\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=idx_[::, 0],\n",
        "            y=idx_[::, 1],\n",
        "            z=idx_[::, 2],\n",
        "            # z=idx_[::, 1] * 0.0,\n",
        "            # color=None,\n",
        "            # colorscale=\"Viridis\",\n",
        "            # showscale=False\n",
        "        ),\n",
        "        row=row,\n",
        "        col=col,\n",
        "    )\n",
        "  fig.show()\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFWbwRwuE9lu"
      },
      "outputs": [],
      "source": [
        "# model.MW.idx[0, 0]\n",
        "threshold = rows * cols * hidden_dim\n",
        "param_id = 0\n",
        "layer_idx = model.MW.idx[:, param_id: param_id + 100] # threshold\n",
        "n_parts = idx_dim // img_dim\n",
        "scaled_idx = (\n",
        "    MTensor\n",
        "    ._soft_kernel(layer_idx, img_dim)[0]\n",
        "    .reshape(-1, n_parts, img_dim)\n",
        ") * (n_parts ** 0.5)\n",
        "idx_att = torch.argmax(\n",
        "    scaled_idx,\n",
        "    dim=-1\n",
        ")[:, :2]\n",
        "idx_att = idx_att.cpu().detach().numpy()\n",
        "grid = np.zeros((rows, cols))\n",
        "# for pos in range(len(idx_att)):\n",
        "#   idxx, idxy = idx_att[pos]\n",
        "#   idxx, idxy = int(idxx), int(idxy)\n",
        "#   # grid[int(idxx), int(idxy)] += 1\n",
        "#   grid[idxx, idxy] += scaled_idx[pos, 0, idxx] * scaled_idx[pos, 1, idxy]\n",
        "for idxx in range(rows):\n",
        "  for idxy in range(cols):\n",
        "    grid[idxx, idxy] = (scaled_idx[:, 0, idxx] * scaled_idx[:, 1, idxy]).sum()\n",
        "grid = grid / grid.max()\n",
        "\n",
        "# plt.imshow(grid)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.matshow(grid, cmap=\"seismic\")\n",
        "\n",
        "for (i, j), z in np.ndenumerate(grid):\n",
        "    ax.text(j, i, \"{:0.2f}\".format(z), ha=\"center\", va=\"center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSCHvt7xQq6q"
      },
      "outputs": [],
      "source": [
        "MTensor._soft_kernel(y_pred.idx, img_dim)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-K_7fUh2anJ"
      },
      "source": [
        "### Visualização dos índices dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNJnesCt2f1p"
      },
      "outputs": [],
      "source": [
        "soft_W_idx = scaled_idx # MTensor._soft_kernel(model.MW.idx, img_dim)\n",
        "threshold = 100 # rows * cols * hidden_dim\n",
        "# First layer\n",
        "soft_W_idx = soft_W_idx[:, :threshold].reshape(1, -1, idx_dim)\n",
        "# Last layer\n",
        "# soft_W_idx = soft_W_idx[:, threshold:].reshape(1, -1, idx_dim)\n",
        "soft_W_idx = soft_W_idx.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gaTHY1L2it1"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "sample_idx = np.random.choice(\n",
        "    len(soft_W_idx),\n",
        "    min(len(soft_W_idx), 10000),\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "W_idx_tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=10,\n",
        ").fit_transform(soft_W_idx[sample_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows6icrE2unf"
      },
      "outputs": [],
      "source": [
        "plot_df = pd.DataFrame(\n",
        "    {\n",
        "        \"W_idx x tsne\": W_idx_tsne[:, 0],\n",
        "        \"W_idx y tsne\": W_idx_tsne[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "plot_df.plot.scatter(\n",
        "    x=\"W_idx x tsne\",\n",
        "    y=\"W_idx y tsne\",\n",
        "    figsize=(24, 4),\n",
        "    grid=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUDvxRS3yZu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "RGCfrrmCXap_",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "e0TdCxX0Jzn0",
        "_T9hF3Uoi3tF",
        "kTfYY3SQXNJF",
        "jdZ8zHIcPQPS",
        "QQRFtDATXUmH",
        "039kGqbPXp4d"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyN6WHoeysKWsmZZ3sH0mXsy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}