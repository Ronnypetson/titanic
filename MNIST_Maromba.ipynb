{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdTbjAGjsnP"
      },
      "source": [
        "## Experimentos do Produto Interno Maromba no MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQSziNdjoUm"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# !pip3 install ipympl\n",
        "# !pip3 install mpl_interactions\n",
        "# %matplotlib widget\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "# import mpl_interactions.ipyplot as iplt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# %matplotlib inline\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6dxGxcHAx5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1827144-80f6-4186-f769-6fdc52edd08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ]\n",
        ")\n",
        "\n",
        "tr = ToTensor()\n",
        "# cifar10_norm = Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "cifar10_norm = Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "_channels = 3\n",
        "channels = 1\n",
        "img_dim = 32\n",
        "\n",
        "def _transform(x):\n",
        "  # return (\n",
        "  #     cifar10_norm(tr(x))\n",
        "  #     .reshape(channels, img_dim, img_dim)\n",
        "  #     .permute(1, 2, 0)\n",
        "  #     .reshape(-1)\n",
        "  # )\n",
        "  return (\n",
        "      cifar10_norm(tr(x))\n",
        "      .reshape(_channels, img_dim, img_dim)\n",
        "      .permute(1, 2, 0)\n",
        "      .mean(dim=-1)\n",
        "      .reshape(-1)\n",
        "  )\n",
        "  # return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "  # return (tr(x)).reshape(-1)\n",
        "\n",
        "bsize = 8 ###\n",
        "\n",
        "# SOURCE_DATASET = MNIST\n",
        "# SOURCE_DATASET = FashionMNIST\n",
        "SOURCE_DATASET = CIFAR10\n",
        "\n",
        "MNIST_train_data = SOURCE_DATASET(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = SOURCE_DATASET(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH7FAomMIRJC"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# # config = {\n",
        "# #     \"features\": {\n",
        "# #         \"pre-set\": channels,\n",
        "# #         \"sets\":    [1024,    1024,    1,],\n",
        "# #         \"samples\": [25,      1,       1024,],\n",
        "# #     },\n",
        "# #     \"params\": {\n",
        "# #         \"sets\":    [4,       1,       num_classes,],\n",
        "# #         \"samples\": [(76, 1), (1, 5),  (1025, 1),],\n",
        "# #         \"is conv\": [True,    True,    False,]\n",
        "# #     },\n",
        "# # }\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 4, 5, padding=\"same\", bias=False)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 8, 5, padding=\"same\", bias=False)\n",
        "#         self.conv2 = nn.Conv2d(4, 1, 1, padding=\"same\", bias=False)\n",
        "#         self.fc1 = nn.Linear(1024, 10, bias=False)\n",
        "#         # self.fc2 = nn.Linear(100, 10, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.elu(self.conv1(x))\n",
        "#         x = F.elu(self.conv2(x))\n",
        "#         # x = F.elu(self.conv3(x))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7DuUfPdITHx"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3spYLqaUIa3n"
      },
      "outputs": [],
      "source": [
        "# trainloader = train_data_loader\n",
        "# for epoch in range(2):  # loop over the dataset multiple times\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 60 == 59:    # print every 60 mini-batches\n",
        "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 60:.3f}')\n",
        "#             running_loss = 0.0\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE6KCRaT_zjU"
      },
      "outputs": [],
      "source": [
        "# img, lbl = next(iter(test_data_loader))\n",
        "# img = img.reshape(-1, 3, 32, 32)\n",
        "# img = img.permute(0, 2, 3, 1).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHhauWzb_4b1"
      },
      "outputs": [],
      "source": [
        "# img[0].min(), img[0].max(), img[0].mean()\n",
        "\n",
        "# cifar10_classes = [\n",
        "#     \"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "# ]\n",
        "# _idx = 3\n",
        "# print(cifar10_classes[lbl[_idx].item()])\n",
        "# plt.imshow(img[_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = 2.0 * ((row + offset) / rows) - 1.0\n",
        "        idx[row, col, ch, 1] = 2.0 * ((col + offset) / cols) - 1.0\n",
        "        idx[row, col, ch, 2] = 0.001 * ((ch  + offset) /  chs) - 0.005\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx\n",
        "\n",
        "def _cartesian_idx(rows, cols, chs=1, d=2, offset=0, mag=0.1):\n",
        "  # idx = np.zeros((rows, cols, chs, d))\n",
        "  idx = np.random.uniform(low=-mag, high=mag, size=(rows, cols, chs, d))\n",
        "  # idx = np.ones((rows, cols, chs, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for ch in range(chs):\n",
        "        idx[row, col, ch, 0] = (row + offset)\n",
        "        idx[row, col, ch, 1] = (col + offset)\n",
        "        idx[row, col, ch, 2] = (ch  + offset)\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols * chs, d)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilOucSYLd2zy"
      },
      "source": [
        "### Kernels, similaridades e funções de índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKu4c8hisNY"
      },
      "source": [
        "#### Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG4U__-kiw2J"
      },
      "outputs": [],
      "source": [
        "def _soft_kernel(idxu, part_dim):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  assert d_idx % part_dim == 0\n",
        "  range = 20.0\n",
        "  norm_idxu = range * idxu.reshape(m, d_u, -1, part_dim) - (range / 2.0)\n",
        "  norm_idxu = torch.softmax(norm_idxu, dim=-1)\n",
        "  dim_norm = (d_idx // part_dim) ** 0.5\n",
        "  norm_idxu = norm_idxu.reshape(m, d_u, d_idx) / dim_norm\n",
        "  return norm_idxu\n",
        "\n",
        "def _cosine_kernel(idxu, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  idxu: M x d_u x d_idx\n",
        "  \"\"\"\n",
        "  # TODO: compute min_idx, max_idx and normalize\n",
        "  m, d_u, d_idx = idxu.shape\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-4\n",
        "  idxu = (idxu - min_idxu) / (max_idxu - min_idxu + eps)\n",
        "  norm_idxu = idxu / (torch.norm(idxu, dim=-1).unsqueeze(-1) + eps)\n",
        "  # Reverse kernel trick for polynomial x^2\n",
        "  idxu2 = norm_idxu.reshape(-1, d_idx, 1)\n",
        "  idxu2 = torch.bmm(idxu2, idxu2.permute(0, 2, 1)).reshape(m, d_u, -1)\n",
        "  # return norm_idxu\n",
        "  return idxu2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLr5gOnn5RRu"
      },
      "source": [
        "#### Similaridades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9RbSzv45T8B"
      },
      "outputs": [],
      "source": [
        "def squared_cosine(idxu, idxv):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = torch.bmm(\n",
        "      idxu.reshape(-1, 1, d_idx),\n",
        "      idxv.reshape(-1, d_idx, 1),\n",
        "  )\n",
        "  # sim = (torch.exp(sim) - 1.0) / (1.718)\n",
        "  sim = sim ** 4.0\n",
        "  return sim\n",
        "\n",
        "def relu_cosine(idxu, idxv, bias=0.9):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  idxv: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  assert idxu.shape == idxv.shape\n",
        "  d_idx = idxu.shape[-1]\n",
        "  sim = nn.functional.relu(\n",
        "      torch.bmm(\n",
        "          idxu.reshape(-1, 1, d_idx),\n",
        "          idxv.reshape(-1, d_idx, 1),\n",
        "      )\n",
        "      - bias\n",
        "  )\n",
        "  sim = sim.reshape(idxu.shape[:-1])\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzFCy32AGsX"
      },
      "source": [
        "#### Funções-valor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvUcMTxDAJlx"
      },
      "outputs": [],
      "source": [
        "def vecsum(u, v):\n",
        "  return u + v\n",
        "\n",
        "def vecmean(u, v):\n",
        "  return (u + v) / 2.0\n",
        "\n",
        "def vecprod(u, v):\n",
        "  \"\"\"\n",
        "  Element-wise product. NOT dot product.\n",
        "  \"\"\"\n",
        "  return u * v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvUP7nZjDd7"
      },
      "source": [
        "#### Dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ytm2bU_JvrK"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzos5YCYJozy"
      },
      "outputs": [],
      "source": [
        "class Pairwise:\n",
        "  def __init__(self, f):\n",
        "    \"\"\"\n",
        "    f: (pre_shape x d_val, pre_shape x d_val) -> pre_shape x d_val_out\n",
        "    \"\"\"\n",
        "    self._f = f\n",
        "\n",
        "  def __call__(self, u, v):\n",
        "    \"\"\"\n",
        "    u: pre_shape_u x d_u x d_val\n",
        "    v: pre_shape_v x d_v x d_val\n",
        "    ans: pre_shape_u x pre_shape_v x d_u x d_v x d_val_out\n",
        "    \"\"\"\n",
        "    ps_u, ps_v = u.shape[:-2], v.shape[:-2]\n",
        "    pps_u, pps_v = np.prod(ps_u), np.prod(ps_v)\n",
        "    d_u, d_val = u.shape[-2:]\n",
        "    d_v, d_valv = v.shape[-2:]\n",
        "    assert d_val == d_valv\n",
        "    # u, v: pps_u x pps_v x d_u x d_v x d_val\n",
        "    u = u.reshape(pps_u,     1, d_u,   1, d_val)\n",
        "    v = v.reshape(    1, pps_v,   1, d_v, d_val)\n",
        "    u = u.repeat(     1, pps_v,   1, d_v,     1)\n",
        "    v = v.repeat( pps_u,     1, d_u,   1,     1)\n",
        "    # fuv: ps_u x ps_v x d_u x d_v x d_val_out\n",
        "    fuv = self._f(u, v)\n",
        "    fuv = fuv.reshape(*ps_u, *ps_v, d_u, d_v, -1)\n",
        "    return fuv\n",
        "\n",
        "def __minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = min_idxu + ((idxu - min_idxu) / (max_idxu - min_idxu + eps))\n",
        "  return idxu\n",
        "\n",
        "def minmax_normalize(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  min_idxu = torch.min(idxu, dim=1)[0].unsqueeze(1)\n",
        "  max_idxu = torch.max(idxu, dim=1)[0].unsqueeze(1)\n",
        "  eps = 1e-6\n",
        "  idxu = 2.0 * ((idxu - min_idxu) / (max_idxu - min_idxu + eps)) - 1.0\n",
        "  return idxu\n",
        "\n",
        "def norm_normalize(u):\n",
        "  \"\"\"\n",
        "  u: pre_shape x d_val\n",
        "  \"\"\"\n",
        "  eps = 1e-6\n",
        "  u = u / (u.norm(dim=-1).unsqueeze(-1) + eps)\n",
        "  return u\n",
        "\n",
        "def normalized(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = minmax_normalize(idxu)\n",
        "  idxu = norm_normalize(idxu)\n",
        "  return idxu\n",
        "\n",
        "def poly1norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  idxu = (0.5 ** 0.5) * torch.cat([idxu, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "def poly2norm(idxu):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  idxu = normalized(idxu)\n",
        "  ones = torch.ones((*idxu.shape[:-1], 1)).to(idxu.device)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  _idxu = idxu.reshape((-1, d_idx, 1))\n",
        "  middle = (\n",
        "      torch.bmm(_idxu, _idxu.permute(0, 2, 1))\n",
        "      .reshape((*idxu.shape[:-1], d_idx ** 2))\n",
        "  )\n",
        "  idxu = 0.5 * torch.cat([(2.0 ** 0.5) * idxu, middle, ones], dim=-1)\n",
        "  return idxu\n",
        "\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict as dd\n",
        "from itertools import product\n",
        "\n",
        "@lru_cache()\n",
        "def poly_terms(idx_dim, degree):\n",
        "  combs = dd(int)\n",
        "  ranges = (range(idx_dim) for _ in range(degree))\n",
        "  for idxs in product(*ranges):\n",
        "      comb = tuple(sorted(idxs))\n",
        "      combs[comb] += 1\n",
        "  return list(combs.items())\n",
        "\n",
        "\"\"\"\n",
        "sigmoid(8*x - 4)\n",
        "1/(1 + e^4)\n",
        "+ (8 e^4 x)/(1 + e^4)^2\n",
        "+ (32 e^4 (e^4 - 1) x^2)/(1 + e^4)^3\n",
        "+ (256 (e^4 - 4 e^8 + e^12) x^3)/(3 (1 + e^4)^4)\n",
        "+ (512 e^4 (-1 + 11 e^4 - 11 e^8 + e^12) x^4)/(3 (1 + e^4)^5)\n",
        "+ (4096 (e^4 - 26 e^8 + 66 e^12 - 26 e^16 + e^20) x^5)/(15 (1 + e^4)^6)\n",
        "+ O(x^6)\n",
        "(Taylor series)\n",
        "-------------------------\n",
        "0.017986  * x^0\n",
        "0.14130   * x^1\n",
        "0.5448747 * x^2\n",
        "1.3474883 * x^3\n",
        "2.290065  * x^4\n",
        "2.4479883 * x^5\n",
        "\"\"\"\n",
        "\n",
        "def poly_norm(idxu, degree):\n",
        "  \"\"\"\n",
        "  idxu: pre_shape x d_idx\n",
        "  \"\"\"\n",
        "  pre_shape = idxu.shape[:-1]\n",
        "  idx_dim = idxu.shape[-1]\n",
        "  idxu = normalized(idxu)\n",
        "  terms = poly_terms(idx_dim, degree)\n",
        "  factors = torch.tensor([term[1] for term in terms]).float().to(idxu.device)\n",
        "  factors = factors ** 0.5\n",
        "  intidx = torch.tensor([list(term[0]) for term in terms]).long().to(idxu.device)\n",
        "  intidx = intidx.reshape(-1)\n",
        "  idxu = idxu.reshape(-1, idx_dim)[:, intidx]\n",
        "  idxu = idxu.reshape(*pre_shape, degree, -1)\n",
        "  idxu = idxu.prod(dim=-2) * factors.reshape(*((1,) * len(pre_shape)), idxu.shape[-1])\n",
        "  return idxu\n",
        "\n",
        "@lru_cache()\n",
        "def get_eye(m, d_u, d_v, n, device=\"cpu\"):\n",
        "  eye = (\n",
        "      torch.eye(max(d_u, d_v))\n",
        "      [:d_u, :d_v]\n",
        "      .unsqueeze(0)\n",
        "      .unsqueeze(-1)\n",
        "      .repeat(m, 1, 1, n)\n",
        "      .to(device)\n",
        "  )\n",
        "  return eye\n",
        "\n",
        "def log_sinkhorn(log_alpha, n_iter):\n",
        "    \"\"\" https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/sampling/permutations.html\n",
        "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
        "    Concerning nonnegative matrices and doubly stochastic\n",
        "    matrices. Pacific Journal of Mathematics, 1967\n",
        "    Args:\n",
        "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
        "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
        "      n_iters: number of sinkhorn iterations\n",
        "    \"\"\"\n",
        "    for _ in range(n_iter):\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "        # log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
        "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
        "    return log_alpha.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TdCxX0Jzn0"
      },
      "source": [
        "##### Dot products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "LBhkMTmSeAP0"
      },
      "outputs": [],
      "source": [
        "def _rdot(u, v, *args):\n",
        "  \"\"\"\n",
        "  \"Regular Dot product\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  \"\"\"\n",
        "  m, d_u, d_val = u.shape\n",
        "  n, d_v, _d_val = v.shape\n",
        "  if d_u != d_v:\n",
        "    return _nsbmd(u, v, *args)\n",
        "  assert _d_val == d_val\n",
        "  dot = (\n",
        "      u.permute(0, 2, 1).reshape(-1, d_u)\n",
        "      @ v.permute(1, 0, 2).reshape(d_v, -1)\n",
        "  ).reshape(m, d_val, n, d_val).permute(0, 2, 1, 3)\n",
        "  dot = torch.diagonal(dot, dim1=2, dim2=3)\n",
        "  return dot\n",
        "\n",
        "def _knndot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"k-NN Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  num_neigh = 1\n",
        "  dots = []\n",
        "  q_idxu = idxu.cpu().detach().numpy().reshape(-1, d_idx)\n",
        "  for _pos in range(n):\n",
        "    neigh = NearestNeighbors(n_neighbors=num_neigh, metric=\"cosine\")\n",
        "    neigh.fit(idxv[_pos].cpu().detach().numpy().reshape(-1, d_idx))\n",
        "    n_idxu = neigh.kneighbors(\n",
        "        q_idxu, return_distance=False\n",
        "    ).reshape(-1)\n",
        "    n_idxu = torch.from_numpy(n_idxu).long()\n",
        "    _v = v[_pos].reshape(-1, d_val)[n_idxu].reshape(m, d_u, d_val)\n",
        "    # _dot: M x d_val x d_val\n",
        "    _dot = torch.bmm(_v.permute(0, 2, 1), _v)\n",
        "    # _dot: M x 1 x d_val\n",
        "    _dot = torch.diagonal(_dot, dim1=1, dim2=2).unsqueeze(1)\n",
        "    dots.append(_dot)\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.cat(dots, dim=1)\n",
        "  return dot\n",
        "\n",
        "def _icbmd(u, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Conv Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x 1 x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_v == 1\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: M x d_u x N\n",
        "  idxv = (\n",
        "      (idxu.reshape(m * d_u, d_idx))\n",
        "      @ idxv.permute(2, 1, 0).reshape(d_idx, n)\n",
        "  ).reshape(m, d_u, n)\n",
        "  # idxv: M x N x d\n",
        "  idxv = torch.bmm(\n",
        "      u.permute(0, 2, 1),\n",
        "      idxv\n",
        "  ).permute(0, 2, 1)\n",
        "  return idxv\n",
        "\n",
        "def _ibmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Index Batch Maromba Dot\"\n",
        "  u: M x d_u x d_valu\n",
        "  v: N x d_v x d_valv\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_valu, d_valv = u.shape[-1], v.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxu = poly_norm(idxu, 1) # / (d_u)\n",
        "  # idxv = poly_norm(idxv, 1) # / (d_v)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxv: N x d_idx x d_valv\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxv: M x d_u x (d_valv * N)\n",
        "  idxv = (\n",
        "      idxu.reshape(m * d_u, d_idx)\n",
        "      @ idxv.permute(1, 2, 0).reshape(d_idx, d_valv * n)\n",
        "  ).reshape(m, d_u, d_valv * n)\n",
        "  # idxv: M x d_valu x (d_valv * N)\n",
        "  idxv = torch.bmm(u.permute(0, 2, 1), idxv)\n",
        "  if d_valv == 1:\n",
        "    # idxv: M x N x d_valu\n",
        "    idxv = idxv.reshape(m, d_valu, n).permute(0, 2, 1)\n",
        "  else:\n",
        "    # idxv: M x N x d_valu or error\n",
        "    idxv = idxv.reshape(m, d_valu, d_valv, n).permute(0, 3, 1, 2)\n",
        "    idxv = torch.diagonal(idxv, dim1=2, dim2=3)\n",
        "  return idxv\n",
        "\n",
        "def _fbmd(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Fast Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  d_idx = idxu.shape[-1]\n",
        "  # idxu: M x d_val x d_idx\n",
        "  # idxv: N x d_idx x d_val\n",
        "  idxu = torch.bmm(u.permute(0, 2, 1), idxu)\n",
        "  idxv = torch.bmm(idxv.permute(0, 2, 1), v)\n",
        "  # idxu: M x N x d_val\n",
        "  idxu = (\n",
        "    (\n",
        "        idxu.reshape(m * d_val, d_idx)\n",
        "        @ (\n",
        "            idxv\n",
        "            .permute(0, 2, 1)\n",
        "            .reshape(n * d_val, d_idx)\n",
        "            .T\n",
        "          )\n",
        "    ).reshape(m, d_val, n, d_val)\n",
        "    .permute(0, 2, 1, 3)\n",
        "  )\n",
        "  idxu = torch.diagonal(idxu, dim1=2, dim2=3)\n",
        "  return idxu\n",
        "\n",
        "def batch_mdot(u, v, idxu, idxv) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  if d_idx * (d_u + n) < n * d_u * (d_idx + 1):\n",
        "    return _fbmd(u, v, idxu, idxv)\n",
        "  else:\n",
        "    return _ibmd(u, v, idxu, idxv)\n",
        "\n",
        "def _nsbmd(u, v, idxu, idxv, bias=0.5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  \"Non-linear Similarity Batch Maromba Dot\"\n",
        "  u: M x d_u x d_val\n",
        "  v: N x d_v x d_val\n",
        "  idxu: M x d_u x d_idx\n",
        "  idxv: N x d_v x d_idx\n",
        "  \"\"\"\n",
        "  m, d_u, d_idx  = idxu.shape\n",
        "  n, d_v, d_idxv = idxv.shape\n",
        "  d_val = u.shape[-1]\n",
        "  assert d_idx == d_idxv\n",
        "  assert d_val == v.shape[-1]\n",
        "  assert (m, d_u) == u.shape[:2]\n",
        "  assert (n, d_v) == v.shape[:2]\n",
        "  idxu = normalized(idxu)\n",
        "  idxv = normalized(idxv)\n",
        "  # idxuv: M x d_u x d_v x N\n",
        "  ###\n",
        "  # idxuv = idxu.reshape(m * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # mag = 80.0\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv - (mag / 2.0), dim=2)\n",
        "  # # idxuv = nn.functional.softmax(mag * idxuv, dim=2)\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), 6)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  from random import randint\n",
        "  bsidx = randint(0, m - 1)\n",
        "  m_ = 1 # m,\n",
        "  idxuv = (\n",
        "      idxu[bsidx].reshape(m_ * d_u, d_idx)\n",
        "      @ idxv.reshape(n * d_v, d_idx).T\n",
        "  )\n",
        "  idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  mag = 80.0\n",
        "  ###\n",
        "  # siter = 6\n",
        "  # idxuv = (\n",
        "  #     log_sinkhorn(mag * idxuv.permute(0, 3, 1, 2), siter)\n",
        "  #     .permute(0, 2, 3, 1)\n",
        "  # )\n",
        "  ###\n",
        "  #### Tanh seems to work for high-dimensional idx\n",
        "  #### ReLU(x - alpha) / (1.0 - alpha) works for small samples\n",
        "  #### Índices idxu bem espaçados induzem um bom (??) casamento\n",
        "  #### para d_v suficientemente grande e algum alpha\n",
        "  # alpha = 1.0 - 1.0 / d_v # 0.95\n",
        "  # alpha = min(0.999, alpha)\n",
        "  # alpha = min(0.999, max(0.9, 1.0 - 1.0 / d_v))\n",
        "  alpha = 0.93\n",
        "  idxuv = nn.functional.relu(idxuv - alpha) / (1.0 - alpha)\n",
        "  ###\n",
        "  idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # from random import randint\n",
        "  # bsidx = randint(0, m - 1)\n",
        "  # m_ = 1\n",
        "  # mag = 80.0\n",
        "  # # idxu = log_sinkhorn(mag * idxu[bsidx].unsqueeze(0), 6)\n",
        "  # # idxv = log_sinkhorn(mag * idxv, 6)\n",
        "  # idxu = nn.functional.softmax(mag * idxu[bsidx].unsqueeze(0), dim=1)\n",
        "  # idxv = nn.functional.softmax(mag * idxv, dim=1)\n",
        "  # idxuv = idxu.reshape(m_ * d_u, d_idx) @ idxv.reshape(n * d_v, d_idx).T\n",
        "  # idxuv = idxuv.reshape(m_, d_u, n, d_v).permute(0, 1, 3, 2)\n",
        "  # idxuv = idxuv.repeat(m, 1, 1, 1)\n",
        "  ###\n",
        "  # uidxuv: (M x d_val x d_v x N) -> (N x d_v x d_val x M)\n",
        "  uidxuv = (\n",
        "      torch.bmm(\n",
        "        u.permute(0, 2, 1),\n",
        "        idxuv.reshape(m, d_u, d_v * n)\n",
        "      )\n",
        "      .reshape(m, d_val, d_v, n)\n",
        "      .permute(3, 2, 1, 0)\n",
        "  )\n",
        "  # uidxuvv: N x M x d_val x d_val\n",
        "  uidxuvv = (\n",
        "      torch.bmm(\n",
        "          uidxuv.permute(0, 3, 2, 1).reshape(n * m, d_val, d_v),\n",
        "          v.unsqueeze(1).repeat(1, m, 1, 1).reshape(n * m, d_v, d_val)\n",
        "      )\n",
        "      .reshape(n, m, d_val, d_val)\n",
        "  )\n",
        "  # dot: M x N x d_val\n",
        "  dot = torch.diagonal(uidxuvv, dim1=2, dim2=3)\n",
        "  dot = dot.permute(1, 0, 2)\n",
        "  return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "    self._eps = 1e-6\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    if dim == -1:\n",
        "      dim = len(mt.data.shape) - 1\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  @staticmethod\n",
        "  def permute(mt, perm):\n",
        "    idx_perm = perm + (-1,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.permute(*perm),\n",
        "        mt.idx.permute(*idx_perm),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns \"mdot\"\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    ###\n",
        "    # _nsbmd\n",
        "    # _rdot\n",
        "    # _knndot\n",
        "    # _fbmd\n",
        "    # _ibmd, _mbmd\n",
        "    # batch_mdot\n",
        "    ###\n",
        "    mdot = _nsbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1], 1),\n",
        "        b.data.reshape(-1, b.data.shape[-1], 1),\n",
        "        aidx,\n",
        "        bidx,\n",
        "    )\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # New indices\n",
        "    # ###\n",
        "    # _aidx = aidx.mean(dim=-2, keepdim=True)\n",
        "    # _bidx = bidx.mean(dim=-2, keepdim=True)\n",
        "    # onesa = torch.ones(_aidx.shape).to(_aidx.device)\n",
        "    # onesb = torch.ones(_bidx.shape).to(_bidx.device)\n",
        "    # midx = (\n",
        "    #     _nsbmd(_aidx, onesb, _aidx, _bidx)\n",
        "    #     + _nsbmd(onesa, _bidx, _aidx, _bidx)\n",
        "    # ) / 2.0\n",
        "    ###\n",
        "    _aidx = aidx.mean(dim=-2, keepdim=True)\n",
        "    _bidx = bidx.mean(dim=-2, keepdim=True)\n",
        "    _bnorm = _bidx.norm(dim=-1)\n",
        "    _bidx *= 0.0\n",
        "    _bidx[:, :, 2] = _bnorm\n",
        "    _aidx[:, :, 2] = 0.0\n",
        "    # midx = (\n",
        "    #     _aidx + _bidx.permute(1, 0, 2)\n",
        "    # ) / 2.0\n",
        "    # midx = (_aidx + _bidx.permute(1, 0, 2))\n",
        "    __bidx = (\n",
        "        torch.arange(1, _bidx.shape[0] + 1)\n",
        "        .to(_bidx.device)\n",
        "        .reshape(1, _bidx.shape[0], 1)\n",
        "        / (_bidx.shape[0])\n",
        "    )\n",
        "    midx = _aidx + 1e-3 * __bidx\n",
        "    ###\n",
        "    # midx = _icbmd(aidx, aidx, bidx.sum(dim=-2, keepdim=True)) # / aidx.shape[-2]\n",
        "    ###\n",
        "    # midx = (\n",
        "    #     _nsbmd(aidx, 1.0 - bidx, aidx, bidx)\n",
        "    #     + _nsbmd(1.0 - aidx, bidx, aidx, bidx)\n",
        "    # )\n",
        "    ###\n",
        "    new_shape = apre + bpre + (d_idx,)\n",
        "    midx = midx.reshape(new_shape)\n",
        "    #\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SknOTQ7O9BS"
      },
      "source": [
        "#### Sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WicPIpyIO3wu"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "def idxhood(xidx, ws, stride=None, num_sets=None, sample=False):\n",
        "  xidx = xidx.reshape(-1, xidx.shape[-1]).cpu().detach().numpy()\n",
        "  # set desired number of neighbors\n",
        "  nneigh = int(np.prod(ws))\n",
        "  nneigh = min(nneigh, len(xidx))\n",
        "  neigh = NearestNeighbors(\n",
        "      n_neighbors=nneigh,\n",
        "      # algorithm=\"brute\",\n",
        "      # metric=\"minkowski\",\n",
        "      # p=1,\n",
        "  )\n",
        "  neigh.fit(xidx)\n",
        "  # select indices of k nearest neighbors of the vectors in the input list\n",
        "  if sample:\n",
        "    if num_sets is None:\n",
        "      num_sets = (len(xidx) + (len(xidx) % stride)) // stride\n",
        "    num_sets = min(num_sets, len(xidx))\n",
        "    subidx = np.random.choice(len(xidx), size=num_sets, replace=False)\n",
        "    all_hoods = neigh.kneighbors(\n",
        "        xidx[subidx], return_distance=False,\n",
        "    )\n",
        "    all_hoods = set(tuple(sorted(hood)) for hood in all_hoods)\n",
        "    all_hoods = np.array(list(all_hoods))\n",
        "    ###\n",
        "    # hood_means = xidx[all_hoods.reshape(-1)]\n",
        "    # hood_means = hood_means.reshape(len(all_hoods), nneigh, xidx.shape[-1])\n",
        "    # hood_mins = hood_means.min(axis=1)[:, None, :]\n",
        "    # hood_maxes = hood_means.max(axis=1)[:, None, :]\n",
        "    # hood_norm = (hood_means - hood_mins) / (hood_maxes - hood_mins + 1e-6)\n",
        "    # hood_norm_ = hood_norm.mean(axis=1)\n",
        "    # hood_consensus = np.median(hood_norm_, axis=0)\n",
        "    # hood_filter = (np.linalg.norm(hood_norm_ - hood_consensus[None, :], axis=1) < 1e-4)\n",
        "    # all_hoods = all_hoods[hood_filter]\n",
        "    ###\n",
        "  else:\n",
        "    _neigh = NearestNeighbors(n_neighbors=nneigh)\n",
        "    _ids = np.array([[pos // img_dim, pos % img_dim] for pos in range(len(xidx))])\n",
        "    # _ids = np.array([[pos] for pos in range(len(xidx))])\n",
        "    _neigh.fit(_ids)\n",
        "    if stride is None:\n",
        "      stride = len(xidx) // num_sets\n",
        "    all_hoods = _neigh.kneighbors(\n",
        "        _ids[::stride], return_distance=False\n",
        "    ) # .reshape(-1)\n",
        "  all_hoods = torch.from_numpy(all_hoods).long()\n",
        "  return all_hoods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCh8kNiFl15G"
      },
      "source": [
        "#### MModule IV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "PAqc0d7Kl6Ud"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def _count_params(config):\n",
        "  tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "  n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "  return n_params\n",
        "\n",
        "class MModule4(nn.Module):\n",
        "  def __init__(self, config, idx_dim=3, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    assert len(config[\"features\"][\"sets\"]) > 0\n",
        "    assert len(config[\"params\"][\"sets\"]) > 0\n",
        "    assert len(config[\"features\"][\"sets\"]) == len(config[\"params\"][\"sets\"])\n",
        "    for key in [\"sets\", \"samples\"]:\n",
        "      assert len(config[\"features\"][key]) == len(config[\"features\"][\"sets\"])\n",
        "      assert len(config[\"params\"][key]) == len(config[\"params\"][\"sets\"])\n",
        "    self._idx_dim = idx_dim\n",
        "    self.device = device\n",
        "    self._config = config\n",
        "    self._config[\"params\"][\"samples\"] = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "    self._ch0 = config[\"features\"][\"pre-set\"]\n",
        "    self._is_conv = config[\"params\"][\"is conv\"]\n",
        "    self._feat_samples = config[\"features\"][\"samples\"]\n",
        "    self._n_params = _count_params(config)\n",
        "    self.W, self.W_idx, self.MW = self._make_pmt(\n",
        "        (self._n_params,), idx_dim, device\n",
        "    )\n",
        "    # self.activation = nn.ELU()\n",
        "    self.activation = nn.ReLU()\n",
        "    # self.activation = nn.LeakyReLU()\n",
        "    self._probe = nn.Linear(self._feat_samples[-1], 10).to(device)\n",
        "    self._num_fwd = 0\n",
        "    ###\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    self._bn_w = nn.Parameter(torch.ones(n_layers - 1).to(device))\n",
        "    self._bn_b = nn.Parameter(torch.zeros(n_layers - 1).to(device))\n",
        "\n",
        "  def _make_pmt(self, shape, idxdim, device):\n",
        "    _mag = 0.1 # 2.0 # 1000.0 * 0.01\n",
        "    _W_idx = _mag * torch.rand((*shape, idxdim), device=device) - (_mag / 2.0)\n",
        "    _std = 0.01\n",
        "    _W = nn.Parameter(\n",
        "        _std * torch.randn(shape, device=device)\n",
        "    )\n",
        "    _mt = MTensor(_W, _W_idx)\n",
        "    return _W, _W_idx, _mt\n",
        "\n",
        "  def _put_one(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    device = x.data.device\n",
        "    assert device == x.idx.device\n",
        "    ones_idx = self._ones_idx.repeat(n, 1, 1)\n",
        "    dummy_ones = MTensor(\n",
        "        values=torch.ones(n, 1).to(device),\n",
        "        indices=ones_idx,\n",
        "    )\n",
        "    x = MTensor.cat([x, dummy_ones], dim=1)\n",
        "    return x\n",
        "\n",
        "  def _reinit_indices(self, pool, wl, wr, samples):\n",
        "    \"\"\"\n",
        "    pool: N x mshape\n",
        "    params: param_shape\n",
        "    \"\"\"\n",
        "    params = self.MW[wl: wr]\n",
        "    idx_dim = pool.idx.shape[-1]\n",
        "    assert params.idx.shape[-1] == idx_dim\n",
        "    pool_idx = pool.idx.reshape(-1, samples, idx_dim)\n",
        "    pool_idx = minmax_normalize(pool_idx).reshape(-1, idx_dim)\n",
        "    # eps = 1e-6\n",
        "    # idx_max = pool_idx.max(dim=0, keepdim=True)[0] + eps\n",
        "    # idx_min = pool_idx.min(dim=0, keepdim=True)[0] - eps\n",
        "    idx_max = pool_idx.max(dim=0, keepdim=True)[0]\n",
        "    idx_min = pool_idx.min(dim=0, keepdim=True)[0]\n",
        "    idx_itv = idx_max - idx_min\n",
        "    n_samples = len(params.idx.reshape(-1, idx_dim))\n",
        "    sampled_idx = torch.rand((n_samples, idx_dim), device=pool.data.device)\n",
        "    sampled_idx = sampled_idx * idx_itv + idx_min\n",
        "    # params.idx = sampled_idx\n",
        "    self.MW.idx[wl: wr] = sampled_idx\n",
        "\n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    ###\n",
        "    n_layers = len(self._config[\"params\"][\"sets\"])\n",
        "    n = x.data.shape[0]\n",
        "    param_sets = deepcopy(self._config[\"params\"][\"sets\"])\n",
        "    param_samples = deepcopy(self._config[\"params\"][\"samples\"])\n",
        "    feat_sets = deepcopy(self._config[\"features\"][\"sets\"])\n",
        "    feat_samples = deepcopy(self._config[\"features\"][\"samples\"])\n",
        "    ###\n",
        "    # self._curr_sets = feat_sets\n",
        "    # self._curr_samples = feat_samples\n",
        "    ###\n",
        "    self.all_pools = []\n",
        "    self.all_samples = []\n",
        "    pool = x\n",
        "    pool = MTensor.reshape(pool, (n, self._ch0, -1))\n",
        "    pool = MTensor.permute(pool, (0, 2, 1))\n",
        "    wl, wr = 0, param_sets[0] * param_samples[0]\n",
        "    for step in range(n_layers):\n",
        "      self.all_pools.append(pool[:4])\n",
        "      if self._is_conv[step]:\n",
        "        idx_slice = pool.idx[0, :, 0]\n",
        "      else:\n",
        "        pool = MTensor.reshape(pool, (n, -1))\n",
        "        # idx_slice = pool.idx[0]\n",
        "        idx_slice = pool.idx[0, :, :3]\n",
        "      # pool = MTensor.reshape(pool, (n, -1))\n",
        "      # idx_slice = pool.idx[0]\n",
        "      idxx = idxhood(\n",
        "          idx_slice,\n",
        "          feat_samples[step],\n",
        "          num_sets=feat_sets[step],\n",
        "          sample=True,\n",
        "      )\n",
        "      ###\n",
        "      feat_sets[step] = idxx.shape[0]\n",
        "      feat_samples[step] = idxx.shape[1]\n",
        "      idxx = idxx.reshape(-1)\n",
        "      ###\n",
        "      # pool: N x (num_windows * window_volume)\n",
        "      pool = pool[:, idxx]\n",
        "      ###\n",
        "      if self._num_fwd == 0:\n",
        "        self._reinit_indices(pool, wl, wr, feat_samples[step])\n",
        "      ###\n",
        "      self.all_samples.append(pool[:4])\n",
        "      ###\n",
        "      mw = MTensor.reshape(\n",
        "          self.MW[wl: wr],\n",
        "          (param_sets[step], param_samples[step])\n",
        "      )\n",
        "      ###\n",
        "      # pool: (N * num_windows) x sets\n",
        "      # pool = MTensor.reshape(pool, (-1, feat_samples[step])) @ mw\n",
        "      ### o 1 do bias parece que atrapalha\n",
        "      # pool = (\n",
        "      #     self._put_one(MTensor.reshape(\n",
        "      #         pool, (n * feat_sets[step], -1)\n",
        "      #     ))\n",
        "      #     @ mw\n",
        "      # )\n",
        "      ###\n",
        "      maromba_only = False\n",
        "      if maromba_only or (step < n_layers - 1):\n",
        "        pool = (\n",
        "            MTensor.reshape(\n",
        "                pool, (n * feat_sets[step], -1)\n",
        "            )\n",
        "            @ mw\n",
        "        )\n",
        "      else:\n",
        "        pool = self._probe(pool.data.reshape(n, -1))\n",
        "        pool = MTensor(\n",
        "            pool,\n",
        "            torch.zeros((*pool.shape, self._idx_dim)).to(pool.device)\n",
        "        )\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, -1))\n",
        "      # pool: N x (num_windows * sets)\n",
        "      if step < n_layers - 1:\n",
        "        pool.data = self.activation(pool.data)\n",
        "        # pool.data = nn.functional.batch_norm(\n",
        "        #     pool.data,\n",
        "        #     pool.data.mean(dim=0).detach(),\n",
        "        #     pool.data.var(dim=0).detach(),\n",
        "        #     training=True,\n",
        "        #     weight=self._bn_w[step],\n",
        "        #     bias=self._bn_b[step],\n",
        "        # )\n",
        "      else:\n",
        "        break\n",
        "      ###\n",
        "      pool = MTensor.reshape(pool, (n, feat_sets[step], param_sets[step]))\n",
        "      ###\n",
        "      nxt_step = step + 1\n",
        "      next_wr = wr + param_sets[nxt_step] * param_samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    self._num_fwd += 1\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss_lr = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  # loss_rl = ce(y_true_match.squeeze(1), torch.argmax(y_pred, dim=-1))\n",
        "  loss_rl = ce(y_pred, torch.argmax(y_true_match.squeeze(1), dim=-1))\n",
        "  loss = loss_lr + loss_rl\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  ###\n",
        "  # pred_index = MTensor._cosine_kernel(pred_index)\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def regular_accuracy(y_true, y_pred):\n",
        "  y_true = torch.argmax(y_true.data, dim=-1).tolist()\n",
        "  y_pred = torch.argmax(y_pred.data, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return acc\n",
        "\n",
        "def regular_loss(y_true, y_pred):\n",
        "  y_true = y_true.data\n",
        "  # y_pred = 10.0 * y_pred.data ### WHY 10x?\n",
        "  y_pred = y_pred.data ### WHY 10x?\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  loss = ce(y_pred, torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "maromba_loss = regular_loss\n",
        "maromba_accuracy = regular_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "hidden_dim = 1 * img_dim\n",
        "clf_dim = (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "idx_dim = 3 # 500 # 3 # 10\n",
        "\n",
        "# template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = cartesian_idx(rows, cols, chs=channels, offset=1, d=idx_dim, mag=2.0) # 1000.0\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "# template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = torch.eye(num_classes)[:, -idx_dim:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_m1YvjxBdj9"
      },
      "source": [
        "### Visualizações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ4DrI6mBn39"
      },
      "outputs": [],
      "source": [
        "def plot_features(x: MTensor):\n",
        "  \"\"\"\n",
        "  x.data: in_dim\n",
        "  x.idx:  in_dim x idx_dim\n",
        "  \"\"\"\n",
        "  n, idx_dim = x.idx.shape\n",
        "  assert x.data.shape == (n,)\n",
        "  tidx = x.idx.cpu().detach().numpy()\n",
        "  tdata = x.data.cpu().detach().numpy()\n",
        "  plot_df = pd.DataFrame(\n",
        "      {\n",
        "          \"x\": tidx[:, 0],\n",
        "          \"y\": tidx[:, 1],\n",
        "          \"z\": tidx[:, 2],\n",
        "          \"val\": tdata,\n",
        "      }\n",
        "  )\n",
        "  fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=\"val\")\n",
        "  fig.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQbo5JtHw3bq"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGn5VTZPw-1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "1dcf424f-c7cc-4f29-82e1-246048d13d70"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "hidden_dim = 10\n",
        "start_mode = True\n",
        "valid_mode = False\n",
        "index_mode = False\n",
        "\n",
        "# TODO: Visualize conv layer output\n",
        "###\n",
        "# config = {\n",
        "#     \"features\": {\n",
        "#         \"pre-set\": channels,\n",
        "#         \"sets\":    [784, 2*784, 4*784, 1,],\n",
        "#         \"samples\": [9, 18, 36, 2904,],\n",
        "#     },\n",
        "#     \"params\": {\n",
        "#         \"sets\":    [2, 4, 6, num_classes,],\n",
        "#         \"samples\": [9, 18, 36, 784,],\n",
        "#         \"is conv\": [False, False, False, False]\n",
        "#     },\n",
        "# }\n",
        "###\n",
        "### Complete indices require not undersampling\n",
        "config = {\n",
        "    \"features\": {\n",
        "        \"pre-set\": channels,\n",
        "        \"sets\":    [1024, 1024, 1,],\n",
        "        \"samples\": [9, 36, 5176,],\n",
        "    },\n",
        "    \"params\": {\n",
        "        \"sets\":    [4, 8, num_classes,],\n",
        "        \"samples\": [4*9, 4*36, 1024,],\n",
        "        \"is conv\": [False, False, False]\n",
        "    },\n",
        "}\n",
        "\n",
        "tot_samples = [np.prod(smp) for smp in config[\"params\"][\"samples\"]]\n",
        "conv_params = int(np.dot(config[\"params\"][\"sets\"][:-1], tot_samples[:-1]))\n",
        "n_params = int(np.dot(config[\"params\"][\"sets\"], tot_samples))\n",
        "\n",
        "if start_mode:\n",
        "  model = MModule4(\n",
        "      config=config,\n",
        "      idx_dim=idx_dim,\n",
        "      device=device,\n",
        "  )\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  # optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "  widx0 = model.MW.idx[:, :conv_params].clone().detach()\n",
        "  train_log = {\n",
        "      \"train loss\": [],\n",
        "      \"eval loss\": [],\n",
        "      \"acc\": [],\n",
        "      \"set\": [],\n",
        "      \"epoch\": [],\n",
        "  }\n",
        "  epoch = 0\n",
        "\n",
        "num_epochs = 7200\n",
        "epoch_len = 10 # 60\n",
        "\n",
        "while epoch < num_epochs:\n",
        "  epoch += 1\n",
        "  ###\n",
        "  # widx_diff = (model.MW.idx[:, :conv_params] - widx0)\n",
        "  # print(widx_diff.abs().mean().item(), widx_diff.min().item(), widx_diff.max().item())\n",
        "  ###\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    ###\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    _loss = loss.item()\n",
        "    # loss += 1e-1 * torch.cat(model._penalties, dim=0).sum()\n",
        "    ###\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(_loss)\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  if valid_mode:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x, y in iter(test_data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        x, y = prepare_input(x, y, device=device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = maromba_loss(y, y_pred)\n",
        "        acc = maromba_accuracy(y, y_pred)\n",
        "        train_log[\"eval loss\"].append(loss.item())\n",
        "        train_log[\"train loss\"].append(np.nan)\n",
        "        train_log[\"acc\"].append(acc.item())\n",
        "        train_log[\"set\"].append(\"eval\")\n",
        "        train_log[\"epoch\"].append(epoch)\n",
        "    metric_cols = [\"eval loss\", \"acc\"]\n",
        "    set_val = \"eval\"\n",
        "  else:\n",
        "    metric_cols = [\"train loss\",]\n",
        "    set_val = \"train\"\n",
        "  if index_mode:\n",
        "    _layer = 2\n",
        "    _batchidx = 0\n",
        "    feat_map = True\n",
        "    if feat_map:\n",
        "      pool = model.all_pools[_layer]\n",
        "      pool = MTensor.reshape(pool[_batchidx], (-1,))\n",
        "      display.clear_output(wait=True)\n",
        "      plot_features(pool)\n",
        "    else:\n",
        "      pool = model.all_samples[_layer - 1]\n",
        "      _shape = (\n",
        "          model._curr_sets[_layer - 1],\n",
        "          model._curr_samples[_layer - 1],\n",
        "      )\n",
        "      _set = (model._curr_sets[_layer - 1]) // 2\n",
        "      pool = MTensor.reshape(pool[_batchidx], _shape)[_set]\n",
        "      display.clear_output(wait=True)\n",
        "      plot_features(pool)\n",
        "    from time import sleep\n",
        "    sleep(3)\n",
        "  else:\n",
        "    group_cols = [\"epoch\"] + metric_cols\n",
        "    df_train = pd.DataFrame(train_log)\n",
        "    df_train = df_train[df_train[\"set\"] == set_val]\n",
        "    display.clear_output(wait=True)\n",
        "    (\n",
        "      df_train[group_cols]\n",
        "      .groupby(\"epoch\")\n",
        "      .agg(lambda x: x.median(skipna=True))\n",
        "      .reset_index()\n",
        "      .sort_values(\"epoch\", ascending=True)\n",
        "      .tail(30)[metric_cols]\n",
        "      .plot(figsize=(16, 3), grid=True)\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Hm-pCJqjTm"
      },
      "outputs": [],
      "source": [
        "# tidx = idxu.reshape(32, -1, 3)[0].cpu().detach().numpy()\n",
        "# tidx = idxu.reshape(32, -1, 18, 3)[0, 0].cpu().detach().numpy()\n",
        "# tidx = idxv.reshape(-1, 3).cpu().detach().numpy()\n",
        "## tidx = model.W_idx[0, :18].cpu().detach().numpy()\n",
        "# plot_df = pd.DataFrame({\"x\": tidx[:, 0], \"y\": tidx[:, 1], \"z\": tidx[:, 2]})\n",
        "# fig = px.scatter_3d(plot_df, x=\"x\", y=\"y\", z=\"z\", color=None); fig.show();\n",
        "##\n",
        "# phi = idxu[0] @ idxv[0].T\n",
        "# import seaborn as sns\n",
        "# sns.heatmap(phi.cpu().detach().numpy()); plt.show()\n",
        "##\n",
        "# iidx = xidx[(all_hoods[hood_filter]).reshape(-1)].reshape(sum(hood_filter), -1, 3)\n",
        "# iidx_ = iidx.mean(axis=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ndQSziNdjoUm",
        "RGCfrrmCXap_",
        "tzKu4c8hisNY",
        "YLr5gOnn5RRu",
        "XzzFCy32AGsX",
        "9Ytm2bU_JvrK",
        "kTfYY3SQXNJF",
        "1SknOTQ7O9BS",
        "QQRFtDATXUmH",
        "039kGqbPXp4d",
        "8_m1YvjxBdj9"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMdMPfH8nuWimuZXOny+bWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}