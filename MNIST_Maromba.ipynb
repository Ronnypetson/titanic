{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ronnypetson/titanic/blob/master/MNIST_Maromba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elxoSeIKAV1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pylab as plt\n",
        "import time\n",
        "from IPython import display\n",
        "from IPython.core.debugger import Pdb\n",
        "\n",
        "def breakpoint():\n",
        "    Pdb().set_trace()\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCfrrmCXap_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6dxGxcHAx5P"
      },
      "outputs": [],
      "source": [
        "tr = ToTensor()\n",
        "\n",
        "img_dim = 10\n",
        "\n",
        "def _transform(x):\n",
        "  x = x.resize((img_dim, img_dim))\n",
        "  return (tr(x) * 2.0 - 1.0).reshape(-1)\n",
        "\n",
        "bsize = 32\n",
        "\n",
        "MNIST_train_data = MNIST(\n",
        "    \"MNIST_root/\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=_transform,\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_train_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "MNIST_test_data = MNIST(\n",
        "    \"MNIST_root_test/\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=_transform,\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    MNIST_test_data,\n",
        "    batch_size=bsize,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cLnafymDzd"
      },
      "outputs": [],
      "source": [
        "def _sin_arr(d, idx, rows):\n",
        "  _x = (np.arange(0, d) / d) * (4 * np.pi * (1 + idx / rows))\n",
        "  return np.sin(_x)\n",
        "\n",
        "def _cos_arr(d, idx, cols):\n",
        "  _x = (np.arange(0, d) / d) * (4 * np.pi * (1 + idx / cols))\n",
        "  return np.cos(_x)\n",
        "\n",
        "def _ind_arr(d, idx, bins):\n",
        "  _x = np.zeros(d)\n",
        "  idx = (d * idx) // bins\n",
        "  _x[idx] = 1.0\n",
        "  return _x\n",
        "\n",
        "def _2ind_arr(d, idx, bins):\n",
        "  return 2 * _ind_arr(d, idx, bins)\n",
        "\n",
        "def _bincat2d(rows, cols, d=32):\n",
        "  bitsr = len(format(rows, \"0b\"))\n",
        "  bitsc = len(format(cols, \"0b\"))\n",
        "  assert 2 * (bitsr + bitsc) <= d\n",
        "  idx = np.zeros((rows, cols, d))\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      binr = format(row, f\"0{bitsr}b\")\n",
        "      binc = format(col, f\"0{bitsc}b\")\n",
        "      for pos, bit in enumerate(binr):\n",
        "        idxidx = 2 * pos + int(bit)\n",
        "        idx[row, col, idxidx] = 1.0\n",
        "      for pos, bit in enumerate(binc):\n",
        "        idxidx = 2 * bitsr + 2 * pos + int(bit)\n",
        "        idx[row, col, idxidx] = 1.0\n",
        "  return idx\n",
        "\n",
        "def _cat2d(rows, cols, d=32):\n",
        "  \"\"\"\n",
        "  Index in the log-softmax scale.\n",
        "  After sotmax (in the partition dimension)\n",
        "  -inf --> 0\n",
        "  1.0  --> 1\n",
        "  \"\"\"\n",
        "  assert rows + cols <= d\n",
        "  inf = 1.0\n",
        "  idx = np.zeros((rows, cols, d)) - inf\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      idx[row, col, row] = 1.0\n",
        "      idx[row, col, rows + col] = 1.0\n",
        "  idx = torch.from_numpy(idx)\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  return idx\n",
        "\n",
        "def _posenc(shape, f_row, f_col, d=32, nonlin=None):\n",
        "  \"\"\"\n",
        "  3D Positional encodings (f_row(row) + f_col(col))\n",
        "  \"\"\"\n",
        "  assert len(shape) == 2\n",
        "  rows, cols = shape\n",
        "  idx_sin = np.zeros((rows, d))\n",
        "  idx_cos = np.zeros((cols, d))\n",
        "  for idx in range(rows):\n",
        "    idx_sin[idx] = f_row(d, idx, rows)\n",
        "  for idx in range(cols):\n",
        "    idx_cos[idx] = f_col(d, idx, cols)\n",
        "  idx_sin = torch.from_numpy(idx_sin)\n",
        "  idx_cos = torch.from_numpy(idx_cos)\n",
        "  idx = (\n",
        "      idx_sin.reshape((rows, 1, d)).repeat(1, cols, 1)\n",
        "      + idx_cos.reshape((1, cols, d)).repeat(rows, 1, 1)\n",
        "  )\n",
        "  idx = idx.reshape(rows * cols, d)\n",
        "  if nonlin:\n",
        "    idx = nonlin(idx)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taSPF5ITc5vQ"
      },
      "outputs": [],
      "source": [
        "# binidx = _cat2d(5, 2, d=10)\n",
        "# binidx @ binidx.T\n",
        "# np.exp(binidx[0]) / np.exp(binidx[0]).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyWPtI28l-pp"
      },
      "outputs": [],
      "source": [
        "# rows, cols, d = 5, 5, 32\n",
        "# pos = _posenc(\n",
        "#     (rows, cols),\n",
        "#     _ind_arr, # _sin_arr,\n",
        "#     _2ind_arr, # _cos_arr,\n",
        "#     d=d,\n",
        "#     nonlin=lambda x: x / 3.0, # torch.sigmoid\n",
        "# ).reshape(rows, cols, d)\n",
        "# fig, axs = plt.subplots(nrows=rows, ncols=cols, layout=None)\n",
        "# for row in range(rows):\n",
        "#   for col in range(cols):\n",
        "#     axs[row][col].plot(range(d), pos[row, col].numpy())\n",
        "# plt.show()\n",
        "# print(pos[0] @ pos[1].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS12YlllA3xE"
      },
      "outputs": [],
      "source": [
        "# x, y = MNIST_train_data[5]\n",
        "# plt.imshow(np.array(x.reshape(7, 7))), y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfYY3SQXNJF"
      },
      "source": [
        "### Classe Tensor Maromba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVRPHg7UvVV"
      },
      "outputs": [],
      "source": [
        "class MTensor:\n",
        "  def __init__(\n",
        "      self,\n",
        "      values: torch.Tensor,\n",
        "      indices: torch.Tensor,\n",
        "      indexer: nn.Module=nn.Identity(),\n",
        "    ):\n",
        "    assert values.shape == indices.shape[:-1]\n",
        "    self.data = values\n",
        "    self.idx = indices\n",
        "    self.idx_dim = indices.shape[-1]\n",
        "    self.indexer = indexer\n",
        "    self._idx_part = img_dim\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return MTensor(self.data[idx], self.idx[idx], self.indexer)\n",
        "\n",
        "  def __setitem__(self, idx, value):\n",
        "    self.data[idx] = value.data\n",
        "    self.idx[idx] = value.idx\n",
        "\n",
        "  def __delitem__(self, idx):\n",
        "    del self.data[idx]\n",
        "    del self.idx[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  @staticmethod\n",
        "  def cat(mts, dim=0):\n",
        "    values = [mt.data for mt in mts]\n",
        "    indices = [mt.idx for mt in mts]\n",
        "    values = torch.cat(values, dim=dim)\n",
        "    indices = torch.cat(indices, dim=dim)\n",
        "    mt = MTensor(values, indices)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def unsqueeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.unsqueeze(dim)\n",
        "    mt.idx = mt.idx.unsqueeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def squeeze(mt, dim=0):\n",
        "    assert dim != -1\n",
        "    assert dim < len(mt.idx.shape) - 1\n",
        "    mt.data = mt.data.squeeze(dim)\n",
        "    mt.idx = mt.idx.squeeze(dim)\n",
        "    return mt\n",
        "\n",
        "  @staticmethod\n",
        "  def clone(mt):\n",
        "    return MTensor(mt.data, mt.idx, mt.indexer)\n",
        "\n",
        "  @staticmethod\n",
        "  def reshape(mt, shape):\n",
        "    idx_shape = shape + (mt.idx_dim,)\n",
        "    nmt = MTensor(\n",
        "        mt.data.reshape(shape),\n",
        "        mt.idx.reshape(idx_shape),\n",
        "        mt.indexer\n",
        "    )\n",
        "    return nmt\n",
        "\n",
        "  def _gbmd(self, u, v, idxu, idxv, kernel=None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    'General Batch Maromba Dot'\n",
        "    Shorter implementation for the 'batch maromba dot' operation.\n",
        "    u: M x d_u\n",
        "    v: N x d_v\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u = u.shape\n",
        "    n, d_v = v.shape\n",
        "    d_idx = idxu.shape[-1]\n",
        "    assert (m, d_u, d_idx) == idxu.shape\n",
        "    assert (n, d_v, d_idx) == idxv.shape\n",
        "    if kernel:\n",
        "      idxu = kernel(idxu, self._idx_part)\n",
        "      idxv = kernel(idxv, self._idx_part)\n",
        "    # uidxu: M x d_idx\n",
        "    # vidxv: N x d_idx\n",
        "    uidxu = torch.bmm(u.reshape(m, 1, d_u), idxu).squeeze(1)\n",
        "    vidxv = torch.bmm(v.reshape(n, 1, d_v), idxv).squeeze(1)\n",
        "    dot = uidxu @ vidxv.T\n",
        "    ### Under experimentation\n",
        "    normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "    dot = dot / normalizer\n",
        "    ###\n",
        "    return dot\n",
        "\n",
        "  @staticmethod\n",
        "  def _soft_kernel(idxu, part_dim):\n",
        "    \"\"\"\n",
        "    idxu: M x d_u x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    assert d_idx % part_dim == 0\n",
        "    norm_idxu = torch.softmax(idxu.reshape(m, d_u, -1, part_dim), dim=-1)\n",
        "    norm_idxu = norm_idxu.reshape(m, d_u, d_idx)\n",
        "    return norm_idxu\n",
        "\n",
        "  def _kernel_idx(self, idxu, idxv, k):\n",
        "    \"\"\"\n",
        "    k: callable: A x B x C -> A x B x C\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    n, d_v, _ = idxv.shape\n",
        "    assert d_idx == idxv.shape[-1]\n",
        "    # kidxu: M x d_u x d_idx\n",
        "    # kidxv: N x d_v x d_idx\n",
        "    kidxu = k(idxu, self._idx_part)\n",
        "    kidxv = k(idxv, self._idx_part)\n",
        "    assert kidxu.shape == idxu.shape\n",
        "    assert kidxv.shape == idxv.shape\n",
        "    # ski: (M * N) x d_idx\n",
        "    # skj: (M * N) x d_idx\n",
        "    # norm: M x N x 1\n",
        "    ski = kidxu.sum(dim=1)\n",
        "    skj = kidxv.sum(dim=1)\n",
        "    norm = (ski @ skj.T).unsqueeze(-1)\n",
        "    ski = ski.unsqueeze(1).repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "    skj = skj.unsqueeze(1).repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "    # idxu, kidxu: (M * d_u) x d_idx x 1\n",
        "    # idxv, kidxv: (N * d_v) x d_idx x 1\n",
        "    idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "    idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "    kidxu = kidxu.reshape(m * d_u, d_idx, 1)\n",
        "    kidxv = kidxv.reshape(n * d_v, d_idx, 1)\n",
        "    # sikiT: M x d_idx x d_idx\n",
        "    # sjkjT: N x d_idx x d_idx\n",
        "    sikiT = torch.bmm(idxu, kidxu.permute(0, 2, 1))\n",
        "    sikiT = sikiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "    sjkjT = torch.bmm(idxv, kidxv.permute(0, 2, 1))\n",
        "    sjkjT = sjkjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1)\n",
        "    # sikiT: (M * N) x d_idx x d_idx\n",
        "    # sjkjT: (M * N) x d_idx x d_idx\n",
        "    sikiT = sikiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    sjkjT = sjkjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    # diag_sikiT_skjjT: (M * N) x d_idx\n",
        "    skjjT = sjkjT.permute(0, 2, 1)\n",
        "    diag_sikiT_skjjT = torch.diagonal(torch.bmm(sikiT, skjjT), dim1=1, dim2=2)\n",
        "    diag_sikiT_skjjT = diag_sikiT_skjjT.unsqueeze(-1)\n",
        "    xor_idx = torch.bmm(sikiT, skj) + torch.bmm(sjkjT, ski) - diag_sikiT_skjjT\n",
        "    xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "    xor_idx = xor_idx / norm\n",
        "    return xor_idx\n",
        "\n",
        "  def _xor_idx(self, idxu, idxv):\n",
        "    \"\"\"\n",
        "    idxu: M x d_u x d_idx\n",
        "    idxv: N x d_v x d_idx\n",
        "    \"\"\"\n",
        "    m, d_u, d_idx = idxu.shape\n",
        "    n, d_v, _ = idxv.shape\n",
        "    assert d_idx == idxv.shape[-1]\n",
        "    ### Under experimentation\n",
        "    # idxu = nn.functional.relu(idxu)\n",
        "    # idxv = nn.functional.relu(idxv)\n",
        "    # norm_idxu = torch.softmax(idxu.reshape(m, d_u, -1, self._idx_part), dim=-1)\n",
        "    # norm_idxu = norm_idxu.reshape(m, d_u, d_idx)\n",
        "    # norm_idxv = torch.softmax(idxv.reshape(n, d_v, -1, self._idx_part), dim=-1)\n",
        "    # norm_idxv = norm_idxv.reshape(n, d_v, d_idx)\n",
        "    normalizer = idxu.sum(dim=1) @ idxv.sum(dim=1).T\n",
        "    normalizer = normalizer.unsqueeze(-1)\n",
        "    ###\n",
        "    # idxu: (M * d_u) x d_idx x 1\n",
        "    # idxv: (N * d_v) x d_idx x 1\n",
        "    idxu = idxu.reshape(m * d_u, d_idx, 1)\n",
        "    idxv = idxv.reshape(n * d_v, d_idx, 1)\n",
        "    # siiT: M x d_idx x d_idx\n",
        "    # sjjT: N x d_idx x d_idx\n",
        "    siiT = torch.bmm(idxu, idxu.permute(0, 2, 1))\n",
        "    siiT = siiT.reshape(m, d_u, d_idx, d_idx).sum(dim=1)\n",
        "    sjjT = torch.bmm(idxv, idxv.permute(0, 2, 1))\n",
        "    sjjT = sjjT.reshape(n, d_v, d_idx, d_idx).sum(dim=1) ###\n",
        "    # siiT: (M * N) x d_idx x d_idx\n",
        "    # sjjT: (M * N) x d_idx x d_idx\n",
        "    siiT = siiT.unsqueeze(1).repeat(1, n, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    sjjT = sjjT.unsqueeze(0).repeat(m, 1, 1, 1).reshape(m * n, d_idx, d_idx)\n",
        "    # si: (M * N) x d_idx x 1\n",
        "    # sj: (M * N) x d_idx x 1\n",
        "    idxu = idxu.reshape(m, d_u, d_idx)\n",
        "    idxv = idxv.reshape(n, d_v, d_idx)\n",
        "    si = idxu.sum(dim=1).unsqueeze(1)\n",
        "    si = si.repeat(1, n, 1).reshape(m * n, d_idx, 1)\n",
        "    sj = idxv.sum(dim=1).unsqueeze(0)\n",
        "    sj = sj.repeat(m, 1, 1).reshape(m * n, d_idx, 1)\n",
        "    diag_siiT_sjjT = torch.diagonal(torch.bmm(siiT, sjjT), dim1=1, dim2=2)\n",
        "    diag_siiT_sjjT = diag_siiT_sjjT.unsqueeze(-1)\n",
        "    # xor_idx = torch.bmm(siiT, sj) + torch.bmm(sjjT, si) - 2 * diag_siiT_sjjT\n",
        "    xor_idx = torch.bmm(siiT, sj) + torch.bmm(sjjT, si) - diag_siiT_sjjT\n",
        "    # xor_idx = xor_idx.reshape(m, n, d_idx) / d_u ### TODO: check this\n",
        "    xor_idx = xor_idx.reshape(m, n, d_idx)\n",
        "    # xor_idx = torch.sigmoid(xor_idx)\n",
        "    ### Under experimentation\n",
        "    xor_idx = xor_idx / normalizer\n",
        "    ###\n",
        "    return xor_idx\n",
        "\n",
        "  def __matmul__(self, b):\n",
        "    \"\"\"\n",
        "    Useful for computing m-product between a batch of inputs (N x ...) and a\n",
        "    parameter matrix (m x n).\n",
        "\n",
        "    self.data: pre_shape(self) x in_dim(self)\n",
        "    self.data.idx: pre_shape(self) x in_dim(self) x d_idx\n",
        "    b.data: pre_shape(b) x in_dim(b)\n",
        "    b.idx: pre_shape(b) x in_dim(b) x d_idx\n",
        "\n",
        "    Returns 'mdot'\n",
        "    mdot.data: pre_shape(self) x pre_shape(b)\n",
        "    mdot.idx: pre_shape(self) x pre_shape(b) x d_idx\n",
        "    \"\"\"\n",
        "    apre = self.data.shape[:-1]\n",
        "    bpre = b.data.shape[:-1]\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert d_idx == b.idx.shape[-1]\n",
        "    aidx = self.idx.reshape(*((-1,) + self.idx.shape[-2:]))\n",
        "    bidx = b.idx.reshape(*((-1,) + b.idx.shape[-2:]))\n",
        "    mdot = self._gbmd(\n",
        "        self.data.reshape(-1, self.data.shape[-1]),\n",
        "        b.data.reshape(-1, b.data.shape[-1]),\n",
        "        aidx,\n",
        "        bidx,\n",
        "        kernel=MTensor._soft_kernel\n",
        "    )\n",
        "    mdot = mdot.reshape(apre + bpre)\n",
        "    # midx = self._xor_idx(aidx, bidx)\n",
        "    midx = self._kernel_idx(aidx, bidx, MTensor._soft_kernel)\n",
        "    midx = midx.reshape(apre + bpre + (d_idx,))\n",
        "    mdot = MTensor(mdot, midx, self.indexer)\n",
        "    return mdot\n",
        "\n",
        "  def __mul__(self, b):\n",
        "    \"\"\"\n",
        "    self: N x out_a x in_a (x d_idx)\n",
        "    b:    N x out_b x in_b (x d_idx)\n",
        "    \"\"\"\n",
        "    n, out_a, in_a = self.data.shape\n",
        "    assert b.data.shape[0] == n\n",
        "    _, out_b, in_b = b.data.shape\n",
        "    d_idx = self.idx.shape[-1]\n",
        "    assert b.idx.shape[-1] == d_idx\n",
        "    # adata = self.data.reshape(n * out_a, in_a).unsqueeze(1)\n",
        "    # aidx = self.idx.reshape(n * out_a, in_a, d_idx)\n",
        "    # bdata = b.data.reshape(n * out_b, in_b).unsqueeze(1)\n",
        "    # bidx = b.idx.reshape(n * out_b, in_b, d_idx)\n",
        "    # # aidxa: N x out_a x d_idx\n",
        "    # # bidxb: N x out_b x d_idx\n",
        "    # aidxa = torch.bmm(adata, aidx).squeeze(1).reshape(n, out_a, d_idx)\n",
        "    # bidxb = torch.bmm(bdata, bidx).squeeze(1).reshape(n, out_b, d_idx)\n",
        "    # # dot: N x out_a x out_b\n",
        "    # dot = torch.bmm(aidxa, bidxb.permute(0, 2, 1))\n",
        "    # # idx = self._xor_idx(aidx, bidx)\n",
        "    # return dot\n",
        "    ### Solução provisória. Calcular o índice com paralelismo ainda não é possível.\n",
        "    mdots = [MTensor.unsqueeze(self[idx] @ b[idx], dim=0) for idx in range(n)]\n",
        "    mdots = MTensor.cat(mdots, dim=0)\n",
        "    return mdots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGg59zEqYGe6"
      },
      "source": [
        "### Classe do Módulo Treinável"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oipx_P9qYUUb"
      },
      "outputs": [],
      "source": [
        "class MModule(nn.Module):\n",
        "  def __init__(self, n_params=600, idx_dim=32, samples=32, sets=64, device=\"cpu\"):\n",
        "    super().__init__()\n",
        "    self.idx_dim = idx_dim\n",
        "    self.samples = samples\n",
        "    self.sets = sets\n",
        "    self.device = device\n",
        "    self.n_params = n_params\n",
        "    ### TODO: checar inicialização de W\n",
        "    self.W = nn.Parameter(torch.randn((1, n_params), device=device))\n",
        "    _W_idx = torch.rand((1, n_params, idx_dim), device=device)\n",
        "    # _W_idx = nn.functional.relu(_W_idx)\n",
        "    # _W_idx = _W_idx / _W_idx.max()\n",
        "    self.W_idx = nn.Parameter(_W_idx)\n",
        "    self.MW = MTensor(self.W, self.W_idx)\n",
        "    self.activation = nn.ReLU()\n",
        "    # self.activation = nn.Tanh() # nn.Sigmoid()\n",
        "\n",
        "  def _msample(self, x: MTensor, n_sets, n_samples):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "\n",
        "    Returns\n",
        "    x_sets: N x n_sets x n_samples\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    x_sets = []\n",
        "    for _ in range(n_sets):\n",
        "      idx = np.random.choice(in_dim, n_samples, replace=False)\n",
        "      idx = torch.tensor(idx).long()\n",
        "      # x_sampled.data: N x 1 x n_samples\n",
        "      x_sampled = MTensor.unsqueeze(x[:, idx], dim=1)\n",
        "      x_sets.append(x_sampled)\n",
        "    # x_sets.data: N x n_sets x n_samples\n",
        "    x_sets = MTensor.cat(x_sets, dim=1)\n",
        "    return x_sets\n",
        "\n",
        "  def _W_step(\n",
        "      self,\n",
        "      x: MTensor,\n",
        "      W: MTensor,\n",
        "      sets,\n",
        "      samples,\n",
        "      random=True,\n",
        "      activation=True):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # Put 1 into x\n",
        "    one = MTensor(\n",
        "        torch.ones(n, 1).to(self.device),\n",
        "        torch.ones(n, 1, idx_dim).to(self.device),\n",
        "    )\n",
        "    x = MTensor.cat([x, one], dim=1)\n",
        "    # Sample W\n",
        "    # W_sets = []\n",
        "    # for _ in range(self.sets):\n",
        "    #   idx = np.random.choice(self.n_params, self.samples, replace=False)\n",
        "    #   idx = torch.tensor(idx).long()\n",
        "    #   W_sets.append(self.MW[:, idx])\n",
        "    # W_sets = MTensor.cat(W_sets, dim=0)\n",
        "    # W_sets = self._msample(self.MW, self.sets, self.samples)\n",
        "    if random:\n",
        "      W_sets = self._msample(W, sets, samples)\n",
        "      W_sets = MTensor.squeeze(W_sets, 0)\n",
        "    else:\n",
        "      W_sets = MTensor.reshape(W, (sets, samples))\n",
        "    # mdot: N x sets\n",
        "    mdot = x @ W_sets\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    return mdot\n",
        "\n",
        "  def _pool_step(self, x: MTensor, sets, samples, activation=True):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n, in_dim, idx_dim = x.idx.shape\n",
        "    assert x.data.shape == (n, in_dim)\n",
        "    # x0: N x 1 x samples\n",
        "    # x1: N x sets x samples\n",
        "    x0 = self._msample(x, 1, samples)\n",
        "    x1 = self._msample(x, sets, samples)\n",
        "    # mdot.data: N x sets\n",
        "    mdot = x0 * x1\n",
        "    mdot = MTensor.squeeze(mdot, 1)\n",
        "    if activation:\n",
        "      mdot.data = self.activation(mdot.data)\n",
        "    return mdot\n",
        "\n",
        "  def _forward(self, x: MTensor, n_steps=2):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    assert n_steps > 0\n",
        "    # pool: N x sets\n",
        "    activate = (n_steps > 1)\n",
        "    pool = self._W_step(x, activation=activate)\n",
        "    for step in range(1, n_steps):\n",
        "      # pool: N x (in_dim + step * sets)\n",
        "      activate = (step < n_steps - 1)\n",
        "      if step % 2 == 0:\n",
        "        pool_new = self._pool_step(pool, activation=activate)\n",
        "      else:\n",
        "        pool_new = self._W_step(pool, activation=activate)\n",
        "      pool = MTensor.cat([pool, pool_new], dim=1)\n",
        "    return pool_new\n",
        "  \n",
        "  def forward(self, x: MTensor):\n",
        "    \"\"\"\n",
        "    x.data: N x in_dim\n",
        "    x.idx: N x in_dim x idx_dim\n",
        "    \"\"\"\n",
        "    n_sets, n_samples = len(self.sets), len(self.samples)\n",
        "    assert n_sets == n_samples\n",
        "    assert n_sets > 0\n",
        "    pool = x\n",
        "    # Pdb().set_trace()\n",
        "    wl, wr = 0, self.sets[0] * self.samples[0]\n",
        "    for step in range(n_sets):\n",
        "      activate = (step < n_sets - 1)\n",
        "      pool = self._W_step(\n",
        "          pool,\n",
        "          self.MW[:, wl: wr],\n",
        "          self.sets[step],\n",
        "          self.samples[step],\n",
        "          random=False,\n",
        "          activation=activate\n",
        "      )\n",
        "      nxt_step = (step + 1) % n_sets\n",
        "      next_wr = wr + self.sets[nxt_step] * self.samples[nxt_step]\n",
        "      wl, wr = wr, next_wr\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRFtDATXUmH"
      },
      "source": [
        "### Função de Custo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX8kHpfLXVzo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _check_shapes(y_true, y_pred, true_index, pred_index):\n",
        "  n, d_out = y_true.shape\n",
        "  assert y_true.shape[0] == y_pred.shape[0]\n",
        "  assert true_index.shape[0] == pred_index.shape[0]\n",
        "  assert true_index.shape[-1] == pred_index.shape[-1]\n",
        "\n",
        "def _maromba_loss(y_true, y_pred, true_index, pred_index):\n",
        "  \"\"\"\n",
        "  y_true: N x d_out(true)\n",
        "  y_pred: N x d_out(pred)\n",
        "  true_index: N x d_out(true) x d_index\n",
        "  pred_index: N x d_out(pred) x d_index\n",
        "  \"\"\"\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  ###\n",
        "  pred_index = MTensor._soft_kernel(pred_index, img_dim)\n",
        "  ###\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  ### Under experimentation\n",
        "  # index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "  ###\n",
        "  # y_true_match: N x 1 x d_out(pred)\n",
        "  # y_pred_match: N x 1 x d_out(true)\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  # y_true_match = torch.bmm(y_true.unsqueeze(1), index_match.permute(0, 2, 1))\n",
        "  # huber = nn.HuberLoss()\n",
        "  # match_loss_lr = huber(y_pred, y_true_match.squeeze(1))\n",
        "  # match_loss_rl = huber(y_true, y_pred_match.squeeze(1))\n",
        "  # loss = match_loss_lr + match_loss_rl\n",
        "  ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "  loss = ce(y_pred_match.squeeze(1), torch.argmax(y_true, dim=-1))\n",
        "  return loss\n",
        "\n",
        "def _pool2category(y_true, y_pred, true_index, pred_index):\n",
        "  _check_shapes(y_true, y_pred, true_index, pred_index)\n",
        "  # index_match: N x d_out(pred) x d_out(true)\n",
        "  index_match = torch.bmm(pred_index, true_index.permute(0, 2, 1))\n",
        "  y_pred_match = torch.bmm(y_pred.unsqueeze(1), index_match)\n",
        "  y_pred_match = torch.argmax(y_pred_match.squeeze(1), dim=-1).tolist()\n",
        "  return y_pred_match\n",
        "\n",
        "def _maromba_accuracy(y_true, y_pred, true_index, pred_index):\n",
        "  y_pred_match = _pool2category(y_true, y_pred, true_index, pred_index)\n",
        "  y_true = torch.argmax(y_true, dim=-1).tolist()\n",
        "  acc = accuracy_score(y_true, y_pred_match)\n",
        "  return acc\n",
        "\n",
        "def maromba_accuracy(y_true, y_pred):\n",
        "  return _maromba_accuracy(y_true.data, y_pred.data, y_true.idx, y_pred.idx)\n",
        "\n",
        "def maromba_loss(y_true, y_pred):\n",
        "  return _maromba_loss(y_true.data, y_pred.data, y_true.idx, y_pred.idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039kGqbPXp4d"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeSzd7OmTDDn"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "rows, cols = img_dim, img_dim\n",
        "idx_dim = rows + cols + (1 + (num_classes - 1) // img_dim) * img_dim\n",
        "\n",
        "# template_x_idx = _posenc(\n",
        "#     (rows, cols),\n",
        "#     _ind_arr,\n",
        "#     _2ind_arr,\n",
        "#     d=idx_dim,\n",
        "#     nonlin=lambda x: x / 3.0,\n",
        "# )\n",
        "template_x_idx = _cat2d(rows, cols, d=idx_dim)\n",
        "template_x_idx = template_x_idx.unsqueeze(0).float().to(device)\n",
        "template_y_idx = torch.eye(idx_dim)[-num_classes:]\n",
        "template_y_idx = template_y_idx.float().unsqueeze(0).to(device)\n",
        "\n",
        "def prepare_input(x, y, device=\"cpu\"):\n",
        "  n = x.shape[0]\n",
        "  x_idx = template_x_idx.repeat(n, 1, 1)\n",
        "  yoh = torch.zeros(n, num_classes)\n",
        "  yoh[range(n), y] = 1.0\n",
        "  yoh = yoh.to(device)\n",
        "  y_idx = template_y_idx.repeat(n, 1, 1)\n",
        "  x = MTensor(x, x_idx)\n",
        "  y = MTensor(yoh, y_idx)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNheVxvNNK30"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 200\n",
        "model = MModule(\n",
        "    n_params=rows * cols * hidden_dim + hidden_dim * num_classes,\n",
        "    idx_dim=idx_dim,\n",
        "    samples=[rows * cols, hidden_dim],\n",
        "    sets=[hidden_dim, num_classes],\n",
        "    device=device\n",
        ")\n",
        "optimizer = Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "num_epochs = 360\n",
        "batch_size = 32\n",
        "epoch_len = 60 # len(MNIST_train_data) // batch_size\n",
        "\n",
        "train_log = {\n",
        "    \"train loss\": [],\n",
        "    \"eval loss\": [],\n",
        "    \"acc\": [],\n",
        "    \"set\": [],\n",
        "    \"epoch\": [],\n",
        "}\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_iter = iter(train_data_loader)\n",
        "  # for x, y in iter(train_data_loader):\n",
        "  for _ in range(epoch_len):\n",
        "    x, y = next(train_iter)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    optimizer.zero_grad()\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    loss.backward()\n",
        "    # Pdb().set_trace()\n",
        "    optimizer.step()\n",
        "    train_log[\"train loss\"].append(loss.item())\n",
        "    train_log[\"eval loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(np.nan)\n",
        "    train_log[\"set\"].append(\"train\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  model.eval()\n",
        "  for x, y in iter(test_data_loader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    x, y = prepare_input(x, y, device=device)\n",
        "    y_pred = model.forward(x)\n",
        "    loss = maromba_loss(y, y_pred)\n",
        "    acc = maromba_accuracy(y, y_pred)\n",
        "    train_log[\"eval loss\"].append(loss.item())\n",
        "    train_log[\"train loss\"].append(np.nan)\n",
        "    train_log[\"acc\"].append(acc.item())\n",
        "    train_log[\"set\"].append(\"eval\")\n",
        "    train_log[\"epoch\"].append(epoch)\n",
        "  df_train = pd.DataFrame(train_log)\n",
        "  display.clear_output(wait=True)\n",
        "  group_cols = [\"epoch\", \"train loss\", \"eval loss\", \"acc\"]\n",
        "  df_train[group_cols].groupby(\"epoch\").agg(lambda x: x.median(skipna=True)).plot(figsize=(24, 4))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPdqOyP9Sdx1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGSWz0GB9api"
      },
      "outputs": [],
      "source": [
        "# y_pred.idx = nn.functional.softmax(y_pred.idx, dim=-1)\n",
        "y_pred.idx = MTensor._soft_kernel(y_pred.idx, img_dim)\n",
        "\n",
        "index_match = torch.bmm(y_pred.idx, y.idx.permute(0, 2, 1))\n",
        "# index_match = nn.functional.softmax(index_match, dim=-1)\n",
        "# y_true_match: N x 1 x d_out(pred)\n",
        "# y_pred_match: N x 1 x d_out(true)\n",
        "y_pred_match = torch.bmm(y_pred.data.unsqueeze(1), index_match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR6CRFgWCpFD"
      },
      "outputs": [],
      "source": [
        "# index_match[0].sum(dim=-1)\n",
        "# y_pred_match\n",
        "# torch.argmax(y.data, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAQUfdi_9wUc"
      },
      "outputs": [],
      "source": [
        "ce = nn.CrossEntropyLoss() # nn.NLLLoss() #\n",
        "loss = ce(y_pred_match.squeeze(1), torch.argmax(y.data, dim=-1))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teVFiPYE_KrH"
      },
      "outputs": [],
      "source": [
        "# y_pred_match[0]\n",
        "# index_match[0]\n",
        "MTensor._soft_kernel(y_pred.idx, 7)[0]\n",
        "# y.idx[0]\n",
        "# y_pred.data[0: 5]\n",
        "# [param.grad.max() for param in model.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DWnC3k4LO6a"
      },
      "outputs": [],
      "source": [
        "model.MW.idx[0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwvDxtrN_fEk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNmuIwpNaRaeG21sqMso/P",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}